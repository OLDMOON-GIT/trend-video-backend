"""
ì¤‘êµ­ì–´ ì˜ìƒì„ í•œêµ­ì–´ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ê¸°ëŠ¥:
1. ì¤‘êµ­ì–´ ìë§‰/ìŒì„± ì¶”ì¶œ
2. ì¤‘êµ­ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­
3. í•œêµ­ì–´ TTS ìƒì„±
4. ì˜ìƒ í•©ì„±

ì‚¬ìš©ë²•:
    python chinese_video_converter.py --input "video.mp4" --output-dir "output"
"""

import os
import sys
import json
import argparse
import logging
from pathlib import Path
import subprocess
import asyncio
from typing import List, Dict, Optional
import tempfile
import shutil

# Windowsì—ì„œ UTF-8 ì¶œë ¥ì„ ìœ„í•´ stdoutì„ UTF-8ë¡œ ì¬ì„¤ì •
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(message)s',
    stream=sys.stdout,
    force=True
)
logger = logging.getLogger(__name__)

# OpenAI API (Whisper ë° TTS)
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("âš ï¸ openai ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openai")

# Anthropic Claude API (ë²ˆì—­)
try:
    from anthropic import Anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    logger.warning("âš ï¸ anthropic ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install anthropic")

# Edge TTS (ëŒ€ì²´ ì˜µì…˜)
try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
except ImportError:
    EDGE_TTS_AVAILABLE = False
    logger.warning("âš ï¸ edge-tts ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


def get_ffmpeg_path():
    """FFmpeg ê²½ë¡œ í™•ì¸"""
    try:
        result = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True)
        if result.returncode == 0:
            return 'ffmpeg'
    except FileNotFoundError:
        pass

    # imageio-ffmpeg ì‹œë„
    try:
        import imageio_ffmpeg
        return imageio_ffmpeg.get_ffmpeg_exe()
    except ImportError:
        pass

    raise RuntimeError("FFmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. FFmpegë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")


def get_video_dimensions(video_path: Path) -> tuple:
    """ë¹„ë””ì˜¤ í•´ìƒë„ ê°€ì ¸ì˜¤ê¸° (width, height)"""
    try:
        ffmpeg = get_ffmpeg_path()
        ffprobe = ffmpeg.replace('ffmpeg', 'ffprobe')

        cmd = [
            ffprobe,
            '-v', 'error',
            '-select_streams', 'v:0',
            '-show_entries', 'stream=width,height',
            '-of', 'csv=p=0',
            str(video_path)
        ]

        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        if result.returncode == 0:
            width, height = map(int, result.stdout.strip().split(','))
            return width, height
        return 1920, 1080  # ê¸°ë³¸ê°’

    except Exception as e:
        logger.warning(f"âš ï¸ ë¹„ë””ì˜¤ í•´ìƒë„ í™•ì¸ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš© (1920x1080)")
        return 1920, 1080


def remove_watermark_ai(input_video: Path, output_video: Path, watermark_region: tuple = None) -> bool:
    """
    AI ê¸°ë°˜ ì›Œí„°ë§ˆí¬ ì œê±° - ì¤‘êµ­ì–´ ìë§‰ ì˜ì—­ ìë™ ê°ì§€ ë° ì œê±°

    Args:
        input_video: ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
        output_video: ì¶œë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
        watermark_region: (x, y, w, h) ì›Œí„°ë§ˆí¬ ì˜ì—­, Noneì´ë©´ ìë™ ê°ì§€ (í•˜ë‹¨ ì¤‘êµ­ì–´ ìë§‰)

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        ffmpeg = get_ffmpeg_path()

        # ì›Œí„°ë§ˆí¬ ì˜ì—­ì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ê°ì§€ (ì¤‘êµ­ì–´ ìë§‰ ìœ„ì¹˜)
        if watermark_region is None:
            # ë¹„ë””ì˜¤ í•´ìƒë„ í™•ì¸
            width, height = get_video_dimensions(input_video)
            logger.info(f"ğŸ“ ë¹„ë””ì˜¤ í•´ìƒë„: {width}x{height}")

            # ì¤‘êµ­ì–´ ìë§‰ì€ ë³´í†µ í™”ë©´ í•˜ë‹¨ì— ê³ ì • ìœ„ì¹˜
            # í•˜ë‹¨ 150px ì˜ì—­ì„ ì¤‘êµ­ì–´ ìë§‰ ì˜ì—­ìœ¼ë¡œ ê°€ì •
            subtitle_height = 150
            x = 0
            y = height - subtitle_height
            w = width
            h = subtitle_height

            logger.info(f"ğŸ¤– ì¤‘êµ­ì–´ ìë§‰ ì˜ì—­ ìë™ ê°ì§€")
            logger.info(f"   ì˜ì—­: x={x}, y={y}, w={w}, h={h} (í•˜ë‹¨ {subtitle_height}px)")
            logger.info(f"   ë°©ë²•: drawbox í•„í„°ë¡œ ê²€ì€ìƒ‰ ë°•ìŠ¤ ì¶”ê°€")

            # drawbox í•„í„°ë¡œ í•˜ë‹¨ ì˜ì—­ì„ ê²€ì€ìƒ‰ìœ¼ë¡œ ì±„ìš°ê¸°
            cmd = [
                ffmpeg,
                '-i', str(input_video),
                '-vf', f"drawbox=x=0:y={y}:w={w}:h={h}:color=black:t=fill",
                '-c:a', 'copy',  # ì˜¤ë””ì˜¤ëŠ” ë³µì‚¬
                '-y',
                str(output_video)
            ]

            logger.info(f"ğŸ¬ FFmpeg ëª…ë ¹ ì‹¤í–‰ ì¤‘...")
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300
            )

            if result.returncode != 0:
                logger.error(f"âŒ ì¤‘êµ­ì–´ ìë§‰ ì œê±° ì‹¤íŒ¨: {result.stderr}")
                # ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ë³µì‚¬
                shutil.copy(input_video, output_video)
                return True

            logger.info(f"âœ… ì¤‘êµ­ì–´ ìë§‰ ì œê±° ì™„ë£Œ (ê²€ì€ìƒ‰ ë°•ìŠ¤ë¡œ ê°€ë¦¼)")
            return True
        else:
            # ì›Œí„°ë§ˆí¬ ì˜ì—­ì´ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ ì˜ì—­ ì œê±°
            x, y, w, h = watermark_region
            logger.info(f"ğŸ¤– ì›Œí„°ë§ˆí¬ ì œê±° ì¤‘ (ì§€ì • ì˜ì—­: x={x}, y={y}, w={w}, h={h})")

            cmd = [
                ffmpeg,
                '-i', str(input_video),
                '-vf', f"drawbox=x={x}:y={y}:w={w}:h={h}:color=black:t=fill",
                '-c:a', 'copy',  # ì˜¤ë””ì˜¤ëŠ” ë³µì‚¬
                '-y',
                str(output_video)
            ]

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300
            )

            if result.returncode != 0:
                logger.error(f"âŒ ì›Œí„°ë§ˆí¬ ì œê±° ì‹¤íŒ¨: {result.stderr}")
                # ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ë³µì‚¬
                shutil.copy(input_video, output_video)
                return True

            logger.info(f"âœ… ì›Œí„°ë§ˆí¬ ì œê±° ì™„ë£Œ")
            return True

    except Exception as e:
        logger.error(f"âŒ ì›Œí„°ë§ˆí¬ ì œê±° ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë³µì‚¬
        shutil.copy(input_video, output_video)
        return True


def extract_audio(video_path: Path, output_audio: Path) -> bool:
    """ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ"""
    try:
        ffmpeg = get_ffmpeg_path()
        logger.info(f"ğŸµ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘: {video_path.name}")

        cmd = [
            ffmpeg,
            '-i', str(video_path),
            '-vn',  # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì œì™¸
            '-acodec', 'pcm_s16le',  # WAV í¬ë§·
            '-ar', '16000',  # ìƒ˜í”Œë§ ë ˆì´íŠ¸
            '-ac', '1',  # ëª¨ë…¸
            '-y',  # ë®ì–´ì“°ê¸°
            str(output_audio)
        ]

        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            logger.error(f"âŒ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨: {result.stderr}")
            return False

        logger.info(f"âœ… ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ: {output_audio.name}")
        return True

    except Exception as e:
        logger.error(f"âŒ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return False


def transcribe_audio_whisper(audio_path: Path, language: str = 'zh') -> Optional[List[Dict]]:
    """Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ì „ì‚¬ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)"""
    try:
        import whisper
        logger.info(f"ğŸ¤ Whisperë¡œ ìŒì„± ì¸ì‹ ì¤‘ (ì–¸ì–´: {language})...")

        # ëª¨ë¸ ë¡œë“œ (medium ì¶”ì²œ, ì •í™•ë„ì™€ ì†ë„ ê· í˜•)
        model = whisper.load_model("medium")

        # ì „ì‚¬
        result = model.transcribe(
            str(audio_path),
            language=language,
            task='transcribe',
            verbose=False
        )

        segments = []
        for segment in result['segments']:
            segments.append({
                'start': segment['start'],
                'end': segment['end'],
                'text': segment['text'].strip()
            })

        logger.info(f"âœ… ìŒì„± ì¸ì‹ ì™„ë£Œ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
        return segments

    except ImportError:
        logger.error("âŒ whisper ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openai-whisper")
        return None
    except Exception as e:
        logger.error(f"âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {e}")
        return None


def transcribe_audio_openai(audio_path: Path) -> Optional[List[Dict]]:
    """OpenAI Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ì „ì‚¬"""
    if not OPENAI_AVAILABLE:
        logger.error("âŒ OpenAI ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    try:
        client = OpenAI()
        logger.info(f"ğŸ¤ OpenAI Whisper APIë¡œ ìŒì„± ì¸ì‹ ì¤‘...")

        with open(audio_path, 'rb') as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                response_format="verbose_json",
                timestamp_granularities=["segment"]
            )

        segments = []
        if hasattr(transcript, 'segments'):
            for segment in transcript.segments:
                segments.append({
                    'start': segment['start'],
                    'end': segment['end'],
                    'text': segment['text'].strip()
                })
        else:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ëŠ” ê²½ìš°
            segments.append({
                'start': 0,
                'end': 0,
                'text': transcript.text
            })

        logger.info(f"âœ… ìŒì„± ì¸ì‹ ì™„ë£Œ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
        return segments

    except Exception as e:
        logger.error(f"âŒ OpenAI ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {e}")
        return None


def translate_segments_claude(segments: List[Dict], source_lang: str = 'zh', target_lang: str = 'ko') -> List[Dict]:
    """Claude APIë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì¼ê´„ ë²ˆì—­ (ë¹ ë¥´ê³  ì €ë ´)"""
    if not ANTHROPIC_AVAILABLE:
        logger.error("âŒ Anthropic ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤.")
        return segments

    try:
        client = Anthropic()
        logger.info(f"ğŸŒ Claudeë¡œ ë²ˆì—­ ì¤‘: {source_lang} â†’ {target_lang} ({len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")

        # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ JSON í˜•íƒœë¡œ í•œ ë²ˆì— ë³´ë‚´ê¸°
        texts_to_translate = []
        for i, segment in enumerate(segments):
            texts_to_translate.append({
                'id': i,
                'text': segment['text']
            })

        # JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        import json
        batch_input = json.dumps(texts_to_translate, ensure_ascii=False, indent=2)

        # Claude API í˜¸ì¶œ (í•œ ë²ˆì— ì „ì²´ ë²ˆì—­)
        message = client.messages.create(
            model="claude-3-5-haiku-20241022",  # ê°€ì¥ ì €ë ´í•˜ê³  ë¹ ë¥¸ ëª¨ë¸
            max_tokens=8000,
            temperature=0.3,
            messages=[
                {
                    "role": "user",
                    "content": f"""ë‹¤ìŒ JSON ë°°ì—´ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ {source_lang}ì—ì„œ {target_lang}ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.
ê°™ì€ í˜•ì‹ì˜ JSONìœ¼ë¡œ ì‘ë‹µí•˜ë˜, text í•„ë“œë§Œ ë²ˆì—­ëœ ë‚´ìš©ìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.
ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•˜ê²Œ ë²ˆì—­í•˜ì„¸ìš”.

ì…ë ¥:
{batch_input}

ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:
[
  {{"id": 0, "text": "ë²ˆì—­ëœ í…ìŠ¤íŠ¸"}},
  {{"id": 1, "text": "ë²ˆì—­ëœ í…ìŠ¤íŠ¸"}},
  ...
]

JSONë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ ë¶™ì´ì§€ ë§ˆì„¸ìš”."""
                }
            ]
        )

        # ì‘ë‹µ íŒŒì‹±
        response_text = message.content[0].text.strip()

        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json ... ``` ë“± ì œê±°)
        if '```json' in response_text:
            response_text = response_text.split('```json')[1].split('```')[0].strip()
        elif '```' in response_text:
            response_text = response_text.split('```')[1].split('```')[0].strip()

        translated_data = json.loads(response_text)

        # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ì„¸ê·¸ë¨¼íŠ¸ì— ë§¤í•‘
        translated_segments = []
        for segment in segments:
            segment_id = segments.index(segment)
            translated_item = next((item for item in translated_data if item['id'] == segment_id), None)

            if translated_item:
                translated_text = translated_item['text']
            else:
                translated_text = segment['text']  # ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ìœ ì§€

            translated_segments.append({
                'start': segment['start'],
                'end': segment['end'],
                'original': segment['text'],
                'translated': translated_text
            })

        logger.info(f"âœ… ë²ˆì—­ ì™„ë£Œ: {len(translated_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")

        # ìƒ˜í”Œ ì¶œë ¥
        for i in range(min(3, len(translated_segments))):
            logger.info(f"  [{i+1}] {translated_segments[i]['original'][:40]}...")
            logger.info(f"      â†’ {translated_segments[i]['translated'][:40]}...")

        return translated_segments

    except Exception as e:
        logger.error(f"âŒ Claude ë²ˆì—­ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return segments


def translate_segments_openai(segments: List[Dict], source_lang: str = 'zh', target_lang: str = 'ko') -> List[Dict]:
    """OpenAI APIë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì¼ê´„ ë²ˆì—­ (ëŒ€ì²´ ì˜µì…˜)"""
    if not OPENAI_AVAILABLE:
        logger.error("âŒ OpenAI ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤.")
        return segments

    try:
        client = OpenAI()
        logger.info(f"ğŸŒ OpenAIë¡œ ë²ˆì—­ ì¤‘: {source_lang} â†’ {target_lang} ({len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")

        # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ë³´ë‚´ê¸°
        import json
        texts_to_translate = []
        for i, segment in enumerate(segments):
            texts_to_translate.append({
                'id': i,
                'text': segment['text']
            })

        batch_input = json.dumps(texts_to_translate, ensure_ascii=False, indent=2)

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": f"ë‹¹ì‹ ì€ {source_lang}ì—ì„œ {target_lang}ë¡œ ë²ˆì—­í•˜ëŠ” ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤."
                },
                {
                    "role": "user",
                    "content": f"""ë‹¤ìŒ JSON ë°°ì—´ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•˜ê³  ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{batch_input}"""
                }
            ],
            temperature=0.3
        )

        response_text = response.choices[0].message.content.strip()

        # JSON ì¶”ì¶œ
        if '```json' in response_text:
            response_text = response_text.split('```json')[1].split('```')[0].strip()
        elif '```' in response_text:
            response_text = response_text.split('```')[1].split('```')[0].strip()

        translated_data = json.loads(response_text)

        translated_segments = []
        for segment in segments:
            segment_id = segments.index(segment)
            translated_item = next((item for item in translated_data if item['id'] == segment_id), None)

            translated_segments.append({
                'start': segment['start'],
                'end': segment['end'],
                'original': segment['text'],
                'translated': translated_item['text'] if translated_item else segment['text']
            })

        logger.info(f"âœ… ë²ˆì—­ ì™„ë£Œ: {len(translated_segments)}ê°œ")
        return translated_segments

    except Exception as e:
        logger.error(f"âŒ OpenAI ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return segments


async def generate_tts_edge(text: str, output_path: Path, voice: str = 'ko-KR-SunHiNeural') -> bool:
    """Edge TTSë¡œ ìŒì„± ìƒì„±

    ì¶”ì²œ í•œêµ­ì–´ ìŒì„±:
    - ko-KR-SunHiNeural: ë°ê³  ê²½ì¾Œí•œ ì—¬ì„± ìŒì„± (ê¸°ë³¸)
    - ko-KR-JiMinNeural: ë¶€ë“œëŸ¬ìš´ ì—¬ì„± ìŒì„±
    """
    if not EDGE_TTS_AVAILABLE:
        logger.error("âŒ edge-tts ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤.")
        return False

    try:
        communicate = edge_tts.Communicate(text, voice)
        await communicate.save(str(output_path))
        return True
    except Exception as e:
        logger.error(f"âŒ Edge TTS ìƒì„± ì‹¤íŒ¨: {e}")
        return False


def generate_tts_openai(text: str, output_path: Path, voice: str = 'shimmer') -> bool:
    """OpenAI TTSë¡œ ìŒì„± ìƒì„±

    ìŒì„± ì˜µì…˜:
    - shimmer: ë°ê³  ê²½ì¾Œí•œ ì—¬ì„± ìŒì„± (ì¶”ì²œ)
    - nova: í™œê¸°ì°¬ ì—¬ì„± ìŒì„±
    - alloy: ì¤‘ì„±ì ì´ê³  ë¶€ë“œëŸ¬ìš´ ìŒì„±
    """
    if not OPENAI_AVAILABLE:
        logger.error("âŒ OpenAI ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤.")
        return False

    try:
        client = OpenAI()

        response = client.audio.speech.create(
            model="tts-1-hd",  # HD í’ˆì§ˆ ì‚¬ìš©
            voice=voice,
            input=text,
            speed=1.0  # ì†ë„ (0.25 ~ 4.0)
        )

        response.stream_to_file(str(output_path))
        return True

    except Exception as e:
        logger.error(f"âŒ OpenAI TTS ìƒì„± ì‹¤íŒ¨: {e}")
        return False


async def generate_audio_for_segments(segments: List[Dict], output_dir: Path, use_openai: bool = False) -> List[Dict]:
    """ê° ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•œ ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„± (ì‹¤ì œ ì˜¤ë””ì˜¤ ê¸¸ì´ ë°˜í™˜)"""
    logger.info(f"ğŸ¤ TTS ìƒì„± ì¤‘: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")

    audio_segments = []

    for i, segment in enumerate(segments, 1):
        audio_path = output_dir / f"segment_{i:03d}.mp3"
        text = segment.get('translated', segment.get('text', ''))

        if not text:
            logger.warning(f"âš ï¸ ì„¸ê·¸ë¨¼íŠ¸ {i}: í…ìŠ¤íŠ¸ ì—†ìŒ")
            continue

        logger.info(f"  [{i}/{len(segments)}] TTS ìƒì„±: {text[:50]}...")

        success = False
        if EDGE_TTS_AVAILABLE:
            success = await generate_tts_edge(text, audio_path)
        elif use_openai and OPENAI_AVAILABLE:
            logger.warning(f"âš ï¸ Edge TTS ì‚¬ìš© ë¶ˆê°€, OpenAI TTS ì‚¬ìš© (ìœ ë£Œ)")
            success = generate_tts_openai(text, audio_path)

        if success and audio_path.exists():
            # ì‹¤ì œ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •
            try:
                import subprocess
                ffmpeg = get_ffmpeg_path()
                result = subprocess.run(
                    [ffmpeg, '-i', str(audio_path)],
                    capture_output=True,
                    text=True
                )
                # FFmpeg stderrì—ì„œ Duration ì¶”ì¶œ
                duration_match = None
                for line in result.stderr.split('\n'):
                    if 'Duration:' in line:
                        import re
                        duration_match = re.search(r'Duration: (\d{2}):(\d{2}):(\d{2}\.\d{2})', line)
                        if duration_match:
                            h, m, s = duration_match.groups()
                            actual_duration = int(h) * 3600 + int(m) * 60 + float(s)
                            break

                if duration_match:
                    audio_segments.append({
                        'path': audio_path,
                        'text': text,
                        'original_start': segment['start'],
                        'original_end': segment['end'],
                        'actual_duration': actual_duration
                    })
                else:
                    # Durationì„ ëª» ì°¾ìœ¼ë©´ ì›ë³¸ ê¸¸ì´ ì‚¬ìš©
                    audio_segments.append({
                        'path': audio_path,
                        'text': text,
                        'original_start': segment['start'],
                        'original_end': segment['end'],
                        'actual_duration': segment['end'] - segment['start']
                    })
            except Exception as e:
                logger.warning(f"âš ï¸ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì‹¤íŒ¨ (ì›ë³¸ ê¸¸ì´ ì‚¬ìš©): {e}")
                audio_segments.append({
                    'path': audio_path,
                    'text': text,
                    'original_start': segment['start'],
                    'original_end': segment['end'],
                    'actual_duration': segment['end'] - segment['start']
                })
        else:
            logger.error(f"âŒ ì„¸ê·¸ë¨¼íŠ¸ {i} TTS ìƒì„± ì‹¤íŒ¨")

    logger.info(f"âœ… TTS ìƒì„± ì™„ë£Œ: {len(audio_segments)}ê°œ íŒŒì¼")
    return audio_segments


def merge_audio_segments(audio_segments: List[Dict], output_audio: Path) -> bool:
    """ì„¸ê·¸ë¨¼íŠ¸ ì˜¤ë””ì˜¤ë¥¼ ìˆœì„œëŒ€ë¡œ ì—°ê²°"""
    try:
        ffmpeg = get_ffmpeg_path()
        logger.info(f"ğŸ”Š ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ë³‘í•© ì¤‘...")

        if len(audio_segments) == 0:
            logger.error("âŒ ë³‘í•©í•  ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False

        # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        list_file = output_audio.parent / "audio_list.txt"
        with open(list_file, 'w', encoding='utf-8') as f:
            for seg in audio_segments:
                f.write(f"file '{seg['path'].absolute()}'\n")

        cmd = [
            ffmpeg,
            '-f', 'concat',
            '-safe', '0',
            '-i', str(list_file),
            '-c', 'copy',
            '-y',
            str(output_audio)
        ]

        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            logger.error(f"âŒ ì˜¤ë””ì˜¤ ë³‘í•© ì‹¤íŒ¨: {result.stderr}")
            return False

        logger.info(f"âœ… ì˜¤ë””ì˜¤ ë³‘í•© ì™„ë£Œ: {output_audio.name}")

        # ì •ë¦¬
        list_file.unlink()

        return True

    except Exception as e:
        logger.error(f"âŒ ì˜¤ë””ì˜¤ ë³‘í•© ì˜¤ë¥˜: {e}")
        return False


def create_simple_srt(full_text: str, audio_path: Path, output_srt: Path) -> bool:
    """ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ 25ì ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ SRT ìë§‰ ìƒì„±"""
    try:
        logger.info(f"ğŸ“ SRT ìë§‰ íŒŒì¼ ìƒì„± ì¤‘: {output_srt.name}")

        # ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •
        ffmpeg = get_ffmpeg_path()
        result = subprocess.run(
            [ffmpeg, '-i', str(audio_path)],
            capture_output=True,
            text=True
        )

        audio_duration = 0
        for line in result.stderr.split('\n'):
            if 'Duration:' in line:
                import re
                match = re.search(r'Duration: (\d{2}):(\d{2}):(\d{2}\.\d{2})', line)
                if match:
                    h, m, s = match.groups()
                    audio_duration = int(h) * 3600 + int(m) * 60 + float(s)
                    break

        if audio_duration == 0:
            logger.error("âŒ ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ ì¸¡ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False

        logger.info(f"â±ï¸ ì˜¤ë””ì˜¤ ê¸¸ì´: {audio_duration:.2f}ì´ˆ")

        # í…ìŠ¤íŠ¸ë¥¼ 25ì ë‹¨ìœ„ë¡œ ë¶„í• 
        words = full_text.split()
        chunks = []
        current_chunk = ""

        for word in words:
            test_chunk = current_chunk + (" " if current_chunk else "") + word
            if len(test_chunk) <= 25:
                current_chunk = test_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = word

        if current_chunk:
            chunks.append(current_chunk)

        if not chunks:
            chunks = [full_text]

        # ìë§‰ íƒ€ì´ë° ê³„ì‚° (ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ ì²­í¬ ìˆ˜ë¡œ ë‚˜ëˆ”)
        time_per_chunk = audio_duration / len(chunks)

        with open(output_srt, 'w', encoding='utf-8') as f:
            for i, chunk in enumerate(chunks, 1):
                start_time = (i - 1) * time_per_chunk
                end_time = i * time_per_chunk

                start_srt = format_srt_time(start_time)
                end_srt = format_srt_time(end_time)

                f.write(f"{i}\n")
                f.write(f"{start_srt} --> {end_srt}\n")
                f.write(f"{chunk}\n")
                f.write("\n")

        logger.info(f"âœ… SRT ìë§‰ ìƒì„± ì™„ë£Œ: {len(chunks)}ê°œ ìë§‰")
        return True

    except Exception as e:
        logger.error(f"âŒ SRT ìƒì„± ì‹¤íŒ¨: {e}")
        return False


def create_srt_subtitle(audio_segments: List[Dict], output_srt: Path) -> bool:
    """ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ì˜ ì‹¤ì œ ê¸¸ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ SRT ìë§‰ íŒŒì¼ ìƒì„±"""
    try:
        logger.info(f"ğŸ“ SRT ìë§‰ íŒŒì¼ ìƒì„± ì¤‘ (ì‹¤ì œ TTS ê¸¸ì´ ê¸°ì¤€): {output_srt.name}")

        with open(output_srt, 'w', encoding='utf-8') as f:
            current_time = 0.0

            for i, segment in enumerate(audio_segments, 1):
                text = segment['text']
                actual_duration = segment['actual_duration']

                if not text:
                    continue

                # ê¸´ í…ìŠ¤íŠ¸ ì¤„ë°”ê¿ˆ ì²˜ë¦¬ (25ì ì´ìƒì´ë©´ ì¤„ë°”ê¿ˆ)
                if len(text) > 25:
                    # ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ë¶„ë¦¬
                    words = text.split()
                    lines = []
                    current_line = ""

                    for word in words:
                        test_line = current_line + (" " if current_line else "") + word
                        if len(test_line) <= 25:
                            current_line = test_line
                        else:
                            if current_line:
                                lines.append(current_line)
                            current_line = word

                    if current_line:
                        lines.append(current_line)

                    # ìµœëŒ€ 2ì¤„ë¡œ ì œí•œ
                    if len(lines) > 2:
                        text = "\n".join(lines[:2])
                    else:
                        text = "\n".join(lines)

                # ì‹¤ì œ TTS ê¸¸ì´ ê¸°ë°˜ìœ¼ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
                start_time = current_time
                end_time = current_time + actual_duration

                # SRT ì‹œê°„ í˜•ì‹ ë³€í™˜ (HH:MM:SS,mmm)
                start_srt = format_srt_time(start_time)
                end_srt = format_srt_time(end_time)

                # SRT í˜•ì‹
                f.write(f"{i}\n")
                f.write(f"{start_srt} --> {end_srt}\n")
                f.write(f"{text}\n")
                f.write("\n")

                current_time = end_time

        logger.info(f"âœ… SRT ìë§‰ ìƒì„± ì™„ë£Œ: {len(audio_segments)}ê°œ ìë§‰")
        return True

    except Exception as e:
        logger.error(f"âŒ SRT ìƒì„± ì‹¤íŒ¨: {e}")
        return False


def format_srt_time(seconds: float) -> str:
    """ì´ˆë¥¼ SRT ì‹œê°„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (HH:MM:SS,mmm)"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"


def replace_video_audio_with_subtitle(
    video_path: Path,
    audio_path: Path,
    subtitle_path: Path,
    output_video: Path,
    burn_subtitle: bool = True
) -> bool:
    """ë¹„ë””ì˜¤ì˜ ì˜¤ë””ì˜¤ë¥¼ êµì²´í•˜ê³  ìë§‰ ì¶”ê°€

    ì£¼ì˜: ì›ë³¸ ì˜ìƒì— í•˜ë“œì½”ë”©ëœ ì¤‘êµ­ì–´ ìë§‰ì€ ì œê±°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    í•˜ë“œì½”ë”©ëœ ìë§‰ì€ ì˜ìƒ í”½ì…€ì— í¬í•¨ë˜ì–´ ìˆì–´ ì™„ì „íˆ ì œê±°í•˜ë ¤ë©´
    OCR + ì¸í˜ì¸íŒ… ë˜ëŠ” ì˜ìƒ í¬ë¡­ì´ í•„ìš”í•©ë‹ˆë‹¤.

    í˜„ì¬ëŠ” í•œêµ­ì–´ ìë§‰ì„ ë” í¬ê³  ì„ ëª…í•˜ê²Œ ì¶”ê°€í•˜ì—¬ ì¤‘êµ­ì–´ ìë§‰ì„ ê°€ë¦¬ë„ë¡ í•©ë‹ˆë‹¤.
    """
    try:
        ffmpeg = get_ffmpeg_path()
        logger.info(f"ğŸï¸ ì˜ìƒ í•©ì„± ì¤‘...")

        if burn_subtitle:
            # ìë§‰ì„ ë¹„ë””ì˜¤ì— í•˜ë“œì½”ë”© (burned-in)
            # Windows ê²½ë¡œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
            subtitle_path_escaped = str(subtitle_path).replace('\\', '/').replace(':', '\\:')

            # ìë§‰ ìŠ¤íƒ€ì¼ (ì ë‹¹í•œ í¬ê¸°, í•˜ë‹¨ì—ì„œ 20px ìœ„)
            subtitle_style = (
                "FontName=NanumGothic,"       # ë‚˜ëˆ”ê³ ë”•
                "Fontsize=21,"                # í°íŠ¸ í¬ê¸° (23 * 0.9 = 20.7 â‰ˆ 21)
                "Bold=1,"                     # ë³¼ë“œ
                "PrimaryColour=&H00FFFFFF,"   # í°ìƒ‰
                "OutlineColour=&H00000000,"   # ê²€ì€ í…Œë‘ë¦¬
                "BorderStyle=1,"              # ì™¸ê³½ì„  ìŠ¤íƒ€ì¼
                "Outline=2,"                  # í…Œë‘ë¦¬
                "Shadow=1,"                   # ê·¸ë¦¼ì
                "MarginV=20,"                 # í•˜ë‹¨ì—ì„œ 20px ìœ„
                "Alignment=2"                 # í•˜ë‹¨ ì¤‘ì•™
            )

            # ì¤‘êµ­ì–´ ìë§‰ ìœ„ì— ë°˜íˆ¬ëª… ê²€ì€ ë ˆì´ì–´ (50% íˆ¬ëª…ë„ = 50% ë¶ˆíˆ¬ëª…, í™”ë©´ í•˜ë‹¨ ì•½ 31%)
            # 32/125 * 1.2 = 38.4/125 (ì†Œìˆ˜ì  í‘œí˜„ì´ ì–´ë ¤ì›Œ ê·¼ì‚¬ê°’ ì‚¬ìš©: 192/625)
            video_filter = (
                f"drawbox=x=0:y=ih*433/625:w=iw:h=ih*192/625:color=black@0.5:t=fill,"  # í•˜ë‹¨ 30.7% ë°˜íˆ¬ëª… ê²€ì€ ë°•ìŠ¤ (50% íˆ¬ëª…)
                f"subtitles='{subtitle_path_escaped}':force_style='{subtitle_style}'"  # í•œêµ­ì–´ ìë§‰ (20px ìœ„)
            )

            # ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •
            result = subprocess.run(
                [ffmpeg, '-i', str(audio_path)],
                capture_output=True,
                text=True
            )
            audio_duration = 0
            for line in result.stderr.split('\n'):
                if 'Duration:' in line:
                    import re
                    match = re.search(r'Duration: (\d{2}):(\d{2}):(\d{2}\.\d{2})', line)
                    if match:
                        h, m, s = match.groups()
                        audio_duration = int(h) * 3600 + int(m) * 60 + float(s)
                        break

            logger.info(f"ğŸµ ì˜¤ë””ì˜¤ ê¸¸ì´: {audio_duration:.2f}ì´ˆ")

            # ì˜ìƒì„ ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ì¶”ê¸° (loop ì‚¬ìš©)
            cmd = [
                ffmpeg,
                '-stream_loop', '-1',  # ë¹„ë””ì˜¤ ë¬´í•œ ë°˜ë³µ
                '-i', str(video_path),
                '-i', str(audio_path),
                '-c:v', 'libx264',     # ë¹„ë””ì˜¤ ì¬ì¸ì½”ë”©
                '-preset', 'medium',   # ì¸ì½”ë”© ì†ë„/í’ˆì§ˆ ê· í˜•
                '-crf', '23',          # í’ˆì§ˆ ì„¤ì • (ë‚®ì„ìˆ˜ë¡ ê³ í’ˆì§ˆ)
                '-c:a', 'aac',         # ì˜¤ë””ì˜¤ AAC ì¸ì½”ë”©
                '-b:a', '192k',        # ì˜¤ë””ì˜¤ ë¹„íŠ¸ë ˆì´íŠ¸
                '-vf', video_filter,   # ë¹„ë””ì˜¤ í•„í„° (ì¤‘êµ­ì–´ ìë§‰ ì œê±° + í•œêµ­ì–´ ìë§‰ ì¶”ê°€)
                '-map', '0:v:0',       # ì²« ë²ˆì§¸ ì…ë ¥ì˜ ë¹„ë””ì˜¤
                '-map', '1:a:0',       # ë‘ ë²ˆì§¸ ì…ë ¥ì˜ ì˜¤ë””ì˜¤
                '-sn',                 # ì›ë³¸ ìë§‰ ìŠ¤íŠ¸ë¦¼ ì œê±° (ì†Œí”„íŠ¸ ìë§‰ë§Œ)
                '-shortest',           # ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ì¶¤ (ì˜¤ë””ì˜¤ê°€ ëë‚˜ë©´ ì˜ìƒë„ ë)
                '-y',
                str(output_video)
            ]
        else:
            # ì†Œí”„íŠ¸ ìë§‰ (ë³„ë„ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì¶”ê°€)
            cmd = [
                ffmpeg,
                '-i', str(video_path),
                '-i', str(audio_path),
                '-i', str(subtitle_path),
                '-c:v', 'copy',        # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ë³µì‚¬
                '-c:a', 'aac',         # ì˜¤ë””ì˜¤ AAC ì¸ì½”ë”©
                '-c:s', 'mov_text',    # ìë§‰ ì¸ì½”ë”©
                '-map', '0:v:0',       # ì²« ë²ˆì§¸ ì…ë ¥ì˜ ë¹„ë””ì˜¤
                '-map', '1:a:0',       # ë‘ ë²ˆì§¸ ì…ë ¥ì˜ ì˜¤ë””ì˜¤
                '-map', '2:s:0',       # ì„¸ ë²ˆì§¸ ì…ë ¥ì˜ ìë§‰
                '-metadata:s:s:0', 'language=kor',
                '-shortest',
                '-y',
                str(output_video)
            ]

        logger.info(f"ğŸ“¹ FFmpeg ëª…ë ¹ ì‹¤í–‰ ì¤‘...")
        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            logger.error(f"âŒ ì˜ìƒ í•©ì„± ì‹¤íŒ¨: {result.stderr}")
            return False

        logger.info(f"âœ… ì˜ìƒ í•©ì„± ì™„ë£Œ: {output_video.name}")
        return True

    except Exception as e:
        logger.error(f"âŒ ì˜ìƒ í•©ì„± ì˜¤ë¥˜: {e}")
        return False


async def convert_chinese_video(
    input_video: Path,
    output_dir: Path,
    use_openai_whisper: bool = False,
    use_openai_tts: bool = False,  # Edge TTS ì‚¬ìš© (ë¬´ë£Œ)
    use_claude: bool = True
) -> Optional[Path]:
    """ì¤‘êµ­ì–´ ì˜ìƒì„ í•œêµ­ì–´ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""

    logger.info("=" * 60)
    logger.info("ğŸ‡¨ğŸ‡³ â†’ ğŸ‡°ğŸ‡· ì¤‘êµ­ì–´ ì˜ìƒ ë³€í™˜ ì‹œì‘")
    logger.info("=" * 60)
    logger.info(f"ì…ë ¥: {input_video.name}")
    logger.info(f"ì¶œë ¥ í´ë”: {output_dir}")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir.mkdir(parents=True, exist_ok=True)
    temp_dir = output_dir / "temp"
    temp_dir.mkdir(exist_ok=True)

    try:
        # 1. ì˜¤ë””ì˜¤ ì¶”ì¶œ
        logger.info("\n" + "=" * 60)
        logger.info("1ï¸âƒ£ ë‹¨ê³„ 1: ì˜¤ë””ì˜¤ ì¶”ì¶œ")
        logger.info("=" * 60)

        audio_path = temp_dir / "original_audio.wav"
        if not extract_audio(input_video, audio_path):
            logger.error("âŒ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨")
            return None

        # 2. ìŒì„± ì¸ì‹ (ì¤‘êµ­ì–´)
        logger.info("\n" + "=" * 60)
        logger.info("2ï¸âƒ£ ë‹¨ê³„ 2: ì¤‘êµ­ì–´ ìŒì„± ì¸ì‹")
        logger.info("=" * 60)

        if use_openai_whisper:
            segments = transcribe_audio_openai(audio_path)
        else:
            segments = transcribe_audio_whisper(audio_path, language='zh')

        if not segments:
            logger.error("âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨")
            return None

        # ìë§‰ ì €ì¥
        transcript_file = output_dir / "chinese_transcript.json"
        with open(transcript_file, 'w', encoding='utf-8') as f:
            json.dump(segments, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ì¤‘êµ­ì–´ ìë§‰ ì €ì¥: {transcript_file.name}")

        # 3. ë²ˆì—­ (ì¤‘êµ­ì–´ â†’ í•œêµ­ì–´)
        logger.info("\n" + "=" * 60)
        logger.info("3ï¸âƒ£ ë‹¨ê³„ 3: ë²ˆì—­ (ì¤‘êµ­ì–´ â†’ í•œêµ­ì–´)")
        logger.info("=" * 60)

        if use_claude and ANTHROPIC_AVAILABLE:
            translated_segments = translate_segments_claude(segments, source_lang='zh', target_lang='ko')
        elif OPENAI_AVAILABLE:
            translated_segments = translate_segments_openai(segments, source_lang='zh', target_lang='ko')
        else:
            logger.error("âŒ ë²ˆì—­ APIê°€ ì—†ìŠµë‹ˆë‹¤ (Claude ë˜ëŠ” OpenAI í•„ìš”)")
            return None

        # ë²ˆì—­ ì €ì¥
        translation_file = output_dir / "korean_translation.json"
        with open(translation_file, 'w', encoding='utf-8') as f:
            json.dump(translated_segments, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ í•œêµ­ì–´ ë²ˆì—­ ì €ì¥: {translation_file.name}")

        # 4. ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ TTSë¡œ ìƒì„±
        logger.info("\n" + "=" * 60)
        logger.info("4ï¸âƒ£ ë‹¨ê³„ 4: í•œêµ­ì–´ ìŒì„± ìƒì„± (ì „ì²´)")
        logger.info("=" * 60)

        # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ì „ì²´ í•©ì¹˜ê¸° (ê° ì„¸ê·¸ë¨¼íŠ¸ ì‚¬ì´ì— 24ê°œ ì¤„ë°”ê¿ˆ ì¶”ê°€)
        newline_separator = '\n' * 24
        full_korean_text = newline_separator.join([seg.get('translated', '') for seg in translated_segments])
        logger.info(f"ğŸ“ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_korean_text)}ì")
        logger.info(f"ğŸ“ ë¯¸ë¦¬ë³´ê¸°: {full_korean_text[:100]}...")
        logger.info(f"ğŸ« ìˆ¨ì‰¬ëŠ” í¬ì¸íŠ¸ ì¶”ê°€ ì™„ë£Œ (ì„¸ê·¸ë¨¼íŠ¸ ì‚¬ì´ë§ˆë‹¤ 24ê°œ ì¤„ë°”ê¿ˆ)")

        # í•˜ë‚˜ì˜ TTS íŒŒì¼ë¡œ ìƒì„±
        korean_audio_path = temp_dir / "korean_audio.mp3"

        if EDGE_TTS_AVAILABLE:
            logger.info("ğŸ¤ Edge TTSë¡œ ìŒì„± ìƒì„± ì¤‘...")
            success = await generate_tts_edge(full_korean_text, korean_audio_path)
        elif use_openai_tts and OPENAI_AVAILABLE:
            logger.warning("âš ï¸ Edge TTS ì‚¬ìš© ë¶ˆê°€, OpenAI TTS ì‚¬ìš© (ìœ ë£Œ)")
            success = generate_tts_openai(full_korean_text, korean_audio_path)
        else:
            logger.error("âŒ TTS ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤")
            return None

        if not success or not korean_audio_path.exists():
            logger.error("âŒ TTS ìƒì„± ì‹¤íŒ¨")
            return None

        logger.info(f"âœ… TTS ìƒì„± ì™„ë£Œ: {korean_audio_path.name}")

        # 5. ê°„ë‹¨í•œ ìë§‰ ìƒì„± (ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ 25ì ë‹¨ìœ„ë¡œ ë¶„í• )
        logger.info("\n" + "=" * 60)
        logger.info("5ï¸âƒ£ ë‹¨ê³„ 5: í•œêµ­ì–´ ìë§‰ ìƒì„±")
        logger.info("=" * 60)

        subtitle_path = output_dir / "korean_subtitle.srt"
        if not create_simple_srt(full_korean_text, korean_audio_path, subtitle_path):
            logger.error("âŒ ìë§‰ ìƒì„± ì‹¤íŒ¨")
            return None

        # 6. ì˜ìƒ í•©ì„± (í•œêµ­ì–´ ìë§‰ + ì˜¤ë””ì˜¤)
        logger.info("\n" + "=" * 60)
        logger.info("6ï¸âƒ£ ë‹¨ê³„ 6: ì˜ìƒ í•©ì„±")
        logger.info("=" * 60)

        # ì›ë³¸ ë¹„ë””ì˜¤ ê²½ë¡œ ì‚¬ìš© (ì›Œí„°ë§ˆí¬ ì œê±° ì•ˆ í•¨)
        output_video = output_dir / f"converted_{input_video.stem}.mp4"
        if not replace_video_audio_with_subtitle(
            input_video,
            korean_audio_path,
            subtitle_path,
            output_video,
            burn_subtitle=True  # ìë§‰ì„ ë¹„ë””ì˜¤ì— í•˜ë“œì½”ë”©
        ):
            logger.error("âŒ ì˜ìƒ í•©ì„± ì‹¤íŒ¨")
            return None

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        logger.info("\nğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘...")
        shutil.rmtree(temp_dir, ignore_errors=True)

        logger.info("\n" + "=" * 60)
        logger.info("âœ… ë³€í™˜ ì™„ë£Œ!")
        logger.info("=" * 60)
        logger.info(f"ì¶œë ¥ íŒŒì¼: {output_video}")

        return output_video

    except Exception as e:
        logger.error(f"\nâŒ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    parser = argparse.ArgumentParser(description='ì¤‘êµ­ì–´ ì˜ìƒì„ í•œêµ­ì–´ë¡œ ë³€í™˜')
    parser.add_argument('--input', type=str, required=True, help='ì…ë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output-dir', type=str, required=True, help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--use-openai-whisper', action='store_true', help='OpenAI Whisper API ì‚¬ìš© (ê¸°ë³¸: ë¡œì»¬ whisper)')
    parser.add_argument('--use-edge-tts', action='store_true', help='Edge TTS ì‚¬ìš© (ê¸°ë³¸: OpenAI TTS)')
    parser.add_argument('--use-openai-translate', action='store_true', help='OpenAIë¡œ ë²ˆì—­ (ê¸°ë³¸: Claude)')

    args = parser.parse_args()

    input_video = Path(args.input)
    output_dir = Path(args.output_dir)

    if not input_video.exists():
        logger.error(f"âŒ ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_video}")
        return

    # í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸
    logger.info("\nğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€:")
    logger.info("  - openai-whisper (ìŒì„± ì¸ì‹)")
    logger.info("  - anthropic (ë²ˆì—­ - ì €ë ´)")
    logger.info("  - openai (TTS)")
    logger.info("  - edge-tts (TTS ëŒ€ì²´)")
    logger.info("\nì„¤ì¹˜: pip install openai-whisper anthropic openai edge-tts\n")

    # ë¹„ë™ê¸° ì‹¤í–‰
    result = asyncio.run(convert_chinese_video(
        input_video,
        output_dir,
        use_openai_whisper=args.use_openai_whisper,
        use_openai_tts=not args.use_edge_tts,
        use_claude=not args.use_openai_translate
    ))

    if result:
        logger.info(f"\nâœ… ì„±ê³µ: {result}")
    else:
        logger.error("\nâŒ ë³€í™˜ ì‹¤íŒ¨")
        sys.exit(1)


if __name__ == '__main__':
    main()
