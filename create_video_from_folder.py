"""
í´ë”ì— ìˆëŠ” story.jsonê³¼ ì´ë¯¸ì§€ë“¤ë¡œ ì˜ìƒì„ ìë™ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python create_video_from_folder.py --folder "path/to/folder"

í´ë” êµ¬ì¡°:
    folder/
        story.json (ë˜ëŠ” story_metadata.json)
        scene_01_image.png
        scene_02_image.png
        ...
"""

import os
import sys
import json
import argparse
import logging
from pathlib import Path
import warnings
from time import time
import signal

# .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # dotenvê°€ ì—†ì–´ë„ ê³„ì† ì§„í–‰

# ë²„ì „ í˜¸í™˜ì„± ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore", message="Model was trained with")
warnings.filterwarnings("ignore", message="Lightning automatically upgraded")
warnings.filterwarnings("ignore", message="torchaudio._backend")
warnings.filterwarnings("ignore", category=UserWarning, module="pyannote")
warnings.filterwarnings("ignore", category=UserWarning, module="speechbrain")
warnings.filterwarnings("ignore", category=UserWarning, module="pytorch_lightning")

# ë¡œê¹… ë ˆë²¨ ì¡°ì • (pytorch_lightning INFO ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°)
import logging as base_logging
base_logging.getLogger("pytorch_lightning").setLevel(base_logging.WARNING)
from typing import Dict, List, Optional
import edge_tts
import asyncio

# Google Cloud TTS (ì„ íƒì )
try:
    from google.cloud import texttospeech
    GOOGLE_TTS_AVAILABLE = True
except ImportError:
    GOOGLE_TTS_AVAILABLE = False
    logger_msg = "âš ï¸ google-cloud-texttospeech íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. pip install google-cloud-texttospeech"

# AWS Polly (ì„ íƒì )
try:
    import boto3
    from botocore.exceptions import BotoCoreError, ClientError
    AWS_POLLY_AVAILABLE = True
except ImportError:
    AWS_POLLY_AVAILABLE = False
    logger_msg = "âš ï¸ boto3 íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. pip install boto3"
from moviepy.editor import ImageClip, AudioFileClip, CompositeVideoClip, concatenate_videoclips, VideoFileClip
import re
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing
import tempfile
from PIL import Image as PILImage
import numpy as np

# OpenCV ì„í¬íŠ¸ ì‹œë„ (ì–¼êµ´ ê°ì§€ìš©)
try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False
    logger_msg = "âš ï¸ OpenCVê°€ ì—†ìŠµë‹ˆë‹¤. ì–¼êµ´ ê°ì§€ ì—†ì´ ì¤‘ì•™ í¬ë¡­ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì„¤ì¹˜: pip install opencv-python"

# ë¡œê¹… ì„¤ì • (ë¨¼ì € ì„¤ì •)
# Windowsì—ì„œ UTF-8 ì¶œë ¥ì„ ìœ„í•´ stdoutì„ UTF-8ë¡œ ì¬ì„¤ì •
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/video_from_folder.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Global cancellation flag and child processes tracking
cancellation_requested = False
child_processes = []

def signal_handler(signum, frame):
    """Handle SIGTERM/SIGINT for graceful shutdown"""
    global cancellation_requested, child_processes
    logger.info("ğŸ›‘ ì·¨ì†Œ ì‹œê·¸ë„ ìˆ˜ì‹ , ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤...")
    cancellation_requested = True

    # ëª¨ë“  ìì‹ í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì¢…ë£Œ
    for proc in child_processes:
        try:
            if proc.poll() is None:  # ì•„ì§ ì‹¤í–‰ ì¤‘
                logger.info(f"ğŸ›‘ ìì‹ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì¤‘: PID {proc.pid}")
                proc.kill()  # SIGKILL
                proc.wait(timeout=2)
        except Exception as e:
            logger.error(f"âŒ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹¤íŒ¨: {e}")

    sys.exit(1)

# DALL-E (ì˜µì…˜)
try:
    from openai import OpenAI
    DALLE_AVAILABLE = True
except ImportError:
    DALLE_AVAILABLE = False
    logger.warning("[WARNING] openai module not found. DALL-E image generation disabled.")

# Anthropic Claude API (í”„ë¡¬í”„íŠ¸ ìˆ˜ì •ìš©)
try:
    from anthropic import Anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    logger.warning("[WARNING] anthropic module not found. Claude prompt refinement disabled.")


class VideoFromFolderCreator:
    """story.jsonê³¼ ì´ë¯¸ì§€ë¡œ ì˜ìƒì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, folder_path: str, voice: str = "ko-KR-SoonBokNeural",
                 aspect_ratio: str = "16:9", add_subtitles: bool = False,
                 image_source: str = "none", image_provider: str = "openai", is_admin: bool = False):
        """
        Args:
            folder_path: story.jsonê³¼ ì´ë¯¸ì§€ê°€ ìˆëŠ” í´ë” ê²½ë¡œ
            voice: TTS ìŒì„± (ê¸°ë³¸: ko-KR-SoonBokNeural)
            aspect_ratio: ë¹„ë””ì˜¤ ë¹„ìœ¨ (ê¸°ë³¸: 16:9)
            add_subtitles: ìë§‰ ì¶”ê°€ ì—¬ë¶€ (ê¸°ë³¸: False)
            image_source: ì´ë¯¸ì§€ ì†ŒìŠ¤ ("none", "dalle", "imagen3")
            image_provider: ì´ë¯¸ì§€ ìƒì„± ì œê³µì ("openai", "imagen3")
            is_admin: ê´€ë¦¬ì ëª¨ë“œ (ë¹„ìš© ë¡œê·¸ í‘œì‹œ)
        """
        self.folder_path = Path(folder_path)

        # TTS ì œê³µì ê²°ì •
        self.voice = voice
        if voice.startswith('google-'):
            self.tts_provider = 'google'
            if not GOOGLE_TTS_AVAILABLE:
                logger.warning(f"âš ï¸ Google Cloud TTS íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. Edge TTSë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                self.tts_provider = 'edge'
                self.voice = "ko-KR-SoonBokNeural"
        elif voice.startswith('aws-'):
            self.tts_provider = 'aws'
            if not AWS_POLLY_AVAILABLE:
                logger.warning(f"âš ï¸ AWS Polly íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. Edge TTSë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                self.tts_provider = 'edge'
                self.voice = "ko-KR-SoonBokNeural"
        else:
            self.tts_provider = 'edge'

        self.aspect_ratio = aspect_ratio
        self.add_subtitles = add_subtitles
        self.image_source = image_source.lower()
        self.image_provider = image_provider.lower()
        self.is_admin = is_admin

        # ì´ë¯¸ì§€ ìƒì„± í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.dalle_client = None
        self.imagen_client = None
        self.imagen_model = None
        self.imagen_api_key = None
        self.anthropic_client = None

        if self.image_source == "dalle":
            if not DALLE_AVAILABLE:
                logger.error("âŒ openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openai")
                self.image_source = "none"
            else:
                import os
                api_key = os.getenv('OPENAI_API_KEY')
                if not api_key:
                    logger.error("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    self.image_source = "none"
                else:
                    self.dalle_client = OpenAI(api_key=api_key)
                    logger.info("âœ… DALL-E 3 ì´ë¯¸ì§€ ìƒì„± í™œì„±í™”ë¨")

                    # Anthropic Claude í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í”„ë¡¬í”„íŠ¸ ìˆ˜ì •ìš©)
                    if ANTHROPIC_AVAILABLE:
                        anthropic_key = os.getenv('ANTHROPIC_API_KEY')
                        if anthropic_key:
                            self.anthropic_client = Anthropic(api_key=anthropic_key)
                            logger.info("âœ… Claude API í™œì„±í™”ë¨ (í”„ë¡¬í”„íŠ¸ ìë™ ìˆ˜ì •)")
                        else:
                            logger.warning("âš ï¸ ANTHROPIC_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ìë™ ìˆ˜ì • ë¹„í™œì„±í™”")

        elif self.image_source == "imagen3":
            # Imagen 3 ì´ˆê¸°í™” (Vertex AI ì‚¬ìš©)
            import os

            # Google Cloud í”„ë¡œì íŠ¸ ì„¤ì •
            project_id = os.getenv('GOOGLE_CLOUD_PROJECT', '66255489700')
            location = os.getenv('GOOGLE_CLOUD_LOCATION', 'us-central1')

            try:
                from vertexai.preview.vision_models import ImageGenerationModel
                import vertexai

                # Vertex AI ì´ˆê¸°í™”
                vertexai.init(project=project_id, location=location)

                # Imagen 3 ëª¨ë¸ ë¡œë“œ
                self.imagen_model = ImageGenerationModel.from_pretrained("imagegeneration@006")
                self.imagen_client = None
                self.imagen_api_key = None

                logger.info(f"âœ… Google Imagen 3 ì´ë¯¸ì§€ ìƒì„± í™œì„±í™”ë¨ (Vertex AI)")
                logger.info(f"â„¹ï¸  í”„ë¡œì íŠ¸: {project_id}, ë¦¬ì „: {location}")

                # Anthropic Claude í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í”„ë¡¬í”„íŠ¸ ìˆ˜ì •ìš©)
                if ANTHROPIC_AVAILABLE:
                    anthropic_key = os.getenv('ANTHROPIC_API_KEY')
                    if anthropic_key:
                        self.anthropic_client = Anthropic(api_key=anthropic_key)
                        logger.info("âœ… Claude API í™œì„±í™”ë¨ (í”„ë¡¬í”„íŠ¸ ìë™ ìˆ˜ì •)")
                    else:
                        logger.warning("âš ï¸ ANTHROPIC_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ìë™ ìˆ˜ì • ë¹„í™œì„±í™”")
            except ImportError:
                logger.error("âŒ Vertex AI SDKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install google-cloud-aiplatform")
                self.image_source = "none"
            except Exception as e:
                logger.error(f"âŒ Vertex AI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                logger.error("ğŸ’¡ Application Default Credentialsë¥¼ ì„¤ì •í•˜ê±°ë‚˜ GOOGLE_APPLICATION_CREDENTIALS í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
                self.image_source = "none"

        # ë¹„ìœ¨ íŒŒì‹±
        if aspect_ratio == "9:16":
            self.width, self.height = 1080, 1920
        elif aspect_ratio == "16:9":
            self.width, self.height = 1920, 1080
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¹„ìœ¨: {aspect_ratio}")

        # story.json ë¡œë“œ
        self.story_data = self._load_story_json()

        # ì¸ë„¤ì¼ ìë™ ìƒì„±
        self._create_thumbnail()

        # GPU ì¸ì½”ë” ê°ì§€
        self.video_codec, self.codec_preset = self._detect_best_encoder()

        # Whisper ëª¨ë¸ ìºì‹± (í•œ ë²ˆë§Œ ë¡œë“œ)
        self._whisper_model = None

    def _detect_best_encoder(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ìµœê³ ì˜ ë¹„ë””ì˜¤ ì¸ì½”ë” ê°ì§€"""
        try:
            # ffmpegì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ì½”ë” ëª©ë¡ í™•ì¸
            result = subprocess.run(
                ['ffmpeg', '-encoders'],
                capture_output=True,
                text=True,
                timeout=5
            )
            encoders = result.stdout

            # NVIDIA GPU ì¸ì½”ë” (ê°€ì¥ ë¹ ë¦„)
            if 'h264_nvenc' in encoders:
                # ë“œë¼ì´ë²„ ë²„ì „ ì²´í¬
                try:
                    driver_check = subprocess.run(
                        ['nvidia-smi', '--query-gpu=driver_version', '--format=csv,noheader'],
                        capture_output=True,
                        text=True,
                        timeout=5
                    )
                    driver_version = driver_check.stdout.strip()
                    driver_major = int(driver_version.split('.')[0])

                    if driver_major >= 570:
                        logger.info(f"âœ“ NVIDIA GPU ì¸ì½”ë” ì‚¬ìš© (h264_nvenc, ë“œë¼ì´ë²„ {driver_version})")
                        return 'h264_nvenc', 'p4'
                    else:
                        logger.warning(f"âš ï¸  NVIDIA ë“œë¼ì´ë²„ê°€ ë‚®ìŒ ({driver_version} < 570.0), CPU ì¸ì½”ë” ì‚¬ìš©")
                        logger.info("   ë“œë¼ì´ë²„ ì—…ë°ì´íŠ¸: https://www.nvidia.com/Download/index.aspx")
                except:
                    # ë“œë¼ì´ë²„ ì²´í¬ ì‹¤íŒ¨ ì‹œ GPU ì‹œë„ (í´ë°± ìˆìŒ)
                    logger.info("âœ“ NVIDIA GPU ì¸ì½”ë” ê°ì§€ (h264_nvenc) - ë“œë¼ì´ë²„ ë²„ì „ í™•ì¸ ì‹¤íŒ¨, ì‹œë„í•©ë‹ˆë‹¤")
                    return 'h264_nvenc', 'p4'

            # Intel QSV ì¸ì½”ë”
            if 'h264_qsv' in encoders:
                logger.info("âœ“ Intel QSV GPU ì¸ì½”ë” ì‚¬ìš© (h264_qsv)")
                return 'h264_qsv', 'fast'

            # AMD GPU ì¸ì½”ë”
            if 'h264_amf' in encoders:
                logger.info("âœ“ AMD GPU ì¸ì½”ë” ì‚¬ìš© (h264_amf)")
                return 'h264_amf', 'speed'

        except Exception as e:
            logger.warning(f"GPU ì¸ì½”ë” ê°ì§€ ì‹¤íŒ¨: {e}")

        # í´ë°±: CPU ì¸ì½”ë”
        logger.info("âœ— GPU ì¸ì½”ë” ì—†ìŒ. CPU ì¸ì½”ë” ì‚¬ìš© (libx264)")
        return 'libx264', 'ultrafast'

    def _load_story_json(self) -> Dict:
        """storyë¡œ ì‹œì‘í•˜ëŠ” JSON íŒŒì¼ ë¡œë“œ"""
        # ê²½ë¡œ ì •ê·œí™” (ë”°ì˜´í‘œ ì œê±°)
        folder_str = str(self.folder_path).strip('"').strip("'")
        self.folder_path = Path(folder_str)

        logger.info(f"í´ë” ê²½ë¡œ: {self.folder_path}")
        logger.info(f"í´ë” ì¡´ì¬ ì—¬ë¶€: {self.folder_path.exists()}")

        # storyê°€ í¬í•¨ëœ ëª¨ë“  JSON íŒŒì¼ ì°¾ê¸°
        story_files = list(self.folder_path.glob("*story*.json"))

        logger.info(f"ì°¾ì€ *story*.json íŒŒì¼: {[f.name for f in story_files]}")

        if story_files:
            # íŒŒì¼ì´ ì—¬ëŸ¬ ê°œë©´ ì²« ë²ˆì§¸ ì‚¬ìš©
            json_path = story_files[0]
            logger.info(f"JSON íŒŒì¼ ë¡œë“œ: {json_path.name}")
            with open(json_path, 'r', encoding='utf-8') as f:
                json_text = f.read()

                # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (```json ... ``` í˜•ì‹)
                json_text = re.sub(r'^```json\s*', '', json_text, flags=re.IGNORECASE)
                json_text = re.sub(r'\s*```\s*$', '', json_text)
                json_text = json_text.strip()

                # JSON íŒŒì‹± ì‹œë„
                try:
                    return json.loads(json_text)
                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨, ìë™ ìˆ˜ì • ì‹œë„ ì¤‘... (ì›ì¸: {e})")

                    # JSON ë¬¸ìì—´ ê°’ ë‚´ë¶€ì˜ ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ ë”°ì˜´í‘œ ìˆ˜ì •
                    # "text": "He said "hello"" -> "text": "He said \"hello\""
                    def fix_quotes(match):
                        key = match.group(1)
                        value = match.group(2)
                        # ê°’ ë‚´ë¶€ì˜ ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ ë”°ì˜´í‘œë¥¼ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                        # ì´ë¯¸ ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œ(\")ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ
                        fixed_value = re.sub(r'(?<!\\)"', r'\\"', value)
                        return f'"{key}": "{fixed_value}"'

                    # "key": "value" íŒ¨í„´ì„ ì°¾ì•„ì„œ value ë‚´ë¶€ì˜ ë”°ì˜´í‘œ ìˆ˜ì •
                    # ë‹¨, value ëì˜ ë”°ì˜´í‘œëŠ” ìœ ì§€
                    json_text_fixed = re.sub(
                        r'"([^"]+)"\s*:\s*"((?:[^"\\]|\\.)*)(?<!\\)"',
                        fix_quotes,
                        json_text
                    )

                    try:
                        logger.info("âœ… JSON ìë™ ìˆ˜ì • ì„±ê³µ")
                        return json.loads(json_text_fixed)
                    except json.JSONDecodeError as e2:
                        logger.error(f"âŒ JSON ìë™ ìˆ˜ì • ì‹¤íŒ¨: {e2}")
                        # ì›ë³¸ ì—ëŸ¬ ë°œìƒ
                        raise e

        raise FileNotFoundError(f"{self.folder_path}ì— 'story'ê°€ í¬í•¨ëœ JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    def _create_thumbnail(self):
        """ê¸€ì”¨ ì“´ ìœ íŠœë¸Œìš© ì¸ë„¤ì¼ ì œì‘ (í•­ìƒ ì‹¤í–‰)"""
        try:
            logger.info("ğŸ–¼ï¸  ì¸ë„¤ì¼ ì œì‘ ì¤‘... (ê¸€ì”¨ ì“°ê¸°)")

            # create_thumbnail.pyë¥¼ subprocessë¡œ ì‹¤í–‰
            # - ì—…ë¡œë“œëœ thumbnail.* íŒŒì¼ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©
            # - ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì”¬ ì´ë¯¸ì§€ ì‚¬ìš©
            import subprocess

            thumbnail_script = Path(__file__).parent / "create_thumbnail.py"

            if not thumbnail_script.exists():
                logger.warning(f"ì¸ë„¤ì¼ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {thumbnail_script}")
                return

            result = subprocess.run(
                [sys.executable, str(thumbnail_script), "-f", str(self.folder_path)],
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='ignore'
            )

            if result.returncode == 0:
                logger.info("âœ… ì¸ë„¤ì¼ ìƒì„± ì™„ë£Œ (ê¸€ì”¨ ì“´ ë²„ì „)")
                if result.stdout:
                    logger.info(f"ì¸ë„¤ì¼ ì¶œë ¥: {result.stdout[:200]}")
            else:
                logger.error(f"ì¸ë„¤ì¼ ìƒì„± ì‹¤íŒ¨ (return code: {result.returncode})")
                if result.stderr:
                    logger.error(f"ì¸ë„¤ì¼ ì—ëŸ¬: {result.stderr}")
                if result.stdout:
                    logger.error(f"ì¸ë„¤ì¼ ì¶œë ¥: {result.stdout}")

        except Exception as e:
            logger.error(f"ì¸ë„¤ì¼ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            import traceback
            logger.error(traceback.format_exc())

    def _detect_focus_area(self, image_path: Path) -> Optional[tuple]:
        """
        ì´ë¯¸ì§€ì—ì„œ ì¸ë¬¼ì´ë‚˜ ì£¼ìš” ë¬¼ì²´ë¥¼ ê°ì§€í•˜ì—¬ ì¤‘ì‹¬ ì¢Œí‘œ ë°˜í™˜
        OpenCV Haar Cascadeë¡œ ì–¼êµ´ ê°ì§€

        Returns:
            (center_x, center_y) ë˜ëŠ” None (ê°ì§€ ì‹¤íŒ¨ ì‹œ)
        """
        if not OPENCV_AVAILABLE:
            return None

        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            img = cv2.imread(str(image_path))
            if img is None:
                return None

            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

            # Haar Cascadeë¡œ ì–¼êµ´ ê°ì§€
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            face_cascade = cv2.CascadeClassifier(cascade_path)

            # ì–¼êµ´ ê°ì§€
            faces = face_cascade.detectMultiScale(
                gray,
                scaleFactor=1.1,
                minNeighbors=5,
                minSize=(30, 30)
            )

            if len(faces) > 0:
                # ê°€ì¥ í° ì–¼êµ´ì„ ì£¼ìš” ì¸ë¬¼ë¡œ ì„ íƒ
                largest_face = max(faces, key=lambda f: f[2] * f[3])
                x, y, w, h = largest_face

                # ì–¼êµ´ ì¤‘ì‹¬ ì¢Œí‘œ
                center_x = x + w // 2
                center_y = y + h // 2

                logger.info(f"  âœ… ì–¼êµ´ ê°ì§€ë¨: ({center_x}, {center_y}), í¬ê¸°: {w}x{h}")
                return (center_x, center_y)

            logger.info(f"  â„¹ï¸ ì–¼êµ´ ë¯¸ê°ì§€ (ì¤‘ì•™ í¬ë¡­ ì‚¬ìš©)")
            return None

        except Exception as e:
            logger.warning(f"  âš ï¸ ì–¼êµ´ ê°ì§€ ì‹¤íŒ¨: {e}")
            return None

    def _smart_crop_to_vertical(self, input_path: Path, output_path: Path) -> bool:
        """
        ê°€ë¡œ ì´ë¯¸ì§€(16:9)ë¥¼ ì„¸ë¡œ(9:16)ë¡œ ìŠ¤ë§ˆíŠ¸ í¬ë¡­ ë³€í™˜
        ì–¼êµ´ì´ ê°ì§€ë˜ë©´ ì–¼êµ´ ì¤‘ì‹¬ìœ¼ë¡œ í¬ë¡­, ì•„ë‹ˆë©´ ì¤‘ì•™ í¬ë¡­

        Args:
            input_path: ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ
            output_path: ì¶œë ¥ ì´ë¯¸ì§€ ê²½ë¡œ

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ì–¼êµ´/ë¬¼ì²´ ê°ì§€
            focus_point = self._detect_focus_area(input_path)

            with PILImage.open(input_path) as img:
                width, height = img.size
                logger.info(f"  ğŸ¨ ìŠ¤ë§ˆíŠ¸ í¬ë¡­: ì›ë³¸ {width}x{height}")

                # ëª©í‘œ ë¹„ìœ¨: 9:16 (ì„¸ë¡œ)
                target_ratio = 9 / 16

                # í˜„ì¬ ë†’ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 9:16 ë¹„ìœ¨ì˜ ë„ˆë¹„ ê³„ì‚°
                new_width = int(height * target_ratio)

                if new_width > width:
                    # ë†’ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•œ ë„ˆë¹„ê°€ ì›ë³¸ë³´ë‹¤ í¬ë©´, ë„ˆë¹„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¬ê³„ì‚°
                    new_height = int(width / target_ratio)
                    new_width = width

                    # ì–¼êµ´ì´ ê°ì§€ë˜ë©´ ì–¼êµ´ ì¤‘ì‹¬ìœ¼ë¡œ, ì•„ë‹ˆë©´ ìƒë‹¨ í¬ë¡­
                    if focus_point:
                        focus_x, focus_y = focus_point
                        center_y = focus_y
                        top = max(0, center_y - new_height // 2)
                        bottom = min(height, top + new_height)

                        if bottom > height:
                            bottom = height
                            top = bottom - new_height
                        if top < 0:
                            top = 0
                            bottom = new_height

                        logger.info(f"  âœ¨ ì–¼êµ´ ì¤‘ì‹¬ í¬ë¡­: y={center_y}")
                    else:
                        # ìƒë‹¨ ë¶€ë¶„ì„ ìš°ì„ ì ìœ¼ë¡œ í¬ë¡­
                        top = 0
                        bottom = new_height

                    left = 0
                    right = width
                else:
                    # ë†’ì´ëŠ” ê·¸ëŒ€ë¡œ, ë„ˆë¹„ë¥¼ í¬ë¡­
                    new_height = height

                    # ì–¼êµ´ì´ ê°ì§€ë˜ë©´ ì–¼êµ´ ì¤‘ì‹¬ìœ¼ë¡œ, ì•„ë‹ˆë©´ ì¤‘ì•™ í¬ë¡­
                    if focus_point:
                        focus_x, focus_y = focus_point
                        center_x = focus_x
                        left = max(0, center_x - new_width // 2)
                        right = min(width, left + new_width)

                        if right > width:
                            right = width
                            left = right - new_width
                        if left < 0:
                            left = 0
                            right = new_width

                        logger.info(f"  âœ¨ ì–¼êµ´ ì¤‘ì‹¬ í¬ë¡­: x={center_x}")
                    else:
                        # ì¤‘ì•™ í¬ë¡­
                        left = (width - new_width) // 2
                        right = left + new_width

                    top = 0
                    bottom = height

                logger.info(f"  âœ‚ï¸ í¬ë¡­ ì˜ì—­: ({left}, {top}) ~ ({right}, {bottom})")

                # ì´ë¯¸ì§€ í¬ë¡­
                img = img.crop((left, top, right, bottom))

                # í‘œì¤€ ì‡¼ì¸  í•´ìƒë„ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (1080x1920)
                target_size = (1080, 1920)
                img = img.resize(target_size, PILImage.Resampling.LANCZOS)

                logger.info(f"  âœ… ìŠ¤ë§ˆíŠ¸ í¬ë¡­ ì™„ë£Œ: {target_size[0]}x{target_size[1]} (9:16)")

                # ì €ì¥
                img.save(output_path, quality=95)
                return True

        except Exception as e:
            logger.error(f"ìŠ¤ë§ˆíŠ¸ í¬ë¡­ ì‹¤íŒ¨: {input_path} - {e}")
            return False

    def _find_all_media_files(self):
        """
        ëª¨ë“  ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì•„ì„œ ì •ë ¬ ì—†ì´ ë°˜í™˜
        Returns: (image_paths, video_paths)
        """
        # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
        all_images_set = set()
        for ext in ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']:
            for img_file in self.folder_path.glob(ext):
                if 'generated_videos' not in str(img_file) and 'thumbnail' not in img_file.name.lower():
                    all_images_set.add(img_file)

        images_folder = self.folder_path / "images"
        if images_folder.exists():
            for ext in ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']:
                for img_file in images_folder.glob(ext):
                    if 'thumbnail' not in img_file.name.lower():
                        all_images_set.add(img_file)

        # ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
        all_videos_set = set()
        for ext in ['*.mp4', '*.mov', '*.avi', '*.mkv', '*.MP4', '*.MOV', '*.AVI', '*.MKV']:
            for vid_file in self.folder_path.glob(ext):
                if 'generated_videos' not in str(vid_file):
                    all_videos_set.add(vid_file)

        videos_folder = self.folder_path / "videos"
        if videos_folder.exists():
            for ext in ['*.mp4', '*.mov', '*.avi', '*.mkv', '*.MP4', '*.MOV', '*.AVI', '*.MKV']:
                for vid_file in videos_folder.glob(ext):
                    all_videos_set.add(vid_file)

        return list(all_images_set), list(all_videos_set)

    def _find_images_with_scene_numbers(self) -> Dict[int, Path]:
        """ì”¬ë³„ ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸° (scene_XX íŒ¨í„´ ë˜ëŠ” ì‹œê°„ìˆœ ìë™ ì •ë ¬)"""
        images = {}

        # 1. scene_XX_image íŒ¨í„´ ì°¾ê¸°
        for file in self.folder_path.glob("scene_*_image.*"):
            match = re.match(r"scene_(\d+)_image\.(png|jpg|jpeg)", file.name)
            if match:
                scene_num = int(match.group(1))
                images[scene_num] = file

        # images ì„œë¸Œí´ë”ì—ì„œë„ ì°¾ê¸°
        images_folder = self.folder_path / "images"
        if images_folder.exists():
            for file in images_folder.glob("scene_*_image.*"):
                match = re.match(r"scene_(\d+)_image\.(png|jpg|jpeg)", file.name)
                if match:
                    scene_num = int(match.group(1))
                    if scene_num not in images:
                        images[scene_num] = file

        # 2. scene íŒ¨í„´ì´ ì—†ìœ¼ë©´ ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
        if not images:
            logger.info("scene_XX íŒ¨í„´ ì—†ìŒ. ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì°¾ìŠµë‹ˆë‹¤.")

            # ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸° (generated_videos í´ë” ë° ì¸ë„¤ì¼ ì œì™¸, ì¤‘ë³µ ì œê±°)
            all_images_set = set()
            for ext in ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']:
                for img_file in self.folder_path.glob(ext):
                    # generated_videos í´ë” ì•ˆì˜ íŒŒì¼ê³¼ ì¸ë„¤ì¼ì€ ì œì™¸
                    if 'generated_videos' not in str(img_file) and 'thumbnail' not in img_file.name.lower():
                        all_images_set.add(img_file)

            if images_folder and images_folder.exists():
                for ext in ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']:
                    for img_file in images_folder.glob(ext):
                        # ì¸ë„¤ì¼ ì œì™¸
                        if 'thumbnail' not in img_file.name.lower():
                            all_images_set.add(img_file)

            # Setì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ì •ë ¬
            # âš ï¸ ì¤‘ìš”: Frontendì™€ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©!
            # 1. ëª…í™•í•œ ì‹œí€€ìŠ¤ íŒ¨í„´ì´ ìˆìœ¼ë©´ ì‹œí€€ìŠ¤ë¡œ ì •ë ¬
            # 2. ì—†ìœ¼ë©´ íŒŒì¼ ìˆ˜ì • ì‹œê°„ìœ¼ë¡œ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
            def extract_sequence(filepath):
                """
                íŒŒì¼ëª…ì—ì„œ ì‹œí€€ìŠ¤ ë²ˆí˜¸ ì¶”ì¶œ (Frontend extractSequenceNumberì™€ ë™ì¼í•œ ë¡œì§)
                - 1.jpg, 02.png (ìˆ«ìë¡œ ì‹œì‘)
                - image_01.jpg, scene-02.png (_ìˆ«ì ë˜ëŠ” -ìˆ«ì)
                - Image_fx (47).jpg (ê´„í˜¸ ì•ˆ ìˆ«ì, ëœë¤ ID ì—†ì„ ë•Œë§Œ)

                Returns: (sequence_number or None, mtime)
                """
                import re
                filename = filepath.name

                # íŒŒì¼ ìˆ˜ì • ì‹œê°„ (í•­ìƒ ê°€ì ¸ì˜¤ê¸°)
                try:
                    mtime = filepath.stat().st_mtime
                except:
                    mtime = 0

                # 1. íŒŒì¼ëª…ì´ ìˆ«ìë¡œ ì‹œì‘: "1.jpg", "02.png"
                match = re.match(r'^(\d+)\.', filename)
                if match:
                    return (int(match.group(1)), mtime)

                # 2. _ìˆ«ì. ë˜ëŠ” -ìˆ«ì. íŒ¨í„´: "image_01.jpg", "scene-02.png"
                match = re.search(r'[_-](\d{1,3})\.', filename)
                if match:
                    return (int(match.group(1)), mtime)

                # 3. (ìˆ«ì) íŒ¨í„´: "Image_fx (47).jpg"
                # ë‹¨, ëœë¤ IDê°€ ì—†ì„ ë•Œë§Œ (8ì ì´ìƒì˜ ì˜ìˆ«ì ì¡°í•©ì´ ì—†ì„ ë•Œ)
                match = re.search(r'\((\d+)\)', filename)
                if match and not re.search(r'[_-]\w{8,}', filename):
                    return (int(match.group(1)), mtime)

                # ì‹œí€€ìŠ¤ ë²ˆí˜¸ ì—†ìŒ - íŒŒì¼ ìˆ˜ì • ì‹œê°„ ì‚¬ìš©
                return (None, mtime)

            # ì •ë ¬: ì‹œí€€ìŠ¤ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ ìš°ì„ , ì—†ìœ¼ë©´ ì‹œê°„ ìˆœì„œ
            all_images_list = list(all_images_set)
            all_images = sorted(all_images_list, key=lambda f: (
                extract_sequence(f)[0] is None,  # ì‹œí€€ìŠ¤ ì—†ëŠ” ê²ƒì„ ë’¤ë¡œ
                extract_sequence(f)[0] if extract_sequence(f)[0] is not None else 0,  # ì‹œí€€ìŠ¤ ì •ë ¬
                extract_sequence(f)[1]  # ì‹œê°„ ì •ë ¬
            ))

            # ì”¬ ë²ˆí˜¸ ìë™ í• ë‹¹ ë° ë¡œê·¸ ì¶œë ¥
            logger.info(f"\nğŸ“· ì´ë¯¸ì§€ ì •ë ¬ ì™„ë£Œ (ì´ {len(all_images)}ê°œ):")
            for idx, img_path in enumerate(all_images, start=1):
                images[idx] = img_path
                seq_info = extract_sequence(img_path)
                if seq_info[0] is not None:
                    logger.info(f"  ì”¬ {idx}: {img_path.name} (ì‹œí€€ìŠ¤: {seq_info[0]})")
                else:
                    import datetime
                    mtime_str = datetime.datetime.fromtimestamp(seq_info[1]).strftime('%Y-%m-%d %H:%M:%S')
                    logger.info(f"  ì”¬ {idx}: {img_path.name} (ì‹œê°„: {mtime_str})")

        logger.info(f"ì´ë¯¸ì§€ {len(images)}ê°œ ë°œê²¬")

        # 3. ìë™ ì´ë¯¸ì§€ ìƒì„±/ë‹¤ìš´ë¡œë“œ (í™œì„±í™”ëœ ê²½ìš°)
        if self.image_source in ["google", "dalle", "imagen3"]:
            images = self._download_missing_images(images)

        return images

    def _find_videos(self) -> Dict[int, Path]:
        """ì”¬ë³„ ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸° (ì´ë¯¸ì§€ì™€ ë™ì¼í•œ ì •ë ¬ ë¡œì§)"""
        videos = {}

        logger.info("ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.")

        # ëª¨ë“  ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸° (generated_videos í´ë” ì œì™¸, ì¤‘ë³µ ì œê±°)
        all_videos_set = set()
        for ext in ['*.mp4', '*.mov', '*.avi', '*.mkv', '*.MP4', '*.MOV', '*.AVI', '*.MKV']:
            for vid_file in self.folder_path.glob(ext):
                # generated_videos í´ë” ì•ˆì˜ íŒŒì¼ì€ ì œì™¸
                if 'generated_videos' not in str(vid_file):
                    all_videos_set.add(vid_file)

        # videos ì„œë¸Œí´ë”ì—ì„œë„ ì°¾ê¸°
        videos_folder = self.folder_path / "videos"
        if videos_folder and videos_folder.exists():
            for ext in ['*.mp4', '*.mov', '*.avi', '*.mkv', '*.MP4', '*.MOV', '*.AVI', '*.MKV']:
                for vid_file in videos_folder.glob(ext):
                    all_videos_set.add(vid_file)

        # ì •ë ¬ ë¡œì§ (ì´ë¯¸ì§€ì™€ ë™ì¼)
        def extract_sequence(filepath):
            """
            íŒŒì¼ëª…ì—ì„œ ì‹œí€€ìŠ¤ ë²ˆí˜¸ ì¶”ì¶œ (Frontend extractSequenceNumberì™€ ë™ì¼í•œ ë¡œì§)
            - 1.mp4, 02.mp4 (ìˆ«ìë¡œ ì‹œì‘)
            - video_01.mp4, scene-02.mp4 (_ìˆ«ì ë˜ëŠ” -ìˆ«ì)
            - Video_fx (47).mp4 (ê´„í˜¸ ì•ˆ ìˆ«ì, ëœë¤ ID ì—†ì„ ë•Œë§Œ)

            Returns: (sequence_number or None, mtime)
            """
            import re
            filename = filepath.name

            # íŒŒì¼ ìˆ˜ì • ì‹œê°„ (í•­ìƒ ê°€ì ¸ì˜¤ê¸°)
            try:
                mtime = filepath.stat().st_mtime
            except:
                mtime = 0

            # 1. íŒŒì¼ëª…ì´ ìˆ«ìë¡œ ì‹œì‘: "1.mp4", "02.mp4"
            match = re.match(r'^(\d+)\.', filename)
            if match:
                return (int(match.group(1)), mtime)

            # 2. _ìˆ«ì. ë˜ëŠ” -ìˆ«ì. íŒ¨í„´: "video_01.mp4", "scene-02.mp4"
            match = re.search(r'[_-](\d{1,3})\.', filename)
            if match:
                return (int(match.group(1)), mtime)

            # 3. (ìˆ«ì) íŒ¨í„´: "Video_fx (47).mp4"
            # ë‹¨, ëœë¤ IDê°€ ì—†ì„ ë•Œë§Œ (8ì ì´ìƒì˜ ì˜ìˆ«ì ì¡°í•©ì´ ì—†ì„ ë•Œ)
            match = re.search(r'\((\d+)\)', filename)
            if match and not re.search(r'[_-]\w{8,}', filename):
                return (int(match.group(1)), mtime)

            # ì‹œí€€ìŠ¤ ë²ˆí˜¸ ì—†ìŒ - íŒŒì¼ ìˆ˜ì • ì‹œê°„ ì‚¬ìš©
            return (None, mtime)

        # ì •ë ¬: ì‹œí€€ìŠ¤ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ ìš°ì„ , ì—†ìœ¼ë©´ ì‹œê°„ ìˆœì„œ
        all_videos_list = list(all_videos_set)
        all_videos = sorted(all_videos_list, key=lambda f: (
            extract_sequence(f)[0] is None,  # ì‹œí€€ìŠ¤ ì—†ëŠ” ê²ƒì„ ë’¤ë¡œ
            extract_sequence(f)[0] if extract_sequence(f)[0] is not None else 0,  # ì‹œí€€ìŠ¤ ì •ë ¬
            extract_sequence(f)[1]  # ì‹œê°„ ì •ë ¬
        ))

        # ì”¬ ë²ˆí˜¸ ìë™ í• ë‹¹ ë° ë¡œê·¸ ì¶œë ¥
        logger.info(f"\nğŸ¬ ë¹„ë””ì˜¤ ì •ë ¬ ì™„ë£Œ (ì´ {len(all_videos)}ê°œ):")
        for idx, vid_path in enumerate(all_videos, start=1):
            videos[idx] = vid_path
            seq_info = extract_sequence(vid_path)
            if seq_info[0] is not None:
                logger.info(f"  ì”¬ {idx}: {vid_path.name} (ì‹œí€€ìŠ¤: {seq_info[0]})")
            else:
                import datetime
                mtime_str = datetime.datetime.fromtimestamp(seq_info[1]).strftime('%Y-%m-%d %H:%M:%S')
                logger.info(f"  ì”¬ {idx}: {vid_path.name} (ì‹œê°„: {mtime_str})")

        logger.info(f"ë¹„ë””ì˜¤ {len(videos)}ê°œ ë°œê²¬")

        return videos

    def _download_missing_images(self, images: Dict[int, Path]) -> Dict[int, Path]:
        """
        ëˆ„ë½ëœ ì´ë¯¸ì§€ë¥¼ DALL-E ë˜ëŠ” Imagen3ìœ¼ë¡œ ìë™ ìƒì„±

        Args:
            images: ê¸°ì¡´ ì´ë¯¸ì§€ ë”•ì…”ë„ˆë¦¬

        Returns:
            ì—…ë°ì´íŠ¸ëœ ì´ë¯¸ì§€ ë”•ì…”ë„ˆë¦¬
        """
        scenes = self.story_data.get('scenes', [])

        if not scenes:
            logger.warning("âš ï¸ story.jsonì— scenes ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return images

        logger.info(f"ğŸ” ì´ {len(scenes)}ê°œ ì”¬ì— ëŒ€í•´ ì´ë¯¸ì§€ í™•ì¸ ì¤‘...")

        missing_scenes = []
        for idx, scene in enumerate(scenes, start=1):
            if idx not in images:
                missing_scenes.append((idx, scene))

        if not missing_scenes:
            logger.info("âœ… ëª¨ë“  ì”¬ì— ì´ë¯¸ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
            return images

        source_name = "DALL-E 3" if self.image_source == "dalle" else "Imagen 3"
        logger.info(f"âš ï¸ {len(missing_scenes)}ê°œ ì”¬ì˜ ì´ë¯¸ì§€ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. {source_name}ë¡œ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        # ë¹„ìš© ì˜ˆì¸¡
        if self.image_source == "dalle":
            self._log_dalle_cost_estimate(len(missing_scenes))

        try:
            success_count = 0
            fail_count = 0

            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
            def generate_single_image(scene_data):
                scene_num, scene = scene_data

                # ì·¨ì†Œ í”Œë˜ê·¸ íŒŒì¼ ì²´í¬
                cancel_file = self.folder_path / '.cancel'
                if cancel_file.exists():
                    logger.warning("ğŸ›‘ ì·¨ì†Œ í”Œë˜ê·¸ ê°ì§€ë¨. ì´ë¯¸ì§€ ìƒì„±ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    return (scene_num, None, "cancelled")

                # image_prompt ì¶”ì¶œ (imagefx_promptë„ ì§€ì›)
                image_prompt = scene.get('image_prompt') or scene.get('imagefx_prompt', '')
                sora_prompt = scene.get('sora_prompt', '')

                if not image_prompt:
                    # ìƒí’ˆ ì½˜í…ì¸ (product-*.*)ëŠ” sora_promptê°€ ìˆìœ¼ë©´ image_prompt ì—†ì–´ë„ ê´œì°®ìŒ
                    version = self.story_data.get('version', '')
                    is_product = version.startswith('product-') or 'product' in version.lower()

                    if is_product and sora_prompt:
                        # Soraë¡œ ë¹„ë””ì˜¤ ìƒì„±í•  ê±°ë‹ˆê¹Œ ì´ë¯¸ì§€ëŠ” ì•ˆ ë§Œë“¤ì–´ë„ ë¨
                        logger.info(f"â„¹ï¸ ì”¬ {scene_num}: ìƒí’ˆ ì½˜í…ì¸ , sora_prompt ìˆìŒ â†’ ì´ë¯¸ì§€ ìƒì„± ìŠ¤í‚µ")
                        return (scene_num, None, "sora_only")

                    logger.warning(f"âš ï¸ ì”¬ {scene_num}: image_prompt ë˜ëŠ” imagefx_promptê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    return (scene_num, None, "no_prompt")

                # íŒŒì¼ëª… ìƒì„±
                filename = f"scene_{scene_num:02d}_image.jpg"

                try:
                    if self.image_source == "dalle":
                        # DALL-E 3 Image Generation
                        logger.info(f"ğŸ¨ ì”¬ {scene_num}: '{image_prompt}' DALL-E 3 ìƒì„± ì¤‘...")
                        generated_path = self._generate_dalle_image(
                            prompt=image_prompt,
                            save_dir=self.folder_path,
                            filename=filename
                        )

                        if generated_path:
                            logger.info(f"âœ… ì”¬ {scene_num}: ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
                            logger.info(f"   â†’ images[{scene_num}] = {generated_path}")
                            return (scene_num, generated_path, "success")
                        else:
                            logger.error(f"âŒ ì”¬ {scene_num}: ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨")
                            return (scene_num, None, "failed")

                    elif self.image_source == "imagen3":
                        # Google Imagen 3 Image Generation
                        logger.info(f"ğŸ–¼ï¸ ì”¬ {scene_num}: '{image_prompt}' Imagen 3 ìƒì„± ì¤‘...")
                        generated_path = self._generate_imagen3_image(
                            prompt=image_prompt,
                            save_dir=self.folder_path,
                            filename=filename
                        )

                        if generated_path:
                            logger.info(f"âœ… ì”¬ {scene_num}: ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
                            logger.info(f"   â†’ images[{scene_num}] = {generated_path}")
                            return (scene_num, generated_path, "success")
                        else:
                            logger.error(f"âŒ ì”¬ {scene_num}: ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨")
                            return (scene_num, None, "failed")
                except Exception as e:
                    logger.error(f"âŒ ì”¬ {scene_num}: ì´ë¯¸ì§€ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                    return (scene_num, None, "error")

                return (scene_num, None, "unknown")

            # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰ (ìµœëŒ€ 3ê°œ ë™ì‹œ ì²˜ë¦¬)
            from concurrent.futures import ThreadPoolExecutor, as_completed
            max_workers = 3  # OpenAI API rate limit ê³ ë ¤

            logger.info(f"ğŸš€ {len(missing_scenes)}ê°œ ì´ë¯¸ì§€ë¥¼ ìµœëŒ€ {max_workers}ê°œì”© ë³‘ë ¬ ìƒì„±í•©ë‹ˆë‹¤...")

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # ëª¨ë“  ì”¬ì— ëŒ€í•´ ì‘ì—… ì œì¶œ
                future_to_scene = {
                    executor.submit(generate_single_image, scene_data): scene_data[0]
                    for scene_data in missing_scenes
                }

                # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
                for future in as_completed(future_to_scene):
                    scene_num = future_to_scene[future]
                    try:
                        scene_num, generated_path, status = future.result()

                        if status == "success" and generated_path:
                            images[scene_num] = generated_path
                            success_count += 1
                        elif status == "cancelled":
                            raise KeyboardInterrupt("User cancelled the operation")
                        elif status in ["failed", "error", "no_prompt"]:
                            fail_count += 1
                    except Exception as e:
                        logger.error(f"âŒ ì”¬ {scene_num}: ì‘ì—… ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {e}")
                        fail_count += 1

        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ìë™ ìƒì„±/ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # ìµœì¢… ë¹„ìš© ìš”ì•½
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š ì´ë¯¸ì§€ ìƒì„±/ë‹¤ìš´ë¡œë“œ ì™„ë£Œ - ì´ {len(images)}ê°œ ì´ë¯¸ì§€ í™•ë³´")
        logger.info(f"   âœ… ì„±ê³µ: {success_count}ê°œ, âŒ ì‹¤íŒ¨: {fail_count}ê°œ")
        logger.info(f"   ğŸ” images ë”•ì…”ë„ˆë¦¬ ë‚´ìš©:")
        for scene_num, img_path in sorted(images.items()):
            logger.info(f"      ì”¬ {scene_num}: {img_path.name}")

        if self.image_source == "dalle":
            total_cost = success_count * 0.080  # HD quality
            logger.info(f"ğŸ’° ì´ ë¹„ìš©: ${total_cost:.2f} (ì•½ â‚©{total_cost * 1300:.0f})")

        logger.info(f"{'='*60}\n")

        return images

    def _refine_prompt_with_claude(self, original_prompt: str, error_message: str, model_name: str) -> Optional[str]:
        """
        Claudeë¥¼ í˜¸ì¶œí•´ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ì—¬ content filterë¥¼ í†µê³¼í•  ìˆ˜ ìˆë„ë¡ í•¨

        Args:
            original_prompt: ì›ë³¸ í”„ë¡¬í”„íŠ¸
            error_message: ì—ëŸ¬ ë©”ì‹œì§€
            model_name: ì‚¬ìš© ì¤‘ì¸ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ì´ë¦„ (DALL-E 3, Imagen 3 ë“±)

        Returns:
            ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸ (ì‹¤íŒ¨ ì‹œ None)
        """
        if not self.anthropic_client:
            logger.warning("âš ï¸ Claude APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ìˆ˜ì •ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None

        try:
            logger.info(f"ğŸ¤– Claudeë¥¼ í˜¸ì¶œí•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ì¤‘...")

            system_prompt = f"""You are an expert at refining image generation prompts to pass content filters for {model_name}.
Your goal is to modify the given prompt to avoid content policy violations while maintaining the core visual intent and quality.

Guidelines:
- Remove any potentially sensitive, violent, or inappropriate content
- Keep photorealistic, professional photography style keywords
- Maintain the aspect ratio, composition, and lighting instructions
- Preserve Korean cultural context if present
- Keep "NO TEXT" instruction at the end
- Make minimal changes necessary to pass content filters"""

            user_prompt = f"""The following image generation prompt was rejected by {model_name}'s content filter:

Error: {error_message}

Original prompt:
{original_prompt}

Please rewrite this prompt to pass the content filter while maintaining the visual quality and intent.
Return ONLY the refined prompt without any explanation or additional text."""

            response = self.anthropic_client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=500,
                temperature=0.3,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )

            refined_prompt = response.content[0].text.strip()
            logger.info(f"âœ… Claudeê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤")
            logger.info(f"   ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸: {refined_prompt[:150]}...")

            return refined_prompt

        except Exception as e:
            logger.error(f"âŒ Claude í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            return None

    def _generate_dalle_image(self, prompt: str, save_dir: Path, filename: str) -> Optional[Path]:
        """
        DALL-E 3ë¡œ ì´ë¯¸ì§€ ìƒì„± ë° ì €ì¥

        Args:
            prompt: ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸
            save_dir: ì €ì¥ ë””ë ‰í† ë¦¬
            filename: ì €ì¥ íŒŒì¼ëª…

        Returns:
            ìƒì„±ëœ ì´ë¯¸ì§€ ê²½ë¡œ (ì‹¤íŒ¨ ì‹œ None)
        """
        # ì·¨ì†Œ í”Œë˜ê·¸ ì²´í¬ (DALL-E ì‹œì‘ ì „)
        cancel_file = self.folder_path / '.cancel'
        if cancel_file.exists():
            logger.warning("ğŸ›‘ ì·¨ì†Œ í”Œë˜ê·¸ ê°ì§€ë¨. DALL-E ì´ë¯¸ì§€ ìƒì„±ì„ ì‹œì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            raise KeyboardInterrupt("User cancelled the operation")

        if not self.dalle_client:
            logger.error("âŒ DALL-E í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        # aspect_ratioì— ë”°ë¼ ì´ë¯¸ì§€ í¬ê¸° ê²°ì •
        if self.aspect_ratio == "9:16":
            image_size = "1024x1792"  # ì„¸ë¡œí˜• (ìˆí¼)
        else:  # 16:9 or other
            image_size = "1792x1024"  # ê°€ë¡œí˜• (ë¡±í¼)

        logger.info(f"ğŸ¨ DALL-E ì´ë¯¸ì§€ ìƒì„± í¬ê¸°: {image_size} ({self.aspect_ratio})")

        max_retries = 3
        current_prompt = prompt
        last_error_message = ""

        for attempt in range(max_retries):
            try:
                # ì¬ì‹œë„ ì‹œ Claudeë¥¼ í˜¸ì¶œí•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
                if attempt == 0:
                    current_prompt = prompt
                    logger.info(f"ğŸ¨ ì²« ë²ˆì§¸ ì‹œë„: ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
                else:
                    # Claudeë¥¼ í˜¸ì¶œí•´ì„œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
                    logger.info(f"ğŸ”„ Content filter ìš°íšŒë¥¼ ìœ„í•´ Claudeë¡œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • (ì‹œë„ {attempt + 1}/{max_retries})")
                    refined_prompt = self._refine_prompt_with_claude(
                        original_prompt=current_prompt,
                        error_message=last_error_message,
                        model_name="DALL-E 3"
                    )

                    if refined_prompt:
                        current_prompt = refined_prompt
                        logger.info(f"   ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸: {current_prompt[:150]}...")
                    else:
                        # Claude í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ í´ë°± ì‚¬ìš©
                        if attempt == 1:
                            current_prompt = f"A calm and peaceful scene depicting: {prompt[:100]}"
                        else:
                            current_prompt = "A beautiful, peaceful landscape with soft lighting"
                        logger.warning(f"   âš ï¸ Claude ìˆ˜ì • ì‹¤íŒ¨, í´ë°± í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {current_prompt}")

                # ì·¨ì†Œ í”Œë˜ê·¸ ì²´í¬ (DALL-E API í˜¸ì¶œ ì§ì „)
                cancel_file = self.folder_path / '.cancel'
                if cancel_file.exists():
                    logger.warning("ğŸ›‘ ì·¨ì†Œ í”Œë˜ê·¸ ê°ì§€ë¨. DALL-E API í˜¸ì¶œì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    raise KeyboardInterrupt("User cancelled the operation")

                # DALL-E 3 API í˜¸ì¶œ
                response = self.dalle_client.images.generate(
                    model="dall-e-3",
                    prompt=current_prompt,
                    size=image_size,
                    quality="hd",
                    n=1
                )

                # ìƒì„±ëœ ì´ë¯¸ì§€ URL ê°€ì ¸ì˜¤ê¸°
                image_url = response.data[0].url

                # ì·¨ì†Œ í”Œë˜ê·¸ ì²´í¬ (ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì „)
                cancel_file = self.folder_path / '.cancel'
                if cancel_file.exists():
                    logger.warning("ğŸ›‘ ì·¨ì†Œ í”Œë˜ê·¸ ê°ì§€ë¨. DALL-E ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    raise KeyboardInterrupt("User cancelled the operation")

                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                import requests
                logger.info(f"ğŸ“¥ DALL-E ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                img_response = requests.get(image_url, timeout=30)
                img_response.raise_for_status()

                # íŒŒì¼ ì €ì¥
                save_path = save_dir / filename
                save_path.parent.mkdir(parents=True, exist_ok=True)

                with open(save_path, 'wb') as f:
                    f.write(img_response.content)

                logger.info(f"âœ… DALL-E ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {save_path.name}")
                if attempt > 0:
                    logger.info(f"   (ì¬ì‹œë„ {attempt}íšŒ ëì— ì„±ê³µ)")
                return save_path

            except Exception as e:
                error_str = str(e)
                last_error_message = error_str  # ì—ëŸ¬ ë©”ì‹œì§€ ì €ì¥ (Claudeì—ê²Œ ì „ë‹¬ìš©)

                # Content policy violation ì²´í¬
                if 'content_policy_violation' in error_str or 'content filters' in error_str:
                    logger.warning(f"âš ï¸ Content filterì— ê±¸ë¦¼ (ì‹œë„ {attempt + 1}/{max_retries})")
                    logger.info(f"   ì—ëŸ¬: {error_str}")

                    if attempt < max_retries - 1:
                        logger.info("   â†’ Claudeë¥¼ í†µí•´ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ì—¬ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                        continue
                    else:
                        logger.error(f"âŒ {max_retries}íšŒ ì¬ì‹œë„ í›„ì—ë„ Content filter í†µê³¼ ì‹¤íŒ¨")
                        logger.error(f"   ìµœì¢… í”„ë¡¬í”„íŠ¸: {current_prompt}")
                        return None
                else:
                    # ë‹¤ë¥¸ ì—ëŸ¬ë„ ì¬ì‹œë„ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±)
                    logger.warning(f"âš ï¸ DALL-E ì´ë¯¸ì§€ ìƒì„± ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {error_str}")
                    return None

        return None

    def _generate_imagen3_image(self, prompt: str, save_dir: Path, filename: str) -> Optional[Path]:
        """
        Google Imagen 3ë¡œ ì´ë¯¸ì§€ ìƒì„± ë° ì €ì¥ (Vertex AI)

        Args:
            prompt: ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸
            save_dir: ì €ì¥ ë””ë ‰í† ë¦¬
            filename: ì €ì¥ íŒŒì¼ëª…

        Returns:
            ìƒì„±ëœ ì´ë¯¸ì§€ ê²½ë¡œ (ì‹¤íŒ¨ ì‹œ None)
        """
        # ì·¨ì†Œ í”Œë˜ê·¸ ì²´í¬
        cancel_file = self.folder_path / '.cancel'
        if cancel_file.exists():
            logger.warning("ğŸ›‘ ì·¨ì†Œ í”Œë˜ê·¸ ê°ì§€ë¨. Imagen 3 ì´ë¯¸ì§€ ìƒì„±ì„ ì‹œì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            raise KeyboardInterrupt("User cancelled the operation")

        if not self.imagen_model:
            logger.error("âŒ Imagen 3 ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        logger.info(f"ğŸ–¼ï¸ Imagen 3 ì´ë¯¸ì§€ ìƒì„± ì¤‘ (Vertex AI)...")

        max_retries = 3
        current_prompt = prompt
        last_error_message = ""

        for attempt in range(max_retries):
            try:
                from PIL import Image
                import io

                # ì¬ì‹œë„ ì‹œ Claudeë¥¼ í˜¸ì¶œí•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
                if attempt == 0:
                    current_prompt = prompt
                    logger.info(f"ğŸ–¼ï¸ ì²« ë²ˆì§¸ ì‹œë„: ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
                else:
                    # Claudeë¥¼ í˜¸ì¶œí•´ì„œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
                    logger.info(f"ğŸ”„ Content filter ìš°íšŒë¥¼ ìœ„í•´ Claudeë¡œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • (ì‹œë„ {attempt + 1}/{max_retries})")
                    refined_prompt = self._refine_prompt_with_claude(
                        original_prompt=current_prompt,
                        error_message=last_error_message,
                        model_name="Google Imagen 3"
                    )

                    if refined_prompt:
                        current_prompt = refined_prompt
                        logger.info(f"   ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸: {current_prompt[:150]}...")
                    else:
                        # Claude í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ í´ë°± ì‚¬ìš©
                        if attempt == 1:
                            current_prompt = f"A calm and peaceful scene depicting: {prompt[:100]}"
                        else:
                            current_prompt = "A beautiful, peaceful landscape with soft lighting"
                        logger.warning(f"   âš ï¸ Claude ìˆ˜ì • ì‹¤íŒ¨, í´ë°± í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {current_prompt}")

                # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì œí•œ (2048ì)
                if len(current_prompt) > 2048:
                    current_prompt = current_prompt[:2045] + "..."
                    logger.warning(f"âš ï¸ í”„ë¡¬í”„íŠ¸ê°€ 2048ìë¥¼ ì´ˆê³¼í•˜ì—¬ ì˜ë ¸ìŠµë‹ˆë‹¤.")

                # ì·¨ì†Œ í”Œë˜ê·¸ ì²´í¬ (API í˜¸ì¶œ ì§ì „)
                cancel_file = self.folder_path / '.cancel'
                if cancel_file.exists():
                    logger.warning("ğŸ›‘ ì·¨ì†Œ í”Œë˜ê·¸ ê°ì§€ë¨. Imagen 3 API í˜¸ì¶œì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    raise KeyboardInterrupt("User cancelled the operation")

                # Imagen 3 ì´ë¯¸ì§€ ìƒì„± (Vertex AI)
                logger.info(f"ğŸ“¡ Vertex AI Imagen 3 API í˜¸ì¶œ ì¤‘...")

                # ì´ë¯¸ì§€ ìƒì„± - ì˜¬ë°”ë¥¸ ë©”ì„œë“œ ì‚¬ìš©
                images = self.imagen_model.generate_images(
                    prompt=current_prompt,
                    number_of_images=1,
                    aspect_ratio="1:1",  # 1:1, 9:16, 16:9, 4:3, 3:4 ì§€ì›
                    safety_filter_level="block_only_high",
                    person_generation="allow_adult",
                )

                if not images:
                    raise Exception("ì‘ë‹µì— ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")

                # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
                generated_image = images[0]

                # PIL Imageë¡œ ë³€í™˜
                img = generated_image._pil_image
                logger.info(f"âœ… ì´ë¯¸ì§€ ìˆ˜ì‹  ì™„ë£Œ: {img.size}")

                # aspect_ratioì— ë§ì¶° ë¦¬ì‚¬ì´ì¦ˆ
                if self.aspect_ratio == "9:16":
                    target_size = (1080, 1920)
                else:  # 16:9
                    target_size = (1920, 1080)

                if img.size != target_size:
                    img = img.resize(target_size, Image.Resampling.LANCZOS)
                    logger.info(f"ğŸ“ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ: {img.size} â†’ {target_size[0]}x{target_size[1]}")

                # íŒŒì¼ ì €ì¥
                save_path = save_dir / filename
                save_path.parent.mkdir(parents=True, exist_ok=True)

                img.save(save_path)

                logger.info(f"âœ… Imagen 3 ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {save_path.name}")
                if attempt > 0:
                    logger.info(f"   (ì¬ì‹œë„ {attempt}íšŒ ëì— ì„±ê³µ)")
                return save_path

            except Exception as e:
                error_str = str(e)
                last_error_message = error_str  # ì—ëŸ¬ ë©”ì‹œì§€ ì €ì¥ (Claudeì—ê²Œ ì „ë‹¬ìš©)

                logger.warning(f"âš ï¸ Imagen 3 ì´ë¯¸ì§€ ìƒì„± ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {error_str}")

                if attempt < max_retries - 1:
                    logger.info("   â†’ Claudeë¥¼ í†µí•´ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ì—¬ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    continue
                else:
                    logger.error(f"âŒ {max_retries}íšŒ ì¬ì‹œë„ í›„ì—ë„ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨")
                    logger.error(f"   ìµœì¢… í”„ë¡¬í”„íŠ¸: {current_prompt}")
                    import traceback
                    logger.error(traceback.format_exc())
                    return None

        return None

    def _log_dalle_cost_estimate(self, num_images: int):
        """
        DALL-E 3 ë¹„ìš© ì˜ˆì¸¡ ë¡œê·¸ ì¶œë ¥ (ê´€ë¦¬ìì—ê²Œë§Œ í‘œì‹œ)

        Args:
            num_images: ìƒì„±í•  ì´ë¯¸ì§€ ê°œìˆ˜
        """
        # ê´€ë¦¬ìê°€ ì•„ë‹ˆë©´ ë¡œê·¸ ì¶œë ¥ ì•ˆ í•¨
        if not self.is_admin:
            return

        # DALL-E 3 pricing
        # - Standard quality (1024x1024): $0.040 per image
        # - HD quality (1024x1792 or 1792x1024): $0.080 per image

        # aspect_ratioì— ë”°ë¼ ì´ë¯¸ì§€ í¬ê¸° ê²°ì •
        # - 9:16 (ìˆí¼) -> 1024x1792 (ì„¸ë¡œí˜•)
        # - 16:9 (ë¡±í¼) -> 1792x1024 (ê°€ë¡œí˜•)
        DALLE_COST_PER_IMAGE_HD = 0.080
        DALLE_COST_PER_IMAGE_STANDARD = 0.040

        # HD quality ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        estimated_cost_hd = num_images * DALLE_COST_PER_IMAGE_HD
        estimated_cost_standard = num_images * DALLE_COST_PER_IMAGE_STANDARD

        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ’° DALL-E 3 ì´ë¯¸ì§€ ìƒì„± ë¹„ìš© ì˜ˆì¸¡ (ê´€ë¦¬ì ì „ìš©)")
        # ì´ë¯¸ì§€ í¬ê¸° í‘œì‹œ
        if self.aspect_ratio == "9:16":
            image_size_str = "1024x1792 (ì„¸ë¡œí˜• ìˆí¼)"
        else:
            image_size_str = "1792x1024 (ê°€ë¡œí˜• ë¡±í¼)"

        logger.info(f"{'='*60}")
        logger.info(f"ğŸ“Š ìƒì„± ì˜ˆì • ì´ë¯¸ì§€: {num_images}ê°œ")
        logger.info(f"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {image_size_str}")
        logger.info(f"ğŸ’µ ì˜ˆìƒ ë¹„ìš© (HD {image_size_str.split()[0]}):        ${estimated_cost_hd:.2f} (ì•½ â‚©{estimated_cost_hd * 1300:.0f})")
        logger.info(f"ğŸ’µ ì˜ˆìƒ ë¹„ìš© (Standard 1024x1024): ${estimated_cost_standard:.2f} (ì•½ â‚©{estimated_cost_standard * 1300:.0f})")
        logger.info(f"â„¹ï¸  HD quality ì‚¬ìš© ê¶Œì¥ ({self.aspect_ratio} ë¹„ìœ¨ì— ì í•©)")
        logger.info(f"{'='*60}\n")

    def _clean_script_for_tts(self, script: str) -> str:
        """TTSìš© í…ìŠ¤íŠ¸ ì •ë¦¬ (ë°±ìŠ¬ë˜ì‹œ, ì—ëŸ¬ ë©”ì‹œì§€, ìˆ«ì ë³€í™˜)"""
        import re

        cleaned = script

        # Remove all backslashes
        cleaned = cleaned.replace('\\', '')

        # Remove error messages
        cleaned = re.sub(r'\[Request interrupted by user\]', '', cleaned)
        cleaned = re.sub(r'\[.*?interrupted.*?\]', '', cleaned, flags=re.IGNORECASE)
        cleaned = re.sub(r'\[.*?error.*?\]', '', cleaned, flags=re.IGNORECASE)

        # Remove markdown formatting symbols (í•œê¸€ "ë³„í‘œ"ëŠ” ìœ ì§€ë¨)
        cleaned = cleaned.replace('```', '')  # ì½”ë“œë¸”ë¡
        cleaned = cleaned.replace('**', '')  # ë³¼ë“œ ê¸°í˜¸
        cleaned = cleaned.replace('__', '')  # ë³¼ë“œ ê¸°í˜¸
        cleaned = cleaned.replace('*', '')  # ì´íƒ¤ë¦­ ê¸°í˜¸
        cleaned = cleaned.replace('`', '')  # ì½”ë“œ ê¸°í˜¸
        cleaned = re.sub(r'^#+\s+', '', cleaned, flags=re.MULTILINE)  # # í—¤ë”©
        cleaned = re.sub(r'^>\s+', '', cleaned, flags=re.MULTILINE)  # > ì¸ìš©
        cleaned = re.sub(r'^\s*[-]\s+', '', cleaned, flags=re.MULTILINE)  # - ë¦¬ìŠ¤íŠ¸ (ë³„í‘œëŠ” ìœ„ì—ì„œ ì œê±°ë¨)
        cleaned = re.sub(r'^\s*\d+\.\s+', '', cleaned, flags=re.MULTILINE)  # 1. ë¦¬ìŠ¤íŠ¸

        # Fix quotes
        cleaned = cleaned.replace('""', '"').replace("''", "'")

        # Convert numbers to Korean
        cleaned = self._convert_numbers_to_korean(cleaned)

        # Clean spaces
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()

        return cleaned

    def _convert_numbers_to_korean(self, text: str) -> str:
        """ìˆ«ìë¥¼ í•œê¸€ë¡œ ë³€í™˜ (3ë²ˆ -> ì„¸ ë²ˆ, 010-1234-5678 -> ê³µ ì¼ ê³µ ...)"""
        import re

        # ì „í™”ë²ˆí˜¸ íŒ¨í„´ ì²˜ë¦¬ (010-1234-5678, 02-123-4567 ë“±)
        def convert_phone_number(match):
            """ì „í™”ë²ˆí˜¸ë¥¼ í•œ ê¸€ìì”© ì½ê¸°"""
            phone = match.group(0)
            digits = re.sub(r'[^\d]', '', phone)
            digit_names = ['ê³µ', 'ì¼', 'ì´', 'ì‚¼', 'ì‚¬', 'ì˜¤', 'ìœ¡', 'ì¹ ', 'íŒ”', 'êµ¬']
            result = ' '.join([digit_names[int(d)] for d in digits])
            return result

        # ì „í™”ë²ˆí˜¸ íŒ¨í„´ (010-xxxx-xxxx, 02-xxx-xxxx ë“±)
        phone_pattern = r'0\d{1,2}[-\s]?\d{3,4}[-\s]?\d{4}'
        text = re.sub(phone_pattern, convert_phone_number, text)

        # ë¹„ë°€ë²ˆí˜¸/ì½”ë“œ íŒ¨í„´ ì²˜ë¦¬
        def convert_code(match):
            """ì—°ì†ëœ ìˆ«ìë¥¼ í•œ ê¸€ìì”© ì½ê¸°"""
            prefix = match.group(1) if match.group(1) else ''
            code = match.group(2)
            digit_names = ['ê³µ', 'ì¼', 'ì´', 'ì‚¼', 'ì‚¬', 'ì˜¤', 'ìœ¡', 'ì¹ ', 'íŒ”', 'êµ¬']
            result = ' '.join([digit_names[int(d)] for d in code])
            return prefix + result

        # ë¹„ë°€ë²ˆí˜¸, ì•”í˜¸, ì½”ë“œ ë“±ì˜ í‚¤ì›Œë“œ ë’¤ì— ì˜¤ëŠ” ìˆ«ì
        code_pattern = r'(ë¹„ë°€ë²ˆí˜¸ëŠ”?|ì•”í˜¸ëŠ”?|ì½”ë“œëŠ”?|ë²ˆí˜¸ëŠ”?)\s*(\d{4,})'
        text = re.sub(code_pattern, convert_code, text)

        def num_to_korean(num: int, sino: bool = True) -> str:
            if sino:
                ones = ['', 'ì¼', 'ì´', 'ì‚¼', 'ì‚¬', 'ì˜¤', 'ìœ¡', 'ì¹ ', 'íŒ”', 'êµ¬']
                tens = ['', 'ì‹­', 'ì´ì‹­', 'ì‚¼ì‹­', 'ì‚¬ì‹­', 'ì˜¤ì‹­', 'ìœ¡ì‹­', 'ì¹ ì‹­', 'íŒ”ì‹­', 'êµ¬ì‹­']

                if num == 0: return 'ì˜'
                if num < 10: return ones[num]
                elif num < 100:
                    return tens[num // 10] + (' ' + ones[num % 10] if num % 10 else '')
                elif num < 1000:
                    result = ('ë°±' if num // 100 == 1 else ones[num // 100] + 'ë°±')
                    if num % 100: result += ' ' + num_to_korean(num % 100, sino=True)
                    return result
                elif num < 10000:
                    result = ('ì²œ' if num // 1000 == 1 else ones[num // 1000] + 'ì²œ')
                    if num % 1000: result += ' ' + num_to_korean(num % 1000, sino=True)
                    return result
                elif num < 100000000:
                    result = num_to_korean(num // 10000, sino=True) + 'ë§Œ'
                    if num % 10000: result += ' ' + num_to_korean(num % 10000, sino=True)
                    return result
                else:
                    return str(num)
            else:
                native = ['', 'í•˜ë‚˜', 'ë‘˜', 'ì…‹', 'ë„·', 'ë‹¤ì„¯', 'ì—¬ì„¯', 'ì¼ê³±', 'ì—¬ëŸ', 'ì•„í™‰', 'ì—´']
                if num < 1 or num > 99: return num_to_korean(num, sino=True)
                if num <= 10: return native[num]
                elif num < 20: return 'ì—´' + (' ' + native[num - 10] if num > 10 else '')
                elif num < 100:
                    tens_native = ['', '', 'ìŠ¤ë¬´', 'ì„œë¥¸', 'ë§ˆí”', 'ì‰°', 'ì˜ˆìˆœ', 'ì¼í”', 'ì—¬ë“ ', 'ì•„í”']
                    result = tens_native[num // 10]
                    if num % 10: result += ' ' + native[num % 10]
                    return result
                else:
                    return num_to_korean(num, sino=True)

        def replace_number(match):
            num_str, unit = match.group(1), match.group(2) if match.group(2) else ''
            try:
                num = int(num_str)
                native_units = ['ë²ˆ', 'ë²ˆì§¸', 'ê°œ', 'ëª…', 'ë§ˆë¦¬', 'ì‚´', 'ì‹œ']
                use_native = any(unit.startswith(u) for u in native_units) and num <= 99
                korean_num = num_to_korean(num, sino=not use_native)

                # ë°›ì¹¨ íƒˆë½ ì²˜ë¦¬: ì…‹â†’ì„¸, ë„·â†’ë„¤, ìŠ¤ë¬¼â†’ìŠ¤ë¬´
                if unit and use_native:
                    if korean_num == 'ì…‹':
                        korean_num = 'ì„¸'
                    elif korean_num == 'ë„·':
                        korean_num = 'ë„¤'
                    elif korean_num.startswith('ì…‹ '):
                        korean_num = 'ì„¸ ' + korean_num[2:]
                    elif korean_num.startswith('ë„· '):
                        korean_num = 'ë„¤ ' + korean_num[2:]

                return korean_num + (' ' + unit if unit else '')
            except ValueError:
                return match.group(0)

        pattern = r'(\d+)(ë²ˆì§¸|ë²ˆ|ë¶„|ì´ˆ|ê°œ|ëª…|ë§ˆë¦¬|ì‚´|ì‹œ|ë“±|ìœ„|ë…„|ì›”|ì¼|íšŒ|ì°¨|ì¸µ|ëŒ€|ê¶Œ|ì¥|ê³¡|í¸|í™”|ê¸°|ì›|ë‹¬ëŸ¬|í‚¬ë¡œ|ë¯¸í„°|ì„¼í‹°|ê·¸ë¨|ë¦¬í„°)?'
        return re.sub(pattern, replace_number, text)

    def _clean_narration(self, text: str) -> str:
        """ë‚˜ë ˆì´ì…˜ í…ìŠ¤íŠ¸ ì •ë¦¬ (ëª…ë ¹ì–´ ë° ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°)"""
        # 1. TTSìš© í…ìŠ¤íŠ¸ ì •ë¦¬ (ë°±ìŠ¬ë˜ì‹œ, ì—ëŸ¬ ë©”ì‹œì§€, ìˆ«ì ë³€í™˜)
        text = self._clean_script_for_tts(text)

        # 2. ëŒ€ê´„í˜¸ ë§ˆì»¤ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜

        # [ë¬´ìŒ Nì´ˆ] â†’ ìˆ«ìë§Œí¼ ì¤„ë°”ê¿ˆ
        def replace_mute(match):
            num_str = match.group(1)
            if num_str:
                num = int(float(num_str))
                return '\n' * num
            return '\n'  # [ë¬´ìŒ] â†’ 1íšŒ
        text = re.sub(r'\[ë¬´ìŒ\s*(\d+(?:\.\d+)?)?ì´ˆ?\]', replace_mute, text)

        # [ì¹¨ë¬µ Nì´ˆ] â†’ NíšŒ ì¤„ë°”ê¿ˆ, ê¸°ë³¸ 3íšŒ
        def replace_silence(match):
            num_str = match.group(1)
            if num_str:
                num = int(float(num_str))
                return '\n' * num
            return '\n' * 3  # [ì¹¨ë¬µ] â†’ 3íšŒ
        text = re.sub(r'\[ì¹¨ë¬µ\s*(\d+(?:\.\d+)?)?ì´ˆ?\]', replace_silence, text)

        # [pause Nì´ˆ] â†’ NíšŒ ì¤„ë°”ê¿ˆ
        def replace_pause(match):
            num_str = match.group(1)
            if num_str:
                num = int(float(num_str))
                return '\n' * num
            return '\n'
        text = re.sub(r'\[pause\s*(\d+(?:\.\d+)?)?ì´ˆ?\]', replace_pause, text)

        # [íšŒìƒ] â†’ 3íšŒ ì¤„ë°”ê¿ˆ
        text = re.sub(r'\[íšŒìƒ\]', '\n\n\n', text)

        # ë‚˜ë¨¸ì§€ ëŒ€ê´„í˜¸ëŠ” ëª¨ë‘ ì œê±° (ê³µê°„, í–‰ë™, ë‚´ë©´ ë“±)
        text = re.sub(r'\[([^\]]+)\]', '', text)

        # ëŒ€í™” ë¶€ë¶„ ì œê±°ëŠ” í•˜ì§€ ì•ŠìŒ - ì„œì‚¬ì  í…ìŠ¤íŠ¸ëŠ” ëŒ€í™”ë¥¼ í¬í•¨í•¨
        # ì£¼ì„ ì²˜ë¦¬: text = re.sub(r'[ê°€-í£]+:\s*"[^"]*"', '', text)
        # ì£¼ì„ ì²˜ë¦¬: text = re.sub(r'[ê°€-í£]+:\s*[^\n/]+(?=/|$)', '', text)

        # / êµ¬ë¶„ì ì œê±°
        text = text.replace(' / ', ' ')

        # ì¤‘ë³µ ê³µë°± ì •ë¦¬ (ì¤„ë°”ê¿ˆì€ ìœ ì§€)
        text = re.sub(r' +', ' ', text)
        text = text.strip()

        return text

    def _add_natural_pauses(self, text: str) -> str:
        """êµ¬ë‘ì  ë’¤ì— ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ì‰¼ íš¨ê³¼"""
        import re

        # ë¨¼ì € ì—°ì†ëœ êµ¬ë‘ì  ì¡°í•© ì²˜ë¦¬ (ì¤‘ë³µ ì¤„ë°”ê¿ˆ ë°©ì§€)
        # ." â†’ ."\n (í•œ ë²ˆë§Œ)
        text = text.replace('."', '."\n')
        # ?" â†’ ?"\n (í•œ ë²ˆë§Œ)
        text = text.replace('?"', '?"\n')
        # !" â†’ !"\n (í•œ ë²ˆë§Œ)
        text = text.replace('!"', '!"\n')

        # ... ë’¤ì— ì¤„ë°”ê¿ˆ ì¶”ê°€ (ê°€ì¥ ê¸´ ì‰¼)
        text = text.replace('...', '...\n')

        # ë‚¨ì€ " ë’¤ì— ì¤„ë°”ê¿ˆ ì¶”ê°€ (ì´ë¯¸ ì²˜ë¦¬ëœ ê²ƒì€ ì œì™¸)
        text = re.sub(r'"(?!\n)', '"\n', text)

        # ? ë’¤ì— ì¤„ë°”ê¿ˆ ì¶”ê°€ (ì´ë¯¸ ì²˜ë¦¬ëœ ê²ƒì€ ì œì™¸)
        text = re.sub(r'\?(?!\n)', '?\n', text)

        # ! ë’¤ì— ì¤„ë°”ê¿ˆ ì¶”ê°€ (ì´ë¯¸ ì²˜ë¦¬ëœ ê²ƒì€ ì œì™¸)
        text = re.sub(r'!(?!\n)', '!\n', text)

        # , ë’¤ì— ì¤„ë°”ê¿ˆ ì¶”ê°€ (ì§§ì€ ì‰¼)
        text = text.replace(',', ',\n')

        # . ë’¤ì— ì¤„ë°”ê¿ˆ ì¶”ê°€ (ë‹¨, ìˆ«ì ë’¤ë‚˜ ì´ë¯¸ ì²˜ë¦¬ëœ ê²ƒì€ ì œì™¸)
        text = re.sub(r'\.(?!\d)(?!\n)', '.\n', text)

        return text

    async def _generate_tts(self, text: str, output_path: Path) -> tuple:
        """TTS ìƒì„± (ì œê³µìë³„ë¡œ ë¼ìš°íŒ…)"""
        if self.tts_provider == 'google':
            return await self._generate_google_tts(text, output_path)
        elif self.tts_provider == 'aws':
            return await self._generate_aws_polly(text, output_path)
        else:
            return await self._generate_edge_tts(text, output_path)

    async def _generate_edge_tts(self, text: str, output_path: Path) -> tuple:
        """Edge TTSë¡œ ìŒì„± ìƒì„± + ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ"""
        logger.info(f"Edge TTS ìƒì„± ì¤‘: {output_path.name}")

        # í…ìŠ¤íŠ¸ ì •ë¦¬
        clean_text = self._clean_narration(text)

        if not clean_text:
            logger.warning("í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ ê¸°ë³¸ ë©”ì‹œì§€ ì‚¬ìš©")
            clean_text = "ë¬´ìŒ"

        # êµ¬ë‘ì ì— ì‰¼í‘œ ì¶”ê°€ (ìì—°ìŠ¤ëŸ¬ìš´ ì‰¼í‘œ íš¨ê³¼)
        tts_text = self._add_natural_pauses(clean_text)

        # ============================================================
        # ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬: 5000ì ì´ìƒì´ë©´ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ”
        # ============================================================
        MAX_CHUNK_SIZE = 5000  # Edge TTS ì•ˆì •ì  ì²˜ë¦¬ ê¸¸ì´

        if len(tts_text) > MAX_CHUNK_SIZE:
            logger.warning(f"í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ì„œ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ”: {len(tts_text)}ì -> {MAX_CHUNK_SIZE}ìì”©")
            return await self._generate_edge_tts_chunked(tts_text, output_path)

        # ============================================================
        # ì¼ë°˜ ì²˜ë¦¬ (5000ì ì´í•˜)
        # ============================================================

        # Edge TTSë¡œ ìƒì„±í•˜ë©´ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ìˆ˜ì§‘
        # rate: -15%ë¡œ ì„¤ì •í•˜ì—¬ ì•½ê°„ ì²œì²œíˆ ë§í•˜ê²Œ í•¨
        communicate = edge_tts.Communicate(tts_text, self.voice, rate='-15%')

        word_timings = []
        sentence_timings = []
        audio_data = b""
        chunk_types_seen = set()

        async for chunk in communicate.stream():
            chunk_type = chunk.get("type", "unknown")
            chunk_types_seen.add(chunk_type)

            if chunk_type == "audio":
                audio_data += chunk["data"]
            elif chunk_type == "WordBoundary":
                # ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥ (ì´ìƒì )
                word_timings.append({
                    "word": chunk["text"],
                    "start": chunk["offset"] / 10_000_000.0,
                    "end": (chunk["offset"] + chunk["duration"]) / 10_000_000.0
                })
            elif chunk_type == "SentenceBoundary":
                # ë¬¸ì¥ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥ (í´ë°±ìš©)
                sentence_timings.append({
                    "text": chunk.get("text", ""),
                    "start": chunk["offset"] / 10_000_000.0,
                    "end": (chunk["offset"] + chunk["duration"]) / 10_000_000.0 if "duration" in chunk else None
                })

        # WordBoundaryê°€ ì—†ìœ¼ë©´ SentenceBoundary ì‚¬ìš©
        if not word_timings and sentence_timings:
            logger.info(f"WordBoundary ì—†ìŒ, SentenceBoundary ì‚¬ìš©: {len(sentence_timings)}ê°œ ë¬¸ì¥")
            # ê° ë¬¸ì¥ì„ ë‹¨ì–´ë¡œ ë¶„í• 
            for sent in sentence_timings:
                text = sent["text"].strip()
                if not text:
                    continue
                words = text.split()
                if not words:
                    continue

                # ë¬¸ì¥ ì‹œê°„ì„ ë‹¨ì–´ ê¸¸ì´(ê¸€ì ìˆ˜)ì— ë¹„ë¡€í•´ì„œ ë¶„ë°°
                # ì˜ˆ: "í• ì•„ë²„ì§€"(4ê¸€ì) + "ì˜"(1ê¸€ì) = 5ê¸€ì
                # â†’ "í• ì•„ë²„ì§€" 80%, "ì˜" 20% ì‹œê°„ í• ë‹¹
                sent_duration = (sent["end"] - sent["start"]) if sent["end"] else 1.0
                total_chars = sum(len(w) for w in words)
                if total_chars == 0:
                    total_chars = len(words)  # í´ë°±

                current_time = sent["start"]
                for word in words:
                    word_chars = len(word)
                    # ê¸€ì ìˆ˜ì— ë¹„ë¡€í•´ì„œ ì‹œê°„ í• ë‹¹
                    word_duration = sent_duration * (word_chars / total_chars)
                    # ìµœì†Œ ì‹œê°„ ë³´ì¥ (ë„ˆë¬´ ì§§ìœ¼ë©´ ì•ˆ ë³´ì„)
                    if word_duration < 0.2:
                        word_duration = 0.2

                    word_start = current_time
                    word_end = current_time + word_duration
                    current_time = word_end

                    word_timings.append({
                        "word": word,
                        "start": word_start,
                        "end": word_end
                    })

        if not word_timings:
            logger.warning(f"íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ìŒ! Chunk types: {chunk_types_seen}")

        # ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥
        with open(output_path, "wb") as f:
            f.write(audio_data)

        # ì˜¤ë””ì˜¤ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°
        try:
            audio_clip = AudioFileClip(str(output_path))
            duration = audio_clip.duration
            audio_clip.close()
        except Exception as e:
            logger.warning(f"ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì‹¤íŒ¨, ê¸°ë³¸ê°’ 1ì´ˆ ì‚¬ìš©: {e}")
            duration = 1.0

        if word_timings:
            logger.info(f"TTS ìƒì„± ì™„ë£Œ: {duration:.2f}ì´ˆ, ë‹¨ì–´ {len(word_timings)}ê°œ (íƒ€ì„ìŠ¤íƒ¬í”„ ìˆìŒ)")
        else:
            logger.warning(f"TTS ìƒì„± ì™„ë£Œ: {duration:.2f}ì´ˆ, íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ìŒ (ëŒ€ë³¸ ê¸°ë°˜ìœ¼ë¡œ í´ë°± ì˜ˆì •)")

        return duration, word_timings

    async def _generate_edge_tts_chunked(self, text: str, output_path: Path) -> tuple:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ  Edge TTS ìƒì„± í›„ ë³‘í•©"""
        logger.info(f"[CHUNKED TTS] ê¸´ í…ìŠ¤íŠ¸ ì¡°ê° ì²˜ë¦¬ ì‹œì‘: {len(text)}ì")

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‚˜ëˆ” (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ê¸°ì¤€)
        import re
        sentences = re.split(r'([.!?]\s+)', text)

        # ë¶„ë¦¬ìë„ í¬í•¨í•˜ì—¬ ì¬ì¡°í•©
        full_sentences = []
        for i in range(0, len(sentences)-1, 2):
            if i+1 < len(sentences):
                full_sentences.append(sentences[i] + sentences[i+1])
            else:
                full_sentences.append(sentences[i])
        if len(sentences) % 2 == 1:
            full_sentences.append(sentences[-1])

        # 5000ìì”© ì²­í¬ë¡œ ë¬¶ê¸°
        MAX_CHUNK_SIZE = 5000
        chunks = []
        current_chunk = ""

        for sentence in full_sentences:
            if len(current_chunk) + len(sentence) <= MAX_CHUNK_SIZE:
                current_chunk += sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence

        if current_chunk:
            chunks.append(current_chunk)

        logger.info(f"[CHUNKED TTS] {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

        # ê° ì²­í¬ì— ëŒ€í•´ TTS ìƒì„±
        all_audio_data = []
        all_word_timings = []
        cumulative_time = 0.0

        for idx, chunk in enumerate(chunks):
            logger.info(f"[CHUNKED TTS] ì²­í¬ {idx+1}/{len(chunks)} ì²˜ë¦¬ ì¤‘... ({len(chunk)}ì)")

            # Edge TTS ìƒì„±
            communicate = edge_tts.Communicate(chunk, self.voice, rate='-15%')

            word_timings = []
            audio_data = b""

            async for chunk_data in communicate.stream():
                chunk_type = chunk_data.get("type", "unknown")

                if chunk_type == "audio":
                    audio_data += chunk_data["data"]
                elif chunk_type == "WordBoundary":
                    # ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥ (ëˆ„ì  ì‹œê°„ ì¶”ê°€)
                    word_timings.append({
                        "word": chunk_data["text"],
                        "start": chunk_data["offset"] / 10_000_000.0 + cumulative_time,
                        "end": (chunk_data["offset"] + chunk_data["duration"]) / 10_000_000.0 + cumulative_time
                    })

            # ì„ì‹œ íŒŒì¼ì— ì˜¤ë””ì˜¤ ì €ì¥
            temp_path = output_path.parent / f"temp_chunk_{idx}.mp3"
            with open(temp_path, "wb") as f:
                f.write(audio_data)

            # ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •
            try:
                audio_clip = AudioFileClip(str(temp_path))
                chunk_duration = audio_clip.duration
                audio_clip.close()
            except Exception as e:
                logger.warning(f"ì²­í¬ {idx} ê¸¸ì´ ì¸¡ì • ì‹¤íŒ¨: {e}")
                chunk_duration = 1.0

            all_audio_data.append(audio_data)
            all_word_timings.extend(word_timings)
            cumulative_time += chunk_duration

            logger.info(f"[CHUNKED TTS] ì²­í¬ {idx+1} ì™„ë£Œ: {chunk_duration:.2f}ì´ˆ, ë‹¨ì–´ {len(word_timings)}ê°œ")

        # ëª¨ë“  ì˜¤ë””ì˜¤ ì¡°ê°ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë³‘í•©
        logger.info(f"[CHUNKED TTS] {len(all_audio_data)}ê°œ ì˜¤ë””ì˜¤ ì²­í¬ ë³‘í•© ì¤‘...")

        # ì„ì‹œ íŒŒì¼ë“¤ì„ ffmpegë¡œ ë³‘í•©
        temp_list_path = output_path.parent / "temp_concat_list.txt"
        with open(temp_list_path, "w", encoding="utf-8") as f:
            for idx in range(len(chunks)):
                temp_chunk_path = output_path.parent / f"temp_chunk_{idx}.mp3"
                # Windows ê²½ë¡œë¥¼ ìœ ë‹‰ìŠ¤ ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜ (ffmpeg í˜¸í™˜)
                unix_path = str(temp_chunk_path).replace('\\', '/')
                f.write(f"file '{unix_path}'\n")

        # ffmpegë¡œ ë³‘í•©
        import subprocess
        ffmpeg_cmd = [
            'ffmpeg', '-y', '-f', 'concat', '-safe', '0',
            '-i', str(temp_list_path),
            '-c', 'copy',
            str(output_path)
        ]

        try:
            subprocess.run(ffmpeg_cmd, check=True, capture_output=True)
            logger.info(f"[CHUNKED TTS] ë³‘í•© ì™„ë£Œ: {output_path.name}")
        except subprocess.CalledProcessError as e:
            logger.error(f"[CHUNKED TTS] ffmpeg ë³‘í•© ì‹¤íŒ¨: {e.stderr.decode()}")
            # í´ë°±: ì²« ë²ˆì§¸ ì²­í¬ë§Œ ì‚¬ìš©
            with open(output_path, "wb") as f:
                f.write(all_audio_data[0])
            logger.warning("[CHUNKED TTS] í´ë°±: ì²« ë²ˆì§¸ ì²­í¬ë§Œ ì €ì¥")

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        for idx in range(len(chunks)):
            temp_chunk_path = output_path.parent / f"temp_chunk_{idx}.mp3"
            try:
                temp_chunk_path.unlink()
            except:
                pass
        try:
            temp_list_path.unlink()
        except:
            pass

        # ìµœì¢… ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •
        try:
            audio_clip = AudioFileClip(str(output_path))
            total_duration = audio_clip.duration
            audio_clip.close()
        except Exception as e:
            logger.warning(f"ìµœì¢… ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì‹¤íŒ¨: {e}")
            total_duration = cumulative_time

        logger.info(f"[CHUNKED TTS] ì „ì²´ ì™„ë£Œ: {total_duration:.2f}ì´ˆ, ì´ ë‹¨ì–´ {len(all_word_timings)}ê°œ")

        return total_duration, all_word_timings

    async def _generate_google_tts(self, text: str, output_path: Path) -> tuple:
        """Google Cloud TTSë¡œ ìŒì„± ìƒì„± + ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ"""
        logger.info(f"Google Cloud TTS ìƒì„± ì¤‘: {output_path.name}")

        # í…ìŠ¤íŠ¸ ì •ë¦¬
        clean_text = self._clean_narration(text)

        if not clean_text:
            logger.warning("í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ ê¸°ë³¸ ë©”ì‹œì§€ ì‚¬ìš©")
            clean_text = "ë¬´ìŒ"

        try:
            # Google Cloud TTS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            client = texttospeech.TextToSpeechClient()

            # ìŒì„± ë§¤í•‘ (google-ko-KR-Neural2-A -> ko-KR-Neural2-A)
            voice_name = self.voice.replace('google-', '')

            # TTS ìš”ì²­ ì„¤ì •
            synthesis_input = texttospeech.SynthesisInput(text=clean_text)
            voice = texttospeech.VoiceSelectionParams(
                language_code="ko-KR",
                name=voice_name
            )
            audio_config = texttospeech.AudioConfig(
                audio_encoding=texttospeech.AudioEncoding.MP3,
                speaking_rate=0.85,  # Edge TTSì˜ -15%ì™€ ìœ ì‚¬
                effects_profile_id=['small-bluetooth-speaker-class-device']
            )

            # TTS ìƒì„± (word-level timestamps í¬í•¨)
            response = client.synthesize_speech(
                input=synthesis_input,
                voice=voice,
                audio_config=audio_config,
                enable_time_pointing=[texttospeech.SynthesisInput.TimepointType.SSML_MARK]
            )

            # ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥
            with open(output_path, "wb") as f:
                f.write(response.audio_content)

            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ (Google TTS timepoints)
            word_timings = []
            if hasattr(response, 'timepoints') and response.timepoints:
                for i, timepoint in enumerate(response.timepoints):
                    word_timings.append({
                        "word": timepoint.mark_name,
                        "start": timepoint.time_seconds,
                        "end": response.timepoints[i + 1].time_seconds if i + 1 < len(response.timepoints) else timepoint.time_seconds + 0.5
                    })

            # ì˜¤ë””ì˜¤ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°
            try:
                audio_clip = AudioFileClip(str(output_path))
                duration = audio_clip.duration
                audio_clip.close()
            except Exception as e:
                logger.warning(f"ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì‹¤íŒ¨, ê¸°ë³¸ê°’ 1ì´ˆ ì‚¬ìš©: {e}")
                duration = 1.0

            # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
            if not word_timings:
                logger.warning("Google TTSì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ê¸°ë°˜ í´ë°± ì‚¬ìš©")
                words = clean_text.split()
                time_per_word = duration / len(words) if words else duration
                for i, word in enumerate(words):
                    word_timings.append({
                        "word": word,
                        "start": i * time_per_word,
                        "end": (i + 1) * time_per_word
                    })

            logger.info(f"Google TTS ìƒì„± ì™„ë£Œ: {duration:.2f}ì´ˆ, ë‹¨ì–´ {len(word_timings)}ê°œ")
            return duration, word_timings

        except Exception as e:
            logger.error(f"Google Cloud TTS ì‹¤íŒ¨: {e}")
            logger.warning("Edge TTSë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            # Edge TTSë¡œ í´ë°±
            self.tts_provider = 'edge'
            self.voice = "ko-KR-SoonBokNeural"
            return await self._generate_edge_tts(text, output_path)

    async def _generate_aws_polly(self, text: str, output_path: Path) -> tuple:
        """AWS Pollyë¡œ ìŒì„± ìƒì„± + ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ"""
        logger.info(f"AWS Polly ìƒì„± ì¤‘: {output_path.name}")

        # í…ìŠ¤íŠ¸ ì •ë¦¬
        clean_text = self._clean_narration(text)

        if not clean_text:
            logger.warning("í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ ê¸°ë³¸ ë©”ì‹œì§€ ì‚¬ìš©")
            clean_text = "ë¬´ìŒ"

        try:
            # AWS Polly í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            polly_client = boto3.client('polly', region_name='us-east-1')

            # ìŒì„± ë§¤í•‘ (aws-Seoyeon -> Seoyeon)
            voice_id = self.voice.replace('aws-', '')

            # Speech marks ìš”ì²­ (íƒ€ì„ìŠ¤íƒ¬í”„ìš©)
            marks_response = polly_client.synthesize_speech(
                Text=clean_text,
                OutputFormat='json',
                VoiceId=voice_id,
                Engine='neural',
                SpeechMarkTypes=['word'],
                LanguageCode='ko-KR'
            )

            # ì˜¤ë””ì˜¤ ìš”ì²­
            audio_response = polly_client.synthesize_speech(
                Text=clean_text,
                OutputFormat='mp3',
                VoiceId=voice_id,
                Engine='neural',
                LanguageCode='ko-KR'
            )

            # ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥
            with open(output_path, "wb") as f:
                f.write(audio_response['AudioStream'].read())

            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
            word_timings = []
            marks_data = marks_response['AudioStream'].read().decode('utf-8')
            for line in marks_data.strip().split('\n'):
                if line:
                    mark = json.loads(line)
                    if mark['type'] == 'word':
                        word_timings.append({
                            "word": mark['value'],
                            "start": mark['time'] / 1000.0,  # ms -> s
                            "end": mark['time'] / 1000.0 + 0.3  # ì„ì‹œ duration
                        })

            # ì˜¤ë””ì˜¤ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°
            try:
                audio_clip = AudioFileClip(str(output_path))
                duration = audio_clip.duration
                audio_clip.close()
            except Exception as e:
                logger.warning(f"ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì‹¤íŒ¨, ê¸°ë³¸ê°’ 1ì´ˆ ì‚¬ìš©: {e}")
                duration = 1.0

            # end ì‹œê°„ ì¡°ì • (ë‹¤ìŒ ë‹¨ì–´ ì‹œì‘ ì‹œê°„ ë˜ëŠ” duration ê¸°ì¤€)
            for i in range(len(word_timings) - 1):
                word_timings[i]['end'] = word_timings[i + 1]['start']
            if word_timings:
                word_timings[-1]['end'] = duration

            logger.info(f"AWS Polly ìƒì„± ì™„ë£Œ: {duration:.2f}ì´ˆ, ë‹¨ì–´ {len(word_timings)}ê°œ")
            return duration, word_timings

        except (BotoCoreError, ClientError) as e:
            logger.error(f"AWS Polly ì‹¤íŒ¨: {e}")
            logger.warning("Edge TTSë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            # Edge TTSë¡œ í´ë°±
            self.tts_provider = 'edge'
            self.voice = "ko-KR-SoonBokNeural"
            return await self._generate_edge_tts(text, output_path)
        except Exception as e:
            logger.error(f"AWS Polly ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            logger.warning("Edge TTSë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            # Edge TTSë¡œ í´ë°±
            self.tts_provider = 'edge'
            self.voice = "ko-KR-SoonBokNeural"
            return await self._generate_edge_tts(text, output_path)

    def _get_video_duration(self, video_path: Path) -> float:
        """FFprobeë¡œ ë¹„ë””ì˜¤ ê¸¸ì´ í™•ì¸"""
        try:
            cmd = [
                'ffprobe',
                '-v', 'error',
                '-show_entries', 'format=duration',
                '-of', 'default=noprint_wrappers=1:nokey=1',
                str(video_path.resolve())
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            return float(result.stdout.strip())
        except Exception as e:
            logger.warning(f"âš ï¸ ë¹„ë””ì˜¤ ê¸¸ì´ í™•ì¸ ì‹¤íŒ¨: {e}")
            return 0.0

    def _combine_video_audio(self, scene_num: int, video_path: Path,
                            audio_path: Path, output_path: Path) -> Optional[Path]:
        """ë¹„ë””ì˜¤ íŒŒì¼ì— ì˜¤ë””ì˜¤ ê²°í•© - FFmpeg ì§ì ‘ ì‚¬ìš© (ìë§‰ ì—†ìŒ)"""
        try:
            logger.info(f"ì”¬ {scene_num} ë¹„ë””ì˜¤ì— ì˜¤ë””ì˜¤ ê²°í•© ì¤‘...")

            # FFmpeg ëª…ë ¹ì–´ë¡œ ë¹„ë””ì˜¤ + ì˜¤ë””ì˜¤ ê²°í•©
            # ìë§‰ì´ ì—†ìœ¼ë¯€ë¡œ -c:v copy ì‚¬ìš© (ë¹ ë¦„)
            cmd = [
                'ffmpeg',
                '-y',
                '-i', str(video_path.resolve()),  # ì…ë ¥ ë¹„ë””ì˜¤
                '-i', str(audio_path.resolve()),  # ì…ë ¥ ì˜¤ë””ì˜¤
                '-c:v', 'copy',  # ë¹„ë””ì˜¤ ì¬ì¸ì½”ë”© ì—†ì´ ë³µì‚¬ (ë¹ ë¦„)
                '-c:a', 'aac',  # ì˜¤ë””ì˜¤ AAC ì¸ì½”ë”©
                '-map', '0:v:0',  # ì²« ë²ˆì§¸ ì…ë ¥ì˜ ë¹„ë””ì˜¤
                '-map', '1:a:0',  # ë‘ ë²ˆì§¸ ì…ë ¥ì˜ ì˜¤ë””ì˜¤
                '-y',  # ë®ì–´ì“°ê¸°
                str(output_path.resolve())  # ì¶œë ¥ ê²½ë¡œ
            ]

            result = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8', errors='ignore')
            logger.info(f"ì”¬ {scene_num} ë¹„ë””ì˜¤+ì˜¤ë””ì˜¤ ê²°í•© ì™„ë£Œ: {output_path}")
            return output_path

        except subprocess.CalledProcessError as e:
            logger.error(f"ì”¬ {scene_num} ë¹„ë””ì˜¤+ì˜¤ë””ì˜¤ ê²°í•© ì‹¤íŒ¨: {e.stderr}")
            return None
        except Exception as e:
            logger.error(f"ì”¬ {scene_num} ë¹„ë””ì˜¤+ì˜¤ë””ì˜¤ ê²°í•© ì‹¤íŒ¨: {e}")
            return None

    def _combine_video_audio_with_subtitles(self, scene_num: int, video_path: Path,
                                           audio_path: Path, output_path: Path,
                                           narration: str, audio_duration: float,
                                           word_timings: list = None) -> Optional[Path]:
        """ë¹„ë””ì˜¤ íŒŒì¼ì— ì˜¤ë””ì˜¤ì™€ ìë§‰ ê²°í•© - FFmpeg ì§ì ‘ ì‚¬ìš© (ì˜ìƒë³‘í•© ë°©ì‹)"""
        try:
            logger.info(f"ì”¬ {scene_num} ë¹„ë””ì˜¤ì— ì˜¤ë””ì˜¤+ìë§‰ ê²°í•© ì¤‘...")

            # ë¹„ë””ì˜¤ ê¸¸ì´ í™•ì¸
            video_duration = self._get_video_duration(video_path)
            logger.info(f"â±ï¸ ë¹„ë””ì˜¤ ê¸¸ì´: {video_duration:.2f}ì´ˆ, ì˜¤ë””ì˜¤ ê¸¸ì´: {audio_duration:.2f}ì´ˆ")

            # Edge TTS íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ASS ìë§‰ íŒŒì¼ ìƒì„± (ë˜ëŠ” ëŒ€ë³¸ ê¸°ë°˜ í´ë°±)
            srt_path = audio_path.with_suffix('.srt')
            ass_path = self._create_srt_with_timings(word_timings or [], srt_path, narration, audio_duration, max_chars_per_line=22)

            # FFmpeg ass í•„í„°ì— ì ˆëŒ€ ê²½ë¡œ ì „ë‹¬ (Windows ê²½ë¡œë¥¼ Unix ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜ + ì½œë¡  ì´ìŠ¤ì¼€ì´í”„)
            ass_absolute_path = str(ass_path.resolve()).replace('\\', '/').replace(':', '\\\\:')

            # ë¹„ë””ì˜¤ì™€ ì˜¤ë””ì˜¤ ê¸¸ì´ ë¹„êµí•˜ì—¬ í•„í„° ì¤€ë¹„ (ì˜ìƒë³‘í•© ë°©ì‹)
            video_filter_parts = []
            audio_filter = None

            if video_duration < audio_duration:
                # ë¹„ë””ì˜¤ê°€ ì§§ìœ¼ë©´: ë§ˆì§€ë§‰ í”„ë ˆì„ì„ freezeí•˜ì—¬ ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ì¶¤
                freeze_duration = audio_duration - video_duration
                video_filter_parts.append(f"tpad=stop_mode=clone:stop_duration={freeze_duration:.3f}")
                logger.info(f"âš ï¸ ë¹„ë””ì˜¤ê°€ TTSë³´ë‹¤ ì§§ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ í”„ë ˆì„ì„ {freeze_duration:.2f}ì´ˆ freezeí•©ë‹ˆë‹¤.")
            elif audio_duration < video_duration:
                # ì˜¤ë””ì˜¤ê°€ ì§§ìœ¼ë©´: ë¬´ìŒ ì¶”ê°€í•˜ì—¬ ë¹„ë””ì˜¤ ê¸¸ì´ì— ë§ì¶¤
                audio_filter = f"apad=whole_dur={video_duration:.3f}"
                logger.info(f"âš ï¸ TTSê°€ ë¹„ë””ì˜¤ë³´ë‹¤ ì§§ìŠµë‹ˆë‹¤. ë¬´ìŒì„ ì¶”ê°€í•˜ì—¬ ë¹„ë””ì˜¤ ê¸¸ì´ì— ë§ì¶¥ë‹ˆë‹¤.")

            # ìë§‰ í•„í„° ì¶”ê°€
            video_filter_parts.append(f"ass={ass_absolute_path}")
            vf_combined = ",".join(video_filter_parts)

            # FFmpeg ëª…ë ¹ì–´ë¡œ ë¹„ë””ì˜¤ + ì˜¤ë””ì˜¤ + ìë§‰ ê²°í•©
            cmd = [
                'ffmpeg',
                '-y',
                '-i', str(video_path.resolve()),  # ì…ë ¥ ë¹„ë””ì˜¤
                '-i', str(audio_path.resolve()),  # ì…ë ¥ ì˜¤ë””ì˜¤
                '-vf', vf_combined,  # ë¹„ë””ì˜¤ í•„í„° (tpad + ass)
                '-c:v', self.video_codec,  # ë¹„ë””ì˜¤ ì¬ì¸ì½”ë”© (ìë§‰ ë•Œë¬¸ì—)
                '-preset', self.codec_preset,
                '-c:a', 'aac',  # ì˜¤ë””ì˜¤ AAC ì¸ì½”ë”©
                '-map', '0:v:0',  # ì²« ë²ˆì§¸ ì…ë ¥ì˜ ë¹„ë””ì˜¤
                '-map', '1:a:0',  # ë‘ ë²ˆì§¸ ì…ë ¥ì˜ ì˜¤ë””ì˜¤
                '-pix_fmt', 'yuv420p',  # í˜¸í™˜ì„±
            ]

            # ì˜¤ë””ì˜¤ í•„í„° ì¶”ê°€ (íŒ¨ë”©ì´ í•„ìš”í•œ ê²½ìš°)
            if audio_filter:
                cmd.extend(['-af', audio_filter])

            cmd.extend([
                '-y',  # ë®ì–´ì“°ê¸°
                str(output_path.resolve())  # ì¶œë ¥ ê²½ë¡œ
            ])

            result = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8', errors='ignore')
            logger.info(f"ì”¬ {scene_num} ë¹„ë””ì˜¤+ì˜¤ë””ì˜¤+ìë§‰ ê²°í•© ì™„ë£Œ: {output_path}")

            # ìë§‰ íŒŒì¼ ì‚­ì œ
            if ass_path.exists():
                ass_path.unlink()
            if srt_path.exists():
                srt_path.unlink()

            return output_path

        except subprocess.CalledProcessError as e:
            # GPU ì¸ì½”ë” ì‹¤íŒ¨ ì‹œ CPU í´ë°±
            if 'h264_nvenc' in str(e.stderr) or 'nvenc' in str(e.stderr):
                logger.warning(f"ì”¬ {scene_num} GPU ì¸ì½”ë” ì‹¤íŒ¨, CPU ì¸ì½”ë”ë¡œ ì¬ì‹œë„...")
                try:
                    cmd_cpu = [
                        'ffmpeg',
                        '-y',
                        '-i', str(video_path.resolve()),
                        '-i', str(audio_path.resolve()),
                        '-vf', vf_combined,  # ë¹„ë””ì˜¤ í•„í„° (tpad + ass)
                        '-c:v', 'libx264',  # CPU ì¸ì½”ë”
                        '-preset', 'ultrafast',
                        '-c:a', 'aac',
                        '-map', '0:v:0',
                        '-map', '1:a:0',
                        '-pix_fmt', 'yuv420p',
                    ]

                    if audio_filter:
                        cmd_cpu.extend(['-af', audio_filter])

                    cmd_cpu.extend(['-y', str(output_path.resolve())])

                    result = subprocess.run(cmd_cpu, check=True, capture_output=True, text=True, encoding='utf-8', errors='ignore')
                    logger.info(f"ì”¬ {scene_num} ë¹„ë””ì˜¤+ì˜¤ë””ì˜¤+ìë§‰ ê²°í•© ì™„ë£Œ (CPU): {output_path}")

                    if ass_path.exists():
                        ass_path.unlink()
                    if srt_path.exists():
                        srt_path.unlink()

                    return output_path
                except subprocess.CalledProcessError as e2:
                    logger.error(f"ì”¬ {scene_num} CPU ì¸ì½”ë”ë„ ì‹¤íŒ¨: {e2.stderr}")
                    return None
            else:
                logger.error(f"ì”¬ {scene_num} ë¹„ë””ì˜¤+ì˜¤ë””ì˜¤+ìë§‰ ê²°í•© ì‹¤íŒ¨: {e.stderr}")
                return None
        except Exception as e:
            logger.error(f"ì”¬ {scene_num} ë¹„ë””ì˜¤+ì˜¤ë””ì˜¤+ìë§‰ ê²°í•© ì‹¤íŒ¨: {e}")
            return None

    def _create_scene_video(self, scene_num: int, image_path: Path,
                           audio_path: Path, output_path: Path) -> Optional[Path]:
        """ì”¬ ë¹„ë””ì˜¤ ìƒì„± (ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤) - FFmpeg ì§ì ‘ ì‚¬ìš©"""
        try:
            logger.info(f"ì”¬ {scene_num} ë¹„ë””ì˜¤ ìƒì„± ì¤‘...")

            # ============================================================
            # ìˆí¼ ì˜ìƒì¸ ê²½ìš° 16:9 ì´ë¯¸ì§€ë¥¼ 9:16ìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸ í¬ë¡­
            # ============================================================
            processed_image_path = image_path
            temp_image_file = None

            if self.aspect_ratio == "9:16":
                try:
                    # ì´ë¯¸ì§€ ë¹„ìœ¨ ì²´í¬
                    with PILImage.open(image_path) as img:
                        width, height = img.size
                        img_ratio = width / height

                        # 16:9 ë¹„ìœ¨ (ê°€ë¡œ ì˜ìƒ)ì¸ì§€ í™•ì¸ (í—ˆìš© ì˜¤ì°¨ Â±10%)
                        target_landscape_ratio = 16 / 9  # 1.778
                        is_landscape = abs(img_ratio - target_landscape_ratio) < 0.2

                        if is_landscape:
                            logger.info(f"  ğŸ¨ ì”¬ {scene_num}: ë¡±í¼ ì´ë¯¸ì§€ ê°ì§€ ({width}x{height}, ë¹„ìœ¨: {img_ratio:.3f})")
                            logger.info(f"  âœ‚ï¸ ìŠ¤ë§ˆíŠ¸ í¬ë¡­ ì ìš© ì¤‘ (ì–¼êµ´/ë¬¼ì²´ ì¤‘ì‹¬)...")

                            # ì„ì‹œ íŒŒì¼ ìƒì„±
                            temp_image_file = image_path.parent / f"temp_cropped_scene_{scene_num}.jpg"

                            # ìŠ¤ë§ˆíŠ¸ í¬ë¡­ ì ìš©
                            if self._smart_crop_to_vertical(image_path, temp_image_file):
                                processed_image_path = temp_image_file
                                logger.info(f"  âœ… ìŠ¤ë§ˆíŠ¸ í¬ë¡­ ì™„ë£Œ: {temp_image_file.name}")
                            else:
                                logger.warning(f"  âš ï¸ ìŠ¤ë§ˆíŠ¸ í¬ë¡­ ì‹¤íŒ¨, ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš© (ë ˆí„°ë°•ìŠ¤ ì ìš©)")
                except Exception as e:
                    logger.warning(f"  âš ï¸ ì´ë¯¸ì§€ ë¹„ìœ¨ ì²´í¬ ì‹¤íŒ¨: {e}, ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©")

            # FFmpeg ëª…ë ¹ì–´ë¡œ ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤ ê²°í•© (ì´ˆê³ ì†)
            # -loop 1: ì´ë¯¸ì§€ ë°˜ë³µ
            # -i: ì…ë ¥ íŒŒì¼
            # -shortest: ì˜¤ë””ì˜¤ ê¸¸ì´ë§Œí¼ë§Œ
            # -c:v: ë¹„ë””ì˜¤ ì½”ë± (GPU ê°€ì†)
            # -c:a: ì˜¤ë””ì˜¤ ì½”ë±
            # -pix_fmt yuv420p: í˜¸í™˜ì„±
            # -vf scale: ë¦¬ìŠ¤ì¼€ì¼ + ë ˆí„°ë°•ìŠ¤

            cmd = [
                'ffmpeg',
                '-loop', '1',  # ì´ë¯¸ì§€ ë°˜ë³µ
                '-i', str(image_path.resolve()),  # ì…ë ¥ ì´ë¯¸ì§€ (ì ˆëŒ€ ê²½ë¡œ)
                '-i', str(audio_path.resolve()),  # ì…ë ¥ ì˜¤ë””ì˜¤ (ì ˆëŒ€ ê²½ë¡œ)
                '-vf', f"scale={self.width}:{self.height}:force_original_aspect_ratio=decrease,pad={self.width}:{self.height}:(ow-iw)/2:(oh-ih)/2:black",  # ë¦¬ìŠ¤ì¼€ì¼ + ë ˆí„°ë°•ìŠ¤
                '-c:v', self.video_codec,  # GPU ê°€ì† ì½”ë±
                '-preset', self.codec_preset,  # í”„ë¦¬ì…‹
                '-c:a', 'aac',  # ì˜¤ë””ì˜¤ ì½”ë±
                '-shortest',  # ì˜¤ë””ì˜¤ ê¸¸ì´ë§Œí¼
                '-pix_fmt', 'yuv420p',  # í˜¸í™˜ì„±
                '-y',  # ë®ì–´ì“°ê¸°
                str(output_path.resolve())  # ì¶œë ¥ ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œ)
            ]

            # FFmpeg ì‹¤í–‰ (UTF-8 ì¸ì½”ë”©)
            result = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8', errors='ignore')

            logger.info(f"ì”¬ {scene_num} ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {output_path}")
            return output_path

        except subprocess.CalledProcessError as e:
            # GPU ì¸ì½”ë” ì‹¤íŒ¨ ì‹œ CPU í´ë°±
            if 'h264_nvenc' in str(e.stderr) or 'nvenc' in str(e.stderr):
                logger.warning(f"ì”¬ {scene_num} GPU ì¸ì½”ë” ì‹¤íŒ¨, CPU ì¸ì½”ë”ë¡œ ì¬ì‹œë„...")

                # CPU ì¸ì½”ë”ë¡œ ì¬ì‹œë„
                cmd_cpu = [
                    'ffmpeg',
                    '-loop', '1',
                    '-i', str(image_path.resolve()),
                    '-i', str(audio_path.resolve()),
                    '-vf', f"scale={self.width}:{self.height}:force_original_aspect_ratio=decrease,pad={self.width}:{self.height}:(ow-iw)/2:(oh-ih)/2:black",
                    '-c:v', 'libx264',  # CPU ì¸ì½”ë”
                    '-preset', 'ultrafast',
                    '-c:a', 'aac',
                    '-shortest',
                    '-pix_fmt', 'yuv420p',
                    '-y',
                    str(output_path.resolve())
                ]

                try:
                    result = subprocess.run(cmd_cpu, check=True, capture_output=True, text=True, encoding='utf-8', errors='ignore')
                    logger.info(f"ì”¬ {scene_num} ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ (CPU): {output_path}")
                    return output_path
                except subprocess.CalledProcessError as e2:
                    logger.error(f"ì”¬ {scene_num} CPU ì¸ì½”ë”ë„ ì‹¤íŒ¨: {e2.stderr}")
                    return None
            else:
                logger.error(f"ì”¬ {scene_num} FFmpeg ì‹¤í–‰ ì‹¤íŒ¨: {e.stderr}")
                return None
        except Exception as e:
            logger.error(f"ì”¨ {scene_num} ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _create_scene_video_with_subtitles(self, scene_num: int, image_path: Path,
                                           audio_path: Path, output_path: Path,
                                           narration: str, audio_duration: float,
                                           word_timings: list = None) -> Optional[Path]:
        """ì”¬ ë¹„ë””ì˜¤ ìƒì„± (ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤ + ìë§‰ í¬í•¨) - FFmpeg ì§ì ‘ ì‚¬ìš©"""
        try:
            logger.info(f"ì”¬ {scene_num} ë¹„ë””ì˜¤ + ìë§‰ ìƒì„± ì¤‘...")

            # Edge TTS íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ASS ìë§‰ íŒŒì¼ ìƒì„± (ë˜ëŠ” ëŒ€ë³¸ ê¸°ë°˜ í´ë°±)
            srt_path = audio_path.with_suffix('.srt')
            ass_path = self._create_srt_with_timings(word_timings or [], srt_path, narration, audio_duration, max_chars_per_line=22)

            # FFmpeg ass í•„í„°ì—ëŠ” íŒŒì¼ëª…ë§Œ ì „ë‹¬ (ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆìŒ)
            ass_filename = ass_path.name
            logger.info(f"DEBUG ì”¬ {scene_num}: ass_filename = {ass_filename}")

            # FFmpeg ëª…ë ¹ì–´: ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤ + ìë§‰ì„ í•œë²ˆì— ì²˜ë¦¬ (ass í•„í„° ì‚¬ìš©)
            cmd = [
                'ffmpeg',
                '-loop', '1',
                '-i', str(image_path.resolve()),
                '-i', str(audio_path.resolve()),
                '-vf', f"scale={self.width}:{self.height}:force_original_aspect_ratio=decrease,pad={self.width}:{self.height}:(ow-iw)/2:(oh-ih)/2:black,ass={ass_filename}",
                '-c:v', self.video_codec,
                '-preset', self.codec_preset,
                '-c:a', 'aac',
                '-shortest',
                '-pix_fmt', 'yuv420p',
                '-y',
                str(output_path.resolve())
            ]

            logger.info(f"DEBUG ì”¬ {scene_num}: FFmpeg ëª…ë ¹ì–´ = {' '.join(cmd)}")
            result = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8', errors='ignore', cwd=str(output_path.parent))
            if result.stderr and 'error' in result.stderr.lower():
                logger.warning(f"FFmpeg ê²½ê³  (ì”¬ {scene_num}): {result.stderr[:500]}")
            logger.info(f"ì”¬ {scene_num} ë¹„ë””ì˜¤ + ìë§‰ ìƒì„± ì™„ë£Œ: {output_path}")
            return output_path

        except subprocess.CalledProcessError as e:
            # GPU ì¸ì½”ë” ì‹¤íŒ¨ ì‹œ CPU í´ë°± (stderr í™•ì¸ ë¶ˆê°€í•˜ë¯€ë¡œ íŒŒì¼ í¬ê¸°ë¡œ íŒë‹¨)
            if not output_path.exists() or output_path.stat().st_size == 0:
                logger.warning(f"ì”¬ {scene_num} GPU ì¸ì½”ë” ì‹¤íŒ¨, CPU ì¸ì½”ë”ë¡œ ì¬ì‹œë„...")

                # ASS ìë§‰ íŒŒì¼ ê²½ë¡œ (ì´ë¯¸ ìƒì„±ë¨)
                srt_path = audio_path.with_suffix('.srt')
                ass_path = srt_path.with_suffix('.ass')
                # FFmpeg ass í•„í„°ì—ëŠ” íŒŒì¼ëª…ë§Œ ì „ë‹¬
                ass_filename = ass_path.name

                cmd_cpu = [
                    'ffmpeg',
                    '-loop', '1',
                    '-i', str(image_path.resolve()),
                    '-i', str(audio_path.resolve()),
                    '-vf', f"scale={self.width}:{self.height}:force_original_aspect_ratio=decrease,pad={self.width}:{self.height}:(ow-iw)/2:(oh-ih)/2:black,ass={ass_filename}",
                    '-c:v', 'libx264',
                    '-preset', 'ultrafast',
                    '-c:a', 'aac',
                    '-shortest',
                    '-pix_fmt', 'yuv420p',
                    '-y',
                    str(output_path.resolve())
                ]

                try:
                    result_cpu = subprocess.run(cmd_cpu, check=True, capture_output=True, text=True, encoding='utf-8', errors='ignore', cwd=str(output_path.parent))
                    if result_cpu.stderr and 'error' in result_cpu.stderr.lower():
                        logger.warning(f"FFmpeg CPU ê²½ê³  (ì”¬ {scene_num}): {result_cpu.stderr[:500]}")
                    logger.info(f"ì”¬ {scene_num} CPU ì¸ì½”ë”ë¡œ ì„±ê³µ")
                    return output_path
                except subprocess.CalledProcessError as e2:
                    logger.error(f"ì”¬ {scene_num} CPU ì¸ì½”ë”ë„ ì‹¤íŒ¨: {e2.stderr if hasattr(e2, 'stderr') else str(e2)}")
                    return None
            else:
                logger.error(f"ì”¬ {scene_num} FFmpeg ì‹¤í–‰ ì‹¤íŒ¨")
                return None
        except Exception as e:
            logger.error(f"ì”¬ {scene_num} ë¹„ë””ì˜¤ + ìë§‰ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _create_scene_video_old_moviepy(self, scene_num: int, image_path: Path,
                           audio_path: Path, output_path: Path) -> Optional[Path]:
        """ì”¬ ë¹„ë””ì˜¤ ìƒì„± (ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤) - MoviePy ë²„ì „ (ëŠë¦¼, ë°±ì—…ìš©)"""
        try:
            logger.info(f"ì”¬ {scene_num} ë¹„ë””ì˜¤ ìƒì„± ì¤‘...")

            # ì˜¤ë””ì˜¤ í´ë¦½ ìƒì„± ë° ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°
            audio_clip = AudioFileClip(str(audio_path))
            duration = audio_clip.duration

            # ì´ë¯¸ì§€ í´ë¦½ ìƒì„±
            img_clip = ImageClip(str(image_path), duration=duration)

            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°
            orig_w, orig_h = img_clip.size
            logger.info(f"ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {orig_w}x{orig_h}, ëª©í‘œ í¬ê¸°: {self.width}x{self.height}")

            # ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ í™”ë©´ì— ë§ì¶”ê¸° (í¬ë¡­ ì—†ì´, ë ˆí„°ë°•ìŠ¤ë¡œ)
            target_ratio = self.width / self.height
            img_ratio = orig_w / orig_h

            if img_ratio > target_ratio:
                # ì´ë¯¸ì§€ê°€ ë” ë„“ìŒ - ë„ˆë¹„ë¥¼ í™”ë©´ì— ë§ì¶”ê³  ìœ„ì•„ë˜ ê²€ì€ ì—¬ë°±
                img_clip = img_clip.resize(width=self.width)
            else:
                # ì´ë¯¸ì§€ê°€ ë” ë†’ìŒ - ë†’ì´ë¥¼ í™”ë©´ì— ë§ì¶”ê³  ì¢Œìš° ê²€ì€ ì—¬ë°±
                img_clip = img_clip.resize(height=self.height)

            # ì¤‘ì•™ ì •ë ¬ (ê²€ì€ ë°°ê²½ì— ì´ë¯¸ì§€ ë°°ì¹˜)
            from moviepy.editor import ColorClip, CompositeVideoClip
            bg = ColorClip(size=(self.width, self.height), color=(0, 0, 0), duration=duration)
            img_clip = CompositeVideoClip([bg, img_clip.set_position('center')], size=(self.width, self.height))

            # ë¹„ë””ì˜¤ + ì˜¤ë””ì˜¤ ê²°í•©
            video = img_clip.set_audio(audio_clip)

            # ì €ì¥ (GPU ê°€ì† ì¸ì½”ë”©)
            video.write_videofile(
                str(output_path),
                fps=24,
                codec=self.video_codec,  # GPU ì¸ì½”ë” ë˜ëŠ” CPU
                audio_codec='aac',
                preset=self.codec_preset,  # ì¸ì½”ë”ì— ë§ëŠ” í”„ë¦¬ì…‹
                threads=4,  # ë©€í‹°ìŠ¤ë ˆë”© í™œì„±í™”
                logger=None
            )

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            img_clip.close()
            audio_clip.close()
            video.close()

            logger.info(f"ì”¬ {scene_num} ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {output_path}")
            return output_path

        except Exception as e:
            logger.error(f"ì”¬ {scene_num} ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _combine_videos(self, video_paths: List[Path], output_path: Path, start_time: float) -> Optional[Path]:
        """ì—¬ëŸ¬ ì”¬ ë¹„ë””ì˜¤ë¥¼ í•˜ë‚˜ë¡œ ê²°í•© - FFmpeg concat demuxer ì‚¬ìš©"""
        import tempfile

        # generated_videos í´ë”ì—ì„œ ì”¬ ë¹„ë””ì˜¤ ì°¾ê¸°
        video_folder = output_path.parent / "generated_videos"
        logger.info(f"ë¹„ë””ì˜¤ ê²°í•© ì‹œì‘: {len(video_paths)}ê°œ ì”¬")
        logger.info(f"ì”¬ ë¹„ë””ì˜¤ ê²€ìƒ‰ ê²½ë¡œ: {video_folder}")

        # scene_XX.mp4 íŒŒì¼ ì°¾ê¸°
        scene_videos = sorted(video_folder.glob("scene_*.mp4"))
        if not scene_videos:
            logger.error(f"ì”¬ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ê²½ë¡œ: {video_folder})")
            return None

        logger.info(f"ë°œê²¬ëœ ì”¬ ë¹„ë””ì˜¤: {len(scene_videos)}ê°œ")

        # concat list íŒŒì¼ ìƒì„± (ì„ì‹œ íŒŒì¼)
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
            concat_file = f.name
            for video in scene_videos:
                # FFmpeg concat demuxer í˜•ì‹: file 'path'
                # Windows ê²½ë¡œë¥¼ ìœ„í•´ forward slashë¡œ ë³€í™˜í•˜ê³  ì´ìŠ¤ì¼€ì´í”„
                video_path_str = str(video.absolute()).replace('\\', '/')
                f.write(f"file '{video_path_str}'\n")

        try:
            # FFmpeg concat demuxerë¡œ ë³‘í•©
            cmd = [
                'ffmpeg',
                '-f', 'concat',
                '-safe', '0',
                '-i', concat_file,
                '-c', 'copy',  # ì¬ì¸ì½”ë”© ì—†ì´ ë³µì‚¬
                '-y',  # ë®ì–´ì“°ê¸°
                str(output_path)
            ]

            logger.info(f"FFmpeg ì‹¤í–‰: {' '.join(cmd)}")
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='ignore'
            )

            if result.returncode != 0:
                logger.error(f"FFmpeg ì‹¤íŒ¨ (ì¢…ë£Œ ì½”ë“œ: {result.returncode})")
                if result.stderr:
                    logger.error(f"ì—ëŸ¬ ë©”ì‹œì§€:\n{result.stderr}")
                return None

            if not output_path.exists():
                logger.error(f"ë³‘í•©ëœ ë¹„ë””ì˜¤ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {output_path}")
                return None

            # ì´ ìˆ˜í–‰ ì‹œê°„
            elapsed_time = time() - start_time
            minutes = int(elapsed_time // 60)
            seconds = int(elapsed_time % 60)
            logger.info(f"ë¹„ë””ì˜¤ ê²°í•© ì™„ë£Œ: {output_path}")
            logger.info(f"ì´ ìˆ˜í–‰ ì‹œê°„: {minutes}ë¶„ {seconds}ì´ˆ")

            return output_path

        finally:
            # ì„ì‹œ concat íŒŒì¼ ì‚­ì œ
            try:
                Path(concat_file).unlink()
            except Exception as e:
                logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

    def _backup_previous_videos(self):
        """ê¸°ì¡´ generated_videos í´ë”ë¥¼ backupìœ¼ë¡œ ì´ë™ (íŒŒì¼ ì‚¬ìš© ì¤‘ì´ë©´ ê±´ë„ˆë›°ê¸°)"""
        import shutil
        from datetime import datetime

        output_folder = self.folder_path / "generated_videos"

        # generated_videos í´ë”ê°€ ìˆê³ , ë‚´ìš©ì´ ìˆìœ¼ë©´ ë°±ì—…
        if output_folder.exists() and any(output_folder.iterdir()):
            try:
                # backup í´ë” ìƒì„±
                backup_root = self.folder_path / "backup"
                backup_root.mkdir(exist_ok=True)

                # ë°±ì—… í´ë”ëª…: backup/YYYYMMDD_HHMMSS
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_folder = backup_root / timestamp

                # ì´ë™
                logger.info(f"ê¸°ì¡´ generated_videosë¥¼ ë°±ì—…í•©ë‹ˆë‹¤: {backup_folder.name}")
                shutil.move(str(output_folder), str(backup_folder))
                logger.info(f"ë°±ì—… ì™„ë£Œ: {backup_folder}")
            except PermissionError as e:
                # íŒŒì¼ì´ ì‚¬ìš© ì¤‘ì´ë©´ ë°±ì—…ì„ ê±´ë„ˆë›°ê³  ê¸°ì¡´ í´ë” ìœ ì§€
                logger.warning(f"âš ï¸  ë°±ì—… ì‹¤íŒ¨ (íŒŒì¼ ì‚¬ìš© ì¤‘)")
                logger.warning(f"   ê¸°ì¡´ generated_videos í´ë”ë¥¼ ìœ ì§€í•˜ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                logger.warning(f"   ğŸ’¡ ë¹„ë””ì˜¤ í”Œë ˆì´ì–´ë‚˜ íƒìƒ‰ê¸°ë¥¼ ë‹«ìœ¼ë©´ ë°±ì—…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                # í´ë”ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³  ë‚´ìš©ë¬¼ë§Œ ì‚­ì œ ì‹œë„
                try:
                    for item in output_folder.iterdir():
                        try:
                            if item.is_file():
                                item.unlink()
                            elif item.is_dir():
                                shutil.rmtree(item)
                        except PermissionError:
                            logger.warning(f"   íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ (ì‚¬ìš© ì¤‘): {item.name}")
                            continue
                except Exception as cleanup_error:
                    logger.warning(f"   í´ë” ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë¬´ì‹œí•˜ê³  ê³„ì†")
                    pass

    async def create_all_videos(self, combine: bool = True) -> Optional[Path]:
        """ëª¨ë“  ì”¬ì˜ ë¹„ë””ì˜¤ ìƒì„± ë° ê²°í•©"""
        start_time = time()

        # ê¸°ì¡´ generated_videos í´ë” ë°±ì—… (ë¹„í™œì„±í™” - backup í´ë” ìƒì„± ë°©ì§€)
        # self._backup_previous_videos()

        # ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸° (ìë™ ìƒì„± í¬í•¨)
        images_dict = self._find_images_with_scene_numbers()  # ì´ë¯¸ì§€ ìë™ ìƒì„± í¬í•¨
        videos_dict = self._find_videos()  # ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°

        # dictë¥¼ ì”¬ ë²ˆí˜¸ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ listë¡œ ë³€í™˜
        image_paths = [images_dict[k] for k in sorted(images_dict.keys())]
        video_paths = [videos_dict[k] for k in sorted(videos_dict.keys())]

        if not image_paths and not video_paths:
            logger.error("ì´ë¯¸ì§€ ë˜ëŠ” ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ 1ê°œ ì´ìƒì˜ ë¯¸ë””ì–´ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return None

        # ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ë¥¼ í†µí•© ì •ë ¬ (íƒ€ì… êµ¬ë¶„ ì—†ì´)
        logger.info(f"ğŸ“Š í†µí•© ì •ë ¬ ì‹œì‘: ì´ë¯¸ì§€ {len(image_paths)}ê°œ, ë¹„ë””ì˜¤ {len(video_paths)}ê°œ")

        # ëª¨ë“  ë¯¸ë””ì–´ íŒŒì¼ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
        all_media_files = []
        for path in image_paths:
            all_media_files.append(('image', path))
        for path in video_paths:
            all_media_files.append(('video', path))

        # í†µí•© ì •ë ¬ í•¨ìˆ˜
        def extract_sequence_unified(media_tuple):
            """ì‹œí€€ìŠ¤ ë²ˆí˜¸ ì¶”ì¶œ (íƒ€ì… ê´€ê³„ì—†ì´)"""
            media_type, filepath = media_tuple
            import re
            name = filepath.stem

            # íŒ¨í„´ ë§¤ì¹­ (image_01, video_02, scene_03, clip_01 ë“±)
            match = re.match(r'^(image|video|scene|clip|img)[-_](\d+)$', name, re.IGNORECASE)
            if match:
                return (int(match.group(2)), 0)

            match = re.match(r'^(image|video|scene|clip|img)\((\d+)\)$', name, re.IGNORECASE)
            if match:
                return (int(match.group(2)), 0)

            match = re.match(r'^\((\d+)\)$', name)
            if match:
                return (int(match.group(1)), 0)

            match = re.match(r'^(\d+)$', name)
            if match:
                return (int(match.group(1)), 0)

            # íŒŒì¼ëª… ì–´ë””ë“  ìˆ«ìê°€ ìˆìœ¼ë©´ ì¶”ì¶œ (ì˜ìƒ01, í•œê¸€01, abc123 ë“±)
            match = re.search(r'(\d+)', name)
            if match:
                return (int(match.group(1)), 0)

            # ìˆ«ìê°€ ì—†ìœ¼ë©´ íŒŒì¼ ì‹œê°„
            try:
                mtime = filepath.stat().st_mtime
            except:
                mtime = 0
            return (None, mtime)

        # ì •ë ¬: ì‹œí€€ìŠ¤ ë²ˆí˜¸ ìš°ì„ , ì—†ìœ¼ë©´ ì‹œê°„ ìˆœ
        all_media_files.sort(key=lambda f: (
            extract_sequence_unified(f)[0] is None,
            extract_sequence_unified(f)[0] if extract_sequence_unified(f)[0] is not None else 0,
            extract_sequence_unified(f)[1]
        ))

        # ì”¬ ë²ˆí˜¸ ì¬í• ë‹¹
        images = {}
        videos = {}
        logger.info(f"\nğŸ¯ í†µí•© ì •ë ¬ ê²°ê³¼ (ì´ {len(all_media_files)}ê°œ):")
        for idx, (media_type, filepath) in enumerate(all_media_files, start=1):
            if media_type == 'image':
                images[idx] = filepath
            else:
                videos[idx] = filepath

            seq_info = extract_sequence_unified((media_type, filepath))
            if seq_info[0] is not None:
                logger.info(f"  ì”¬ {idx}: {filepath.name} ({media_type.upper()}, ì‹œí€€ìŠ¤: {seq_info[0]})")
            else:
                import datetime
                mtime_str = datetime.datetime.fromtimestamp(seq_info[1]).strftime('%Y-%m-%d %H:%M:%S')
                logger.info(f"  ì”¬ {idx}: {filepath.name} ({media_type.upper()}, ì‹œê°„: {mtime_str})")

        logger.info(f"âœ… ìµœì¢…: ì´ë¯¸ì§€ {len(images)}ê°œ, ë¹„ë””ì˜¤ {len(videos)}ê°œ")

        # scenes ê°€ì ¸ì˜¤ê¸°
        scenes = self.story_data.get("scenes", [])

        if not scenes:
            logger.error("story.jsonì— scenesê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ë‚˜ë ˆì´ì…˜ì´ ìˆëŠ” ì”¬ ê°œìˆ˜ ê³„ì‚° (scene_num > 0)
        narration_count = 0
        for scene in scenes:
            scene_num = scene.get("scene_number")
            if scene_num is None:
                scene_id = scene.get("scene_id", "")
                import re
                match = re.search(r'scene_(\d+)', scene_id)
                if match:
                    scene_num = int(match.group(1))
            if scene_num and scene_num > 0:
                narration_count += 1

        # ë¯¸ë””ì–´ ê°œìˆ˜ (ì´ë¯¸ì§€ + ë¹„ë””ì˜¤)
        total_media = len(images) + len(videos)

        logger.info(f"ğŸ“ ë‚˜ë ˆì´ì…˜ ì”¬ ê°œìˆ˜: {narration_count}ê°œ")
        logger.info(f"ğŸ¬ ì´ ë¯¸ë””ì–´ ê°œìˆ˜: {total_media}ê°œ")

        # ë‚˜ë ˆì´ì…˜ì´ ë¯¸ë””ì–´ë³´ë‹¤ ë§ìœ¼ë©´ ë¯¸ë””ì–´ë¥¼ ê· ë“± ë¶„ë°°
        if narration_count > total_media and total_media > 0:
            logger.info(f"âš ï¸ ë‚˜ë ˆì´ì…˜({narration_count})ì´ ë¯¸ë””ì–´({total_media})ë³´ë‹¤ ë§ìŠµë‹ˆë‹¤.")
            logger.info(f"ğŸ“Š ë¯¸ë””ì–´ë¥¼ ê· ë“± ë¶„ë°°í•˜ì—¬ ê° ì”¬ì— í• ë‹¹í•©ë‹ˆë‹¤.")

            # ë¯¸ë””ì–´ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì´ë¯¸ í†µí•© ì •ë ¬ëœ ìˆœì„œëŒ€ë¡œ)
            # imagesì™€ videosì˜ í‚¤ë¥¼ í•©ì³ì„œ ì •ë ¬í•˜ë©´ í†µí•© ì •ë ¬ëœ ìˆœì„œê°€ ìœ ì§€ë¨
            all_media = []
            all_keys = sorted(set(images.keys()) | set(videos.keys()))

            for idx in all_keys:
                if idx in images:
                    all_media.append(('image', idx, images[idx]))
                elif idx in videos:
                    all_media.append(('video', idx, videos[idx]))

            # ê° ë¯¸ë””ì–´ê°€ ì²˜ë¦¬í•  ì”¬ ê°œìˆ˜ ê³„ì‚° (ê· ë“± ë¶„ë°°)
            scenes_per_media = narration_count // total_media  # ê¸°ë³¸ ê°œìˆ˜
            extra_scenes = narration_count % total_media  # ë‚˜ë¨¸ì§€ (ì¼ë¶€ ë¯¸ë””ì–´ì— +1)

            new_images = {}
            new_videos = {}
            current_scene = 1

            for media_idx, (media_type, orig_num, media_path) in enumerate(all_media):
                # ì´ ë¯¸ë””ì–´ê°€ ì²˜ë¦¬í•  ì”¬ ê°œìˆ˜ (ë‚˜ë¨¸ì§€ëŠ” ë’¤ì— í• ë‹¹)
                # ì˜ˆ: ëŒ€ë³¸3ê°œ/ì˜ìƒ2ê°œ â†’ ì˜ìƒ1(1ê°œ), ì˜ìƒ2(2ê°œ)
                remaining_media = total_media - media_idx
                if remaining_media > extra_scenes:
                    num_scenes = scenes_per_media
                else:
                    num_scenes = scenes_per_media + 1

                logger.info(f"  {media_type.upper()} {media_path.name} â†’ {num_scenes}ê°œ ì”¬ ì²˜ë¦¬")

                for _ in range(num_scenes):
                    if current_scene > narration_count:
                        break

                    # ì›ë³¸ ë¯¸ë””ì–´ íŒŒì¼ì„ ì”¬ì— í• ë‹¹ (í•­ìƒ ì›ë³¸ ì‚¬ìš©!)
                    if media_type == 'image':
                        new_images[current_scene] = media_path  # ì›ë³¸ ê²½ë¡œ
                    else:
                        new_videos[current_scene] = media_path  # ì›ë³¸ ê²½ë¡œ

                    logger.info(f"    ì”¬ {current_scene}: {media_path.name} (ì›ë³¸)")
                    current_scene += 1

            images = new_images
            videos = new_videos
            logger.info(f"âœ… ê· ë“± ë¶„ë°° ì™„ë£Œ: ì´ë¯¸ì§€ {len(images)}ê°œ, ë¹„ë””ì˜¤ {len(videos)}ê°œ")

        # ê¸°ì¡´ generated_videos í´ë” ë°±ì—… ê±´ë„ˆëœ€ (íŒŒì¼ ë®ì–´ì“°ê¸° í—ˆìš©)

        # ì¶œë ¥ í´ë” ìƒì„± (ë°±ì—… í›„ ìƒˆë¡œ ìƒì„±)
        output_folder = self.folder_path / "generated_videos"
        output_folder.mkdir(exist_ok=True)

        # 1ë‹¨ê³„: TTS ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)
        logger.info("=" * 70)
        logger.info("1ë‹¨ê³„: TTS ìŒì„± ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)")
        logger.info("=" * 70)

        tts_tasks = []
        scene_data_list = []

        # ë§ˆì§€ë§‰ìœ¼ë¡œ ì‚¬ìš©í•œ ë¯¸ë””ì–´ ì¶”ì  (ì˜ìƒë³‘í•© ë°©ì‹)
        last_media_path = None
        last_media_type = None

        for scene in scenes:
            # ì·¨ì†Œ í”Œë˜ê·¸ íŒŒì¼ ì²´í¬
            cancel_file = self.folder_path / '.cancel'
            if cancel_file.exists():
                logger.warning("ğŸ›‘ ì·¨ì†Œ í”Œë˜ê·¸ ê°ì§€ë¨. ì˜ìƒ ìƒì„±ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                raise KeyboardInterrupt("User cancelled the operation")

            # scene_numberê°€ ì—†ìœ¼ë©´ scene_idì—ì„œ ì¶”ì¶œ
            scene_num = scene.get("scene_number")
            if scene_num is None:
                scene_id = scene.get("scene_id", "")
                # scene_01_main, scene_02_main ë“±ì—ì„œ ë²ˆí˜¸ ì¶”ì¶œ
                import re
                match = re.search(r'scene_(\d+)', scene_id)
                if match:
                    scene_num = int(match.group(1))
                else:
                    logger.warning(f"ì”¬ ID '{scene_id}'ì—ì„œ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€.")
                    continue

            narration = scene.get("narration") or scene.get("content", "")

            # scene_numì´ 0ì´ë©´ ê±´ë„ˆëœ€ (ì¸íŠ¸ë¡œ/í­íƒ„ì”¬)
            if scene_num == 0:
                logger.info(f"ì”¬ {scene_num} (ì¸íŠ¸ë¡œ/í­íƒ„ì”¬): ê±´ë„ˆëœ€.")
                continue

            # ì´ë¯¸ì§€ ë˜ëŠ” ë¹„ë””ì˜¤ê°€ ìˆëŠ”ì§€ í™•ì¸
            has_image = scene_num in images
            has_video = scene_num in videos

            # ë¯¸ë””ì–´ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€ (ìˆœí™˜ í• ë‹¹ìœ¼ë¡œ ì¸í•´ ëŒ€ë¶€ë¶„ ìˆì„ ê²ƒ)
            if not has_image and not has_video:
                logger.warning(f"ì”¬ {scene_num}: ë¯¸ë””ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€.")
                continue

            # ë¯¸ë””ì–´ íƒ€ì… ê²°ì • (ë¹„ë””ì˜¤ ìš°ì„ )
            media_type = 'video' if has_video else 'image'
            media_path = videos[scene_num] if has_video else images[scene_num]
            logger.info(f"ì”¬ {scene_num}: {media_type.upper()} ì‚¬ìš© - {media_path.name}")

            # ë‚˜ë ˆì´ì…˜ í…ìŠ¤íŠ¸ ì €ì¥
            narration_txt_path = output_folder / f"scene_{scene_num:02d}_narration.txt"
            clean_narration = self._clean_narration(narration)
            with open(narration_txt_path, 'w', encoding='utf-8') as f:
                f.write(clean_narration)

            # TTS íƒœìŠ¤í¬ ìƒì„±
            audio_path = output_folder / f"scene_{scene_num:02d}_audio.mp3"
            tts_tasks.append(self._generate_tts(narration, audio_path))

            scene_data_list.append({
                'scene_num': scene_num,
                'media_path': media_path,
                'media_type': media_type,
                'audio_path': audio_path,
                'clean_narration': clean_narration
            })

        # TTS ë³‘ë ¬ ìƒì„± (8ê°œì”© ì œí•œ) - íƒ€ì„ìŠ¤íƒ¬í”„ë„ í•¨ê»˜ ë°›ìŒ!
        logger.info(f"âš¡ TTS ë³‘ë ¬ ìƒì„±: ìµœëŒ€ 8ê°œì”© ë™ì‹œ ì²˜ë¦¬ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)")

        tts_results = []
        batch_size = 8
        for i in range(0, len(tts_tasks), batch_size):
            batch = tts_tasks[i:i+batch_size]
            batch_results = await asyncio.gather(*batch)
            tts_results.extend(batch_results)
            logger.info(f"TTS ë°°ì¹˜ ì™„ë£Œ: {i+1}~{min(i+len(batch), len(tts_tasks))}/{len(tts_tasks)}")

        logger.info(f"TTS ìƒì„± ì™„ë£Œ: {len(tts_tasks)}ê°œ")

        # ì˜¤ë””ì˜¤ ê¸¸ì´ì™€ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ scene_dataì— ì €ì¥
        for i, scene_data in enumerate(scene_data_list):
            duration, word_timings = tts_results[i]
            scene_data['audio_duration'] = duration
            scene_data['word_timings'] = word_timings  # Edge TTS íƒ€ì„ìŠ¤íƒ¬í”„!

        # 2ë‹¨ê³„: ê±´ë„ˆëœ€ (Whisper ëŒ€ì‹  ëŒ€ë³¸ ì‚¬ìš©)
        # Whisper ìŒì„± ì¸ì‹ ì—†ì´ ëŒ€ë³¸ì„ ì§ì ‘ ì‚¬ìš©í•˜ë¯€ë¡œ í›¨ì”¬ ë¹ ë¦„!

        # 3ë‹¨ê³„: ë¹„ë””ì˜¤ ìƒì„± + ìë§‰ ì¶”ê°€ (ë³‘ë ¬ ì²˜ë¦¬)
        logger.info("=" * 70)
        logger.info("3ë‹¨ê³„: ë¹„ë””ì˜¤ ìƒì„± ë° ìë§‰ ì¶”ê°€")
        logger.info("=" * 70)

        # ì¸ì½”ë” ì •ë³´ í‘œì‹œ
        encoder_type = "GPU ê°€ì†" if self.video_codec != 'libx264' else "CPU"
        logger.info(f"ğŸ¬ ë¹„ë””ì˜¤ ì¸ì½”ë”: {self.video_codec} ({encoder_type})")
        logger.info(f"ğŸ“Š ì´ {len(scene_data_list)}ê°œ ì”¬ ì²˜ë¦¬ ì˜ˆì •")

        # ì‹œìŠ¤í…œì— ë¬´ë¦¬ ì•ˆ ê°€ë„ë¡ ì›Œì»¤ ìˆ˜ ì œí•œ (CPU ì½”ì–´ì˜ 75%, ìµœì†Œ 2, ìµœëŒ€ 3)
        cpu_count = multiprocessing.cpu_count()
        max_workers = max(2, min(3, (cpu_count * 3) // 4))
        logger.info(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: {max_workers}ê°œ ì›Œì»¤ (CPU ì½”ì–´: {cpu_count}ê°œ)")
        logger.info("=" * 70)

        scene_videos = []
        all_narrations = []

        # ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜
        def process_scene(idx, scene_data):
            scene_num = scene_data['scene_num']
            media_path = scene_data['media_path']
            media_type = scene_data['media_type']
            audio_path = scene_data['audio_path']
            clean_narration = scene_data['clean_narration']

            progress = f"[{idx}/{len(scene_data_list)}]"
            logger.info(f"\n{progress} ì”¬ {scene_num} ì²˜ë¦¬ ì¤‘... ({media_type.upper()})")

            # ë¹„ë””ì˜¤ ìƒì„± (ìë§‰ í¬í•¨)
            video_path = output_folder / f"scene_{scene_num:02d}.mp4"

            # ë¹„ë””ì˜¤ íŒŒì¼ì´ ì´ë¯¸ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ì˜¤ë””ì˜¤ì™€ ê²°í•©
            if media_type == 'video':
                logger.info(f"{progress} ì”¬ {scene_num}: ë¹„ë””ì˜¤ íŒŒì¼ì— ì˜¤ë””ì˜¤ ê²°í•© ì¤‘...")
                # ë¹„ë””ì˜¤ì— ì˜¤ë””ì˜¤ë¥¼ ê²°í•© (ìë§‰ì€ ì„ íƒì‚¬í•­)
                if self.add_subtitles:
                    audio_duration = scene_data.get('audio_duration', 1.0)
                    word_timings = scene_data.get('word_timings', [])
                    result = self._combine_video_audio_with_subtitles(
                        scene_num, media_path, audio_path, video_path,
                        clean_narration, audio_duration, word_timings
                    )
                else:
                    result = self._combine_video_audio(scene_num, media_path, audio_path, video_path)
            else:
                # ì´ë¯¸ì§€ì—ì„œ ë¹„ë””ì˜¤ ìƒì„±
                logger.info(f"{progress} ì”¬ {scene_num} ë¹„ë””ì˜¤ ìƒì„± ì¤‘... ({encoder_type})")
                if self.add_subtitles:
                    audio_duration = scene_data.get('audio_duration', 1.0)
                    word_timings = scene_data.get('word_timings', [])  # Edge TTS íƒ€ì„ìŠ¤íƒ¬í”„
                    result = self._create_scene_video_with_subtitles(
                        scene_num, media_path, audio_path, video_path,
                        clean_narration, audio_duration, word_timings
                    )
                else:
                    result = self._create_scene_video(scene_num, media_path, audio_path, video_path)

            if result:
                logger.info(f"{progress} âœ… ì”¬ {scene_num} ì™„ë£Œ!")
                return (scene_num, result, clean_narration)
            return None

        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(process_scene, idx, scene_data): idx
                      for idx, scene_data in enumerate(scene_data_list, 1)}

            for future in as_completed(futures):
                result = future.result()
                if result:
                    scene_num, video_path, narration = result
                    scene_videos.append((scene_num, video_path))
                    all_narrations.append(narration)

        # ì”¬ ë²ˆí˜¸ ìˆœì„œë¡œ ì •ë ¬
        scene_videos.sort(key=lambda x: x[0])
        scene_videos = [path for _, path in scene_videos]

        if not scene_videos:
            logger.error("ìƒì„±ëœ ì”¬ ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ì „ì²´ ë‚˜ë ˆì´ì…˜ ì €ì¥
        full_narration_path = output_folder / "full_narration.txt"
        with open(full_narration_path, 'w', encoding='utf-8') as f:
            f.write('\n\n'.join(all_narrations))
        logger.info(f"ì „ì²´ ë‚˜ë ˆì´ì…˜ ì €ì¥: {full_narration_path}")

        # ê²°í•©
        if combine and len(scene_videos) > 1:
            # titleì´ ìµœìƒìœ„ì— ìˆê±°ë‚˜ metadata ì•ˆì— ìˆì„ ìˆ˜ ìˆìŒ
            title = self.story_data.get("title")
            if not title and "metadata" in self.story_data:
                title = self.story_data["metadata"].get("title")
            if not title:
                title = "video"

            # íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ íŠ¹ìˆ˜ë¬¸ì ì œê±°
            safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '_', '-', '.')).strip()
            safe_title = safe_title.replace(' ', '_')
            # ìµœì¢… ì˜ìƒì„ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ì €ì¥ (ì˜ìƒë³‘í•©ê³¼ ê°™ì€ ìœ„ì¹˜)
            final_path = self.folder_path / f"{safe_title}.mp4"
            logger.info(f"ğŸ“ ìµœì¢… ì˜ìƒ ì œëª©: {title} â†’ {safe_title}.mp4")
            logger.info(f"ğŸ“‚ ìµœì¢… ì˜ìƒ ìœ„ì¹˜: {final_path}")
            return self._combine_videos(scene_videos, final_path, start_time)
        elif scene_videos:
            logger.info(f"ì”¬ ë¹„ë””ì˜¤ {len(scene_videos)}ê°œ ìƒì„± ì™„ë£Œ (ê²°í•© ì•ˆ í•¨)")
            return scene_videos[0]

        return None

    async def _generate_word_timestamps_async(self, audio_path: Path) -> list:
        """Whisperë¡œ ìŒì„± ë¶„ì„í•˜ì—¬ ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (async ë²„ì „)"""
        import concurrent.futures

        def _run_whisper(audio_path_str):
            try:
                import whisper

                logger.info(f"Whisper ë¶„ì„ ì¤‘: {Path(audio_path_str).name}")

                # Whisper ëª¨ë¸ ë¡œë“œ (base ëª¨ë¸: ë¹ ë¥´ê³  ì¶©ë¶„íˆ ì •í™•í•¨)
                model = whisper.load_model("base")

                # ìŒì„± ì¸ì‹ ì‹¤í–‰ (ì„¸ê·¸ë¨¼íŠ¸ ë‹¨ìœ„ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ)
                result = model.transcribe(
                    audio_path_str,
                    language="ko",
                    verbose=False,
                    fp16=False  # CPUì—ì„œ FP16 ê²½ê³  ë°©ì§€
                )

                # ì„¸ê·¸ë¨¼íŠ¸ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë³€í™˜
                word_segments = []
                for segment in result.get("segments", []):
                    # ì„¸ê·¸ë¨¼íŠ¸ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¡œ ë¶„ë¦¬
                    text = segment.get("text", "").strip()
                    words = text.split()

                    if not words:
                        continue

                    # ì„¸ê·¸ë¨¼íŠ¸ ì‹œê°„ì„ ë‹¨ì–´ ê°œìˆ˜ë¡œ ë‚˜ëˆ ì„œ ê·¼ì‚¬ì¹˜ ê³„ì‚°
                    start_time = segment.get("start", 0)
                    end_time = segment.get("end", 0)
                    duration = end_time - start_time
                    time_per_word = duration / len(words) if words else 0

                    for i, word in enumerate(words):
                        word_start = start_time + (i * time_per_word)
                        word_end = start_time + ((i + 1) * time_per_word)
                        word_segments.append({
                            "word": word.strip(),
                            "start": word_start,
                            "end": word_end
                        })

                logger.info(f"Whisper ì™„ë£Œ: {Path(audio_path_str).name} - {len(word_segments)}ê°œ ë‹¨ì–´")
                return word_segments

            except Exception as e:
                logger.error(f"Whisper ë¶„ì„ ì‹¤íŒ¨ ({Path(audio_path_str).name}): {e}")
                import traceback
                logger.error(traceback.format_exc())
                # ìë§‰ ìƒì„± ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ ì˜ìƒ ì œì‘ ì¤‘ë‹¨
                raise RuntimeError(f"ìë§‰ ìƒì„± ì‹¤íŒ¨: {Path(audio_path_str).name} - {e}")

        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            return await loop.run_in_executor(executor, _run_whisper, str(audio_path))

    def _generate_word_timestamps(self, audio_path: Path) -> list:
        """Whisperë¡œ ìŒì„± ë¶„ì„í•˜ì—¬ ì„¸ê·¸ë¨¼íŠ¸ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (ë™ê¸° ë²„ì „)"""
        try:
            import whisper

            logger.info(f"Whisperë¡œ ìŒì„± ë¶„ì„ ì¤‘: {audio_path.name}")

            # Whisper ëª¨ë¸ ë¡œë“œ (base ëª¨ë¸: ë¹ ë¥´ê³  ì¶©ë¶„íˆ ì •í™•í•¨)
            model = whisper.load_model("base")

            # ìŒì„± ì¸ì‹ ì‹¤í–‰
            result = model.transcribe(
                str(audio_path),
                language="ko",
                verbose=False,
                fp16=False  # CPUì—ì„œ FP16 ê²½ê³  ë°©ì§€
            )

            # ì„¸ê·¸ë¨¼íŠ¸ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ í›„ ë‹¨ì–´ë¡œ ë¶„í• 
            word_segments = []

            if not result or "segments" not in result:
                logger.warning("Whisperê°€ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ")
                return []

            for segment in result.get("segments", []):
                text = segment.get("text", "").strip()
                start_time = segment.get("start", 0.0)
                end_time = segment.get("end", 0.0)

                if not text:
                    continue

                # ì„¸ê·¸ë¨¼íŠ¸ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¡œ ë¶„í• 
                words = text.split()
                if not words:
                    continue

                # ê° ë‹¨ì–´ì— ê· ë“±í•˜ê²Œ ì‹œê°„ ë¶„ë°°
                duration_per_word = (end_time - start_time) / len(words)

                for i, word in enumerate(words):
                    word_start = start_time + (i * duration_per_word)
                    word_end = word_start + duration_per_word

                    word_segments.append({
                        "word": word.strip(),
                        "start": word_start,
                        "end": word_end
                    })

            logger.info(f"ë‹¨ì–´ {len(word_segments)}ê°œì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ ì™„ë£Œ")
            return word_segments

        except Exception as e:
            logger.error(f"Whisper ë¶„ì„ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            # ìë§‰ ìƒì„± ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ ì˜ìƒ ì œì‘ ì¤‘ë‹¨
            raise RuntimeError(f"ìë§‰ ìƒì„± ì‹¤íŒ¨ (Fallback): {audio_path.name} - {e}")

    def _create_srt_with_timings(self, word_timings: list, srt_path: Path, narration: str, audio_duration: float, max_chars_per_line: int = 22):
        """Edge TTS íƒ€ì„ìŠ¤íƒ¬í”„ ë˜ëŠ” ëŒ€ë³¸ ê¸°ë°˜ ASS ìë§‰ ìƒì„±"""
        try:
            # Edge TTSì—ì„œ ë°›ì€ ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš©
            word_segments = word_timings

            if not word_segments:
                logger.warning("íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ë¹„ì–´ìˆìŒ â†’ ëŒ€ë³¸ ê¸°ë°˜ ìë§‰ìœ¼ë¡œ í´ë°±")
                return self._create_srt_from_script(narration, audio_duration, srt_path, max_chars_per_line)

            logger.info(f"Edge TTS íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ìë§‰ ìƒì„± ì¤‘... ({len(word_segments)}ê°œ ë‹¨ì–´)")

            # ë‹¨ì–´ë“¤ì„ max_chars_per_lineì— ë§ì¶° ê·¸ë£¹í™”
            subtitles = []
            current_text = ""
            current_start = None
            current_end = None

            MIN_REMAINING_CHARS = 5

            for i, word_info in enumerate(word_segments):
                word = word_info["word"]
                start = word_info["start"]
                end = word_info["end"]

                # ë¹ˆ ë‹¨ì–´ëŠ” ê±´ë„ˆë›°ê¸°
                if not word.strip():
                    continue

                # ì²« ë‹¨ì–´ë©´ ì‹œì‘ ì‹œê°„ ì„¤ì •
                if current_start is None:
                    current_start = start

                # ë‹¤ìŒ í…ìŠ¤íŠ¸ ê³„ì‚°
                next_text = current_text + (" " if current_text else "") + word

                # ë‚¨ì€ ë‹¨ì–´ë“¤ ê³„ì‚°
                remaining_words = word_segments[i+1:]
                remaining_text = " ".join([w["word"] for w in remaining_words]) if remaining_words else ""

                # ìŠ¤ë§ˆíŠ¸ ì¤„ë°”ê¿ˆ ë¡œì§
                if len(next_text) > max_chars_per_line and current_text:
                    # ë‚¨ì€ ê¸€ìê°€ ë„ˆë¬´ ì ìœ¼ë©´ í˜„ì¬ ë¼ì¸ì— í¬í•¨
                    if len(remaining_text) > 0 and len(remaining_text) < MIN_REMAINING_CHARS:
                        current_text = next_text + (" " + remaining_text if remaining_text else "")
                        # ë‚¨ì€ ëª¨ë“  ë‹¨ì–´ì˜ ë ì‹œê°„ ì°¾ê¸°
                        if remaining_words:
                            current_end = remaining_words[-1]["end"]
                        else:
                            current_end = end

                        subtitles.append({
                            "start": current_start,
                            "end": current_end,
                            "text": current_text.strip()
                        })
                        break  # ëª¨ë“  ë‹¨ì–´ ì²˜ë¦¬ ì™„ë£Œ
                    else:
                        # ì •ìƒì ìœ¼ë¡œ ì¤„ë°”ê¿ˆ - í˜„ì¬ ë¼ì¸ì˜ ë ì‹œê°„ì€ current_end (ì´ì „ ë‹¨ì–´ì˜ ë)
                        subtitles.append({
                            "start": current_start,
                            "end": current_end,  # ìˆ˜ì •: end ëŒ€ì‹  current_end ì‚¬ìš©
                            "text": current_text.strip()
                        })
                        current_text = word
                        current_start = start
                        current_end = end
                else:
                    current_text = next_text
                    current_end = end

            # ë‚¨ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            if current_text:
                subtitles.append({
                    "start": current_start,
                    "end": current_end,
                    "text": current_text.strip()
                })

            # ìë§‰ì´ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ì‹œê°„ ì¡°ì • (ì˜ìƒë³‘í•© ë°©ì‹)
            for i in range(len(subtitles) - 1):
                current_sub = subtitles[i]
                next_sub = subtitles[i + 1]

                # í˜„ì¬ ìë§‰ì´ ë‹¤ìŒ ìë§‰ê³¼ ê²¹ì¹˜ê±°ë‚˜ ê°„ê²©ì´ ë„ˆë¬´ ì¢ìœ¼ë©´ ì¡°ì •
                if current_sub["end"] >= next_sub["start"]:
                    # ë‹¤ìŒ ìë§‰ ì‹œì‘ ì§ì „ìœ¼ë¡œ ì¡°ì • (0.05ì´ˆ ê°„ê²©)
                    gap = 0.05
                    adjusted_end = next_sub["start"] - gap
                    # ìµœì†Œ í‘œì‹œ ì‹œê°„ ë³´ì¥ (0.3ì´ˆ)
                    min_duration = 0.3
                    if adjusted_end - current_sub["start"] < min_duration:
                        # í˜„ì¬ ìë§‰ì´ ë„ˆë¬´ ì§§ì•„ì§€ë©´ ë‹¤ìŒ ìë§‰ ì‹œì‘ì„ ë’¤ë¡œ ë°€ê¸°
                        current_sub["end"] = current_sub["start"] + min_duration
                        next_sub["start"] = current_sub["end"] + gap
                    else:
                        current_sub["end"] = adjusted_end

            # ë§ˆì§€ë§‰ ìë§‰ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì¡°ì •
            if subtitles and subtitles[-1]["end"] > audio_duration:
                subtitles[-1]["end"] = audio_duration

            # ASS íŒŒì¼ ì‘ì„±
            ass_path = srt_path.with_suffix('.ass')

            with open(ass_path, 'w', encoding='utf-8') as f:
                # ASS í—¤ë”
                f.write("[Script Info]\n")
                f.write("ScriptType: v4.00+\n")
                f.write("PlayResX: 1920\n")
                f.write("PlayResY: 1080\n\n")

                # ìŠ¤íƒ€ì¼ ì •ì˜ - NanumGothic 96pt, ë§¨ í•˜ë‹¨
                f.write("[V4+ Styles]\n")
                f.write("Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding\n")
                f.write("Style: Default,NanumGothic,96,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,3,2,2,10,10,20,1\n\n")

                # ì´ë²¤íŠ¸ (ìë§‰) - audio_durationì„ ì´ˆê³¼í•˜ëŠ” ìë§‰ í•„í„°ë§
                f.write("[Events]\n")
                f.write("Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n")

                filtered_subtitles = []
                for sub in subtitles:
                    # audio_durationì„ ì´ˆê³¼í•˜ëŠ” ìë§‰ì€ ì œì™¸
                    if sub["start"] < audio_duration:
                        # ë ì‹œê°„ì´ durationì„ ì´ˆê³¼í•˜ë©´ durationìœ¼ë¡œ ì˜ë¼ëƒ„
                        end_time_adjusted = min(sub["end"], audio_duration)
                        filtered_subtitles.append({
                            "start": sub["start"],
                            "end": end_time_adjusted,
                            "text": sub["text"]
                        })

                for sub in filtered_subtitles:
                    start_time = self._format_ass_timestamp(sub["start"])
                    end_time = self._format_ass_timestamp(sub["end"])
                    text = sub["text"].replace('\n', '\\N')
                    f.write(f"Dialogue: 0,{start_time},{end_time},Default,,0,0,0,,{text}\n")

            logger.info(f"Edge TTS íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ASS ìë§‰ ì™„ë£Œ: {len(filtered_subtitles)}ê°œ ë¼ì¸ (duration: {audio_duration:.2f}ì´ˆ)")
            return ass_path

        except Exception as e:
            logger.error(f"ìë§‰ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise RuntimeError(f"ìë§‰ ìƒì„± ì‹¤íŒ¨: {e}")

    def _create_srt_from_script(self, narration: str, audio_duration: float, srt_path: Path, max_chars_per_line: int = 22):
        """ëŒ€ë³¸ì„ ê¸°ë°˜ìœ¼ë¡œ SRT ìë§‰ ìƒì„± (Whisper ì—†ì´)"""
        if not narration or not narration.strip():
            raise RuntimeError("ìë§‰ ìƒì„± ì‹¤íŒ¨: ëŒ€ë³¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        # ëŒ€ë³¸ì„ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬ (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ê¸°ì¤€)
        import re
        sentences = re.split(r'([.!?ã€‚ï¼ï¼Ÿ])', narration)

        # ë¶„ë¦¬ëœ êµ¬ë‘ì ì„ ì• ë¬¸ì¥ì— ë¶™ì´ê¸°
        combined_sentences = []
        for i in range(0, len(sentences)-1, 2):
            if i+1 < len(sentences):
                combined_sentences.append((sentences[i] + sentences[i+1]).strip())

        # ë§ˆì§€ë§‰ ë¬¸ì¥ ì²˜ë¦¬
        if len(sentences) % 2 == 1 and sentences[-1].strip():
            combined_sentences.append(sentences[-1].strip())

        # ë¬¸ì¥ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ
        if not combined_sentences:
            combined_sentences = [narration.strip()]

        # ì „ì²´ ê¸€ì ìˆ˜ ê³„ì‚° (ê³µë°± í¬í•¨)
        total_text = " ".join(combined_sentences)
        total_chars = len(total_text)
        time_per_char = audio_duration / total_chars if total_chars > 0 else 0

        # ê° ë¬¸ì¥ì„ 22ì ë‹¨ìœ„ë¡œ ë¶„í•  (ê¸€ì ìˆ˜ ê¸°ë°˜ íƒ€ì´ë°)
        subtitles = []
        current_time = 0.0

        MIN_REMAINING_CHARS = 5  # ë‚¨ì€ ê¸€ìê°€ ì´ë³´ë‹¤ ì ìœ¼ë©´ í˜„ì¬ ë¼ì¸ì— í¬í•¨

        for sentence in combined_sentences:
            # ë¬¸ì¥ì„ ë‹¨ì–´ë¡œ ë¶„ë¦¬
            words = sentence.split()
            if not words:
                continue

            current_text = ""
            for i, word in enumerate(words):
                next_text = current_text + (" " if current_text else "") + word

                # ë‹¤ìŒ ë‹¨ì–´ê¹Œì§€ í¬í•¨í•˜ë©´ ì–¼ë§ˆë‚˜ ë‚¨ëŠ”ì§€ ê³„ì‚°
                remaining_words = words[i+1:]
                remaining_text = " ".join(remaining_words) if remaining_words else ""

                # ìŠ¤ë§ˆíŠ¸ ì¤„ë°”ê¿ˆ ë¡œì§
                if len(next_text) > max_chars_per_line and current_text:
                    # ë‚¨ì€ ê¸€ìê°€ ë„ˆë¬´ ì ìœ¼ë©´ (1-2ê¸€ì) í˜„ì¬ ë¼ì¸ì— í¬í•¨
                    if len(remaining_text) > 0 and len(remaining_text) < MIN_REMAINING_CHARS:
                        # í˜„ì¬ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ê³  ì¤„ë°”ê¿ˆ (ë‹¤ìŒ ë‹¨ì–´ë“¤ë„ í•¨ê»˜)
                        current_text = next_text + (" " + remaining_text if remaining_text else "")
                        duration = len(current_text) * time_per_char
                        end_time = current_time + duration
                        subtitles.append({
                            "start": current_time,
                            "end": end_time,
                            "text": current_text.strip()
                        })
                        current_text = ""
                        current_time = end_time
                        break  # ì´ ë¬¸ì¥ ë
                    else:
                        # ì •ìƒì ìœ¼ë¡œ ì¤„ë°”ê¿ˆ
                        duration = len(current_text) * time_per_char
                        end_time = current_time + duration
                        subtitles.append({
                            "start": current_time,
                            "end": end_time,
                            "text": current_text.strip()
                        })
                        current_text = word
                        current_time = end_time
                else:
                    current_text = next_text

            # ë‚¨ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            if current_text:
                duration = len(current_text) * time_per_char
                end_time = current_time + duration
                subtitles.append({
                    "start": current_time,
                    "end": end_time,
                    "text": current_text.strip()
                })
                current_time = end_time

        # ASS íŒŒì¼ ì‘ì„± (ìŠ¤íƒ€ì¼ í¬í•¨)
        ass_path = srt_path.with_suffix('.ass')

        with open(ass_path, 'w', encoding='utf-8') as f:
            # ASS í—¤ë”
            f.write("[Script Info]\n")
            f.write("ScriptType: v4.00+\n")
            f.write("PlayResX: 1920\n")
            f.write("PlayResY: 1080\n\n")

            # ìŠ¤íƒ€ì¼ ì •ì˜
            f.write("[V4+ Styles]\n")
            f.write("Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding\n")
            f.write("Style: Default,NanumGothic,96,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,3,2,2,10,10,20,1\n\n")

            # ì´ë²¤íŠ¸ (ìë§‰)
            f.write("[Events]\n")
            f.write("Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n")

            for sub in subtitles:
                start = self._format_ass_timestamp(sub["start"])
                end = self._format_ass_timestamp(sub["end"])
                text = sub['text'].replace('\n', '\\N')
                f.write(f"Dialogue: 0,{start},{end},Default,,0,0,0,,{text}\n")

        logger.info(f"ëŒ€ë³¸ ê¸°ë°˜ ASS ìƒì„± ì™„ë£Œ: {len(subtitles)}ê°œ êµ¬ê°„")

        # SRT ê²½ë¡œë¥¼ ASS ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸ (í˜¸ì¶œìê°€ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡)
        if srt_path != ass_path:
            # srt_path ë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ass_path ë°˜í™˜
            pass

        return ass_path

    def _create_srt_from_timestamps(self, word_segments: list, srt_path: Path, max_chars_per_line: int = 22):
        """ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ SRT ìë§‰ íŒŒì¼ ìƒì„± (ì ì ˆí•œ ê¸¸ì´ë¡œ ê·¸ë£¹í™”)"""
        if not word_segments:
            raise RuntimeError("ìë§‰ ìƒì„± ì‹¤íŒ¨: ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ìŠµë‹ˆë‹¤.")

        subtitles = []
        current_text = ""
        start_time = None
        min_remaining_chars = 5  # ë‚¨ì€ ê¸€ìê°€ ì´ë³´ë‹¤ ì ìœ¼ë©´ í˜„ì¬ ë¼ì¸ì— í¬í•¨

        for i, segment in enumerate(word_segments):
            word = segment["word"]

            # ì²« ë‹¨ì–´ë©´ ì‹œì‘ ì‹œê°„ ê¸°ë¡
            if not current_text:
                start_time = segment["start"]

            # ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì¶”ê°€í–ˆì„ ë•Œì˜ ê¸¸ì´ í™•ì¸ (ë„ì–´ì“°ê¸° í¬í•¨)
            next_text = current_text + (" " if current_text else "") + word
            is_last_word = (i == len(word_segments) - 1)

            # ë§ˆì§€ë§‰ ë‹¨ì–´ì´ê±°ë‚˜, ë‹¤ìŒ ë‹¨ì–´ê¹Œì§€ ì¶”ê°€í•´ë„ ì ì ˆí•œ ê¸¸ì´ë©´ ì¶”ê°€
            if is_last_word:
                current_text = next_text
                subtitles.append({
                    "start": start_time,
                    "end": segment["end"],
                    "text": current_text.strip()
                })
            elif len(next_text) >= max_chars_per_line:
                # í˜„ì¬ê¹Œì§€ì˜ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ìë§‰ ìƒì„±
                if current_text:
                    subtitles.append({
                        "start": start_time,
                        "end": word_segments[i-1]["end"] if i > 0 else segment["end"],
                        "text": current_text.strip()
                    })
                    current_text = word
                    start_time = segment["start"]
                else:
                    # ë‹¨ì–´ í•˜ë‚˜ê°€ max_chars_per_lineë³´ë‹¤ ê¸´ ê²½ìš°
                    current_text = next_text
            else:
                # ë‹¤ìŒ ë‹¨ì–´ ì¶”ê°€
                current_text = next_text

                # ë‹¤ìŒë‹¤ìŒ ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ í™•ì¸ (ë‚¨ì€ ê¸€ì ìˆ˜ ì˜ˆì¸¡)
                if i + 1 < len(word_segments):
                    peek_text = current_text + " " + word_segments[i + 1]["word"]
                    # ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì¶”ê°€í•˜ë©´ maxë¥¼ ë„˜ê³ , ë‚¨ì€ ê¸€ìê°€ ë„ˆë¬´ ì ì„ ê²ƒ ê°™ìœ¼ë©´ ì§€ê¸ˆ ëŠê¸°
                    if len(peek_text) > max_chars_per_line and len(peek_text) - max_chars_per_line < min_remaining_chars:
                        # ì§€ê¸ˆ ìë§‰ ìƒì„±
                        subtitles.append({
                            "start": start_time,
                            "end": segment["end"],
                            "text": current_text.strip()
                        })
                        current_text = ""

        # SRT íŒŒì¼ ì‘ì„±
        with open(srt_path, 'w', encoding='utf-8') as f:
            for i, sub in enumerate(subtitles, 1):
                start = self._format_timestamp(sub["start"])
                end = self._format_timestamp(sub["end"])
                f.write(f"{i}\n")
                f.write(f"{start} --> {end}\n")
                f.write(f"{sub['text']}\n\n")

        logger.info(f"SRT ìë§‰ ìƒì„± ì™„ë£Œ: {len(subtitles)}ê°œ êµ¬ê°„")
        return True

    def _format_timestamp(self, seconds: float) -> str:
        """ì´ˆë¥¼ SRT íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (00:00:00,000)"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

    def _format_ass_timestamp(self, seconds: float) -> str:
        """ì´ˆë¥¼ ASS íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (0:00:00.00)"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        centisecs = int((seconds % 1) * 100)
        return f"{hours}:{minutes:02d}:{secs:02d}.{centisecs:02d}"

    def _add_subtitles_with_segments(self, video_path: Path, audio_path: Path, output_path: Path, word_segments: list):
        """ë¯¸ë¦¬ ë¶„ì„ëœ Whisper íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ìë§‰ ì¶”ê°€ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        import subprocess

        # ìë§‰ ìŠ¤íƒ€ì¼ (ë‚˜ëˆ”ê³ ë”•, í° ê¸€ì”¨) - _add_subtitles_from_scriptì™€ ë™ì¼í•˜ê²Œ
        subtitle_style = (
            "FontName=NanumGothic,"  # ë‚˜ëˆ”ê³ ë”•
            "Fontsize=32,"  # 20 -> 32 (ë” í¼)
            "Bold=1,"  # ë³¼ë“œ
            "PrimaryColour=&H00FFFFFF,"  # í°ìƒ‰
            "OutlineColour=&H00000000,"  # ê²€ì€ í…Œë‘ë¦¬
            "BorderStyle=1,"
            "Outline=3,"  # ë” ë‘êº¼ìš´ í…Œë‘ë¦¬
            "Shadow=2,"  # ë” ì§„í•œ ê·¸ë¦¼ì
            "MarginV=20,"  # ë§¨ í•˜ë‹¨
            "Alignment=2"  # í•˜ë‹¨ ì¤‘ì•™
        )

        # SRT íŒŒì¼ ìƒì„±
        srt_path = audio_path.with_suffix('.srt')

        # ë¯¸ë¦¬ ë¶„ì„ëœ Whisper íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì •í™•í•œ ìë§‰ ìƒì„±
        # word_segmentsê°€ ì—†ìœ¼ë©´ _create_srt_from_timestampsì—ì„œ ì˜ˆì™¸ ë°œìƒ
        self._create_srt_from_timestamps(word_segments, srt_path, max_chars_per_line=22)

        # FFmpeg ëª…ë ¹ì–´
        cmd = [
            'ffmpeg', '-i', str(video_path),
            '-vf', f"subtitles={str(srt_path)}:force_style='{subtitle_style}'",
            '-c:a', 'copy',
            '-y',
            str(output_path)
        ]

        subprocess.run(cmd, check=True, capture_output=True)

    def _add_subtitles_from_script(self, video_path: Path, audio_path: Path, output_path: Path, narration: str, audio_duration: float):
        """ëŒ€ë³¸ì„ ê¸°ë°˜ìœ¼ë¡œ ìë§‰ ì¶”ê°€ (Whisper ì—†ì´)"""
        import subprocess

        # SRT íŒŒì¼ ê²½ë¡œ (ì‹¤ì œë¡œëŠ” ASS íŒŒì¼ì´ ìƒì„±ë¨)
        srt_path = audio_path.with_suffix('.srt')

        # ëŒ€ë³¸ ê¸°ë°˜ ASS ìë§‰ ìƒì„± (ìŠ¤íƒ€ì¼ í¬í•¨)
        ass_path = self._create_srt_from_script(narration, audio_duration, srt_path, max_chars_per_line=22)

        # FFmpeg ass í•„í„°ì—ëŠ” íŒŒì¼ëª…ë§Œ ì „ë‹¬
        ass_filename = ass_path.name

        # FFmpeg ëª…ë ¹ì–´ (ASS íŒŒì¼ì€ ì´ë¯¸ ìŠ¤íƒ€ì¼ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ force_style ë¶ˆí•„ìš”)
        cmd = [
            'ffmpeg', '-i', str(video_path),
            '-vf', f"ass={ass_filename}",
            '-c:a', 'copy',
            '-y',
            str(output_path)
        ]

        try:
            result = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8', errors='ignore', cwd=str(audio_path.parent))
        except subprocess.CalledProcessError as e:
            logger.error(f"FFmpeg ìë§‰ ì¶”ê°€ ì‹¤íŒ¨: {e.stderr}")
            raise

    def _add_subtitles(self, video_path: Path, audio_path: Path, output_path: Path):
        """Whisperë¡œ ìŒì„± ë¶„ì„ í›„ ì •í™•í•œ íƒ€ì´ë°ì˜ ìë§‰ ì¶”ê°€ (ë™ê¸° ë²„ì „ - í˜¸í™˜ì„±ìš©)"""
        # Whisperë¡œ ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
        word_segments = self._generate_word_timestamps(audio_path)
        # ì¶”ì¶œëœ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ìë§‰ ì¶”ê°€
        self._add_subtitles_with_segments(video_path, audio_path, output_path, word_segments)


def main():
    # Register signal handlers for graceful shutdown
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)

    parser = argparse.ArgumentParser(description="story.jsonê³¼ ì´ë¯¸ì§€ë¡œ ì˜ìƒ ìƒì„±")
    parser.add_argument("--folder", "-f", required=True, help="story.jsonê³¼ ì´ë¯¸ì§€ê°€ ìˆëŠ” í´ë” ê²½ë¡œ")
    parser.add_argument("--voice", "-v", default="ko-KR-SoonBokNeural",
                       help="TTS ìŒì„± (ê¸°ë³¸: ko-KR-SoonBokNeural)")
    parser.add_argument("--aspect-ratio", "-a", default="9:16", choices=["9:16", "16:9"],
                       help="ë¹„ë””ì˜¤ ë¹„ìœ¨ (ê¸°ë³¸: 9:16)")
    parser.add_argument("--add-subtitles", "-s", action="store_true", default=True,
                       help="ìë§‰ ì¶”ê°€ (ê¸°ë³¸: ì¶”ê°€í•¨, --no-subtitlesë¡œ ë„ê¸°)")
    parser.add_argument("--no-subtitles", action="store_false", dest="add_subtitles",
                       help="ìë§‰ ì¶”ê°€ ì•ˆ í•¨")
    parser.add_argument("--image-source", "-i", default="none", choices=["none", "dalle", "imagen3"],
                       help="ì´ë¯¸ì§€ ì†ŒìŠ¤ (ê¸°ë³¸: none - ìˆ˜ë™ ì—…ë¡œë“œ, dalle - DALL-E 3, imagen3 - Google Imagen 3)")
    parser.add_argument("--image-provider", default="openai", choices=["openai", "imagen3"],
                       help="ì´ë¯¸ì§€ ìƒì„± ì œê³µì (ê¸°ë³¸: openai - DALL-E 3, imagen3 - Google Imagen 3)")
    parser.add_argument("--is-admin", action="store_true",
                       help="ê´€ë¦¬ì ëª¨ë“œ (ë¹„ìš© ë¡œê·¸ í‘œì‹œ)")
    parser.add_argument("--job-id", default=None,
                       help="Job ID (ì¶”ì ìš©)")

    args = parser.parse_args()

    # DB ë¡œê¹… ì„¤ì • (JOB_IDê°€ ìˆìœ¼ë©´)
    job_id = args.job_id or os.environ.get('JOB_ID')
    if job_id:
        try:
            from src.utils import auto_setup_db_logging
            global logger
            logger = auto_setup_db_logging()
            logger.info(f"DB ë¡œê¹… í™œì„±í™”ë¨ - Job ID: {job_id}")
        except Exception as e:
            logger.warning(f"DB ë¡œê¹… ì„¤ì • ì‹¤íŒ¨: {e}")

    # ë¡œê·¸ í´ë” ìƒì„±
    os.makedirs("logs", exist_ok=True)

    print("=" * 70)
    print("VideoFromFolder Creator")
    print("=" * 70)
    if args.job_id:
        print(f"ğŸ†” Job ID: {args.job_id}")
    print(f"í´ë”: {args.folder}")
    print(f"ìŒì„±: {args.voice}")
    print(f"ë¹„ìœ¨: {args.aspect_ratio}")
    print(f"ìë§‰: {'ì¶”ê°€' if args.add_subtitles else 'ì¶”ê°€ ì•ˆ í•¨'}")
    print(f"ì´ë¯¸ì§€ ì†ŒìŠ¤: {args.image_source}")
    print("=" * 70)

    # í¬ë¦¬ì—ì´í„° ìƒì„±
    creator = VideoFromFolderCreator(
        folder_path=args.folder,
        voice=args.voice,
        aspect_ratio=args.aspect_ratio,
        add_subtitles=args.add_subtitles,
        image_source=args.image_source,
        image_provider=args.image_provider,
        is_admin=args.is_admin
    )

    # ë¹„ë””ì˜¤ ìƒì„± (í•­ìƒ ë³‘í•©)
    result = asyncio.run(creator.create_all_videos(combine=True))

    if result:
        print("=" * 70)
        print("âœ“ ì„±ê³µ!")
        print("=" * 70)
        if args.job_id:
            print(f"ğŸ†” Job ID: {args.job_id}")
        print(f"ì¶œë ¥: {result}")
        print("=" * 70)
    else:
        print("âœ— ì‹¤íŒ¨!")
        if args.job_id:
            print(f"ğŸ†” Job ID: {args.job_id}")
        sys.exit(1)


if __name__ == "__main__":
    main()
