"""AI narration generation."""

import logging
import os
from pathlib import Path
from typing import Dict, Any
from openai import OpenAI
from moviepy.editor import VideoFileClip, CompositeAudioClip, AudioFileClip


class Narrator:
    """Generate AI narration for videos."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger("AutoShortsEditor.Narrator")
        self.client = None

        # Initialize OpenAI client if API key exists
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            self.client = OpenAI(api_key=api_key)

    def _load_prompt_template(self, filename: str) -> str:
        """Load prompt template from file."""
        prompt_path = Path(__file__).parent.parent / "prompts" / filename

        if not prompt_path.exists():
            self.logger.warning(f"Prompt file not found: {prompt_path}, using default")
            return None

        try:
            with open(prompt_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            self.logger.error(f"Failed to load prompt template: {e}")
            return None

    def generate_narration_script(
        self,
        video_path: Path,
        transcription: str = None,
        use_gpt: bool = False
    ) -> str:
        """
        Generate Korean narration script (FREE or with GPT).

        ì£¼ì˜: ì˜ìƒ í™”ë©´ ë¶„ì„ì„ ë©”ì¸ìœ¼ë¡œ, ì˜¤ë””ì˜¤ ì¸ì‹ì€ ì°¸ê³ ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        (Whisper ì˜¤ì¸ì‹ ë¬¸ì œë¡œ ì¸í•´ í™”ë©´ ë¶„ì„ì„ ìš°ì„ í•©ë‹ˆë‹¤)

        Args:
            video_path: Path to video file
            transcription: Transcribed audio content from video
            use_gpt: Use GPT for script generation (costs money!)

        Returns:
            Korean narration script text
        """
        self.logger.info("Generating Korean narration script (Vision-first approach)")
        print(f"\nâœ ì˜ìƒ ë‚´ìš© ë¶„ì„ ë° ë‚˜ë ˆì´ì…˜ ìƒì„±...")
        print(f"   ğŸ“Œ ì „ëµ: ì˜ìƒ í™”ë©´ ë©”ì¸ + ìŒì„± ì°¸ê³  (Whisper ì˜¤ì¸ì‹ ë°©ì§€)")

        # Get video info
        clip = VideoFileClip(str(video_path))
        duration = clip.duration
        clip.close()

        # === STEP 1: VISION ANALYSIS (PRIMARY) ===
        print(f"\n    [1ë‹¨ê³„] ì˜ìƒ í™”ë©´ ë¶„ì„ ì¤‘ (ë©”ì¸ ì†ŒìŠ¤)...")
        vision_description = None
        try:
            vision_description = self._analyze_video_with_vision(video_path)
            if vision_description:
                print(f"   [OK] ì˜ìƒ í™”ë©´ ë¶„ì„ ì™„ë£Œ")
                print(f"   í™”ë©´ ë‚´ìš©: {vision_description[:150]}...")
                self.logger.info(f"Vision (primary): {vision_description[:200]}")
        except Exception as e:
            self.logger.error(f"Vision analysis failed: {e}")
            print(f"   âš  ì˜ìƒ í™”ë©´ ë¶„ì„ ì‹¤íŒ¨")

        # === STEP 2: AUDIO TRANSCRIPTION (REFERENCE ONLY) ===
        print(f"\n    [2ë‹¨ê³„] ìŒì„± ì¸ì‹ ì¤‘ (ì°¸ê³ ìš©)...")
        audio_transcription = None

        if not transcription:
            try:
                from .transcriber import Transcriber
                transcriber = Transcriber(self.config)
                segments = transcriber.transcribe_video(video_path)
                audio_transcription = " ".join([seg["text"] for seg in segments])

                if audio_transcription and audio_transcription.strip():
                    print(f"   [OK] ìŒì„± ì¸ì‹ ì™„ë£Œ (ì°¸ê³ ìš©)")
                    print(f"   ì¸ì‹ ë‚´ìš©: '{audio_transcription[:100]}...'")
                    self.logger.info(f"Audio (reference): {audio_transcription[:200]}")

                    # Filter out CTA phrases from audio
                    cliche_phrases = ["êµ¬ë…", "ì¢‹ì•„ìš”", "ì•Œë¦¼", "ëˆŒëŸ¬ì£¼ì„¸ìš”", "ë¶€íƒ", "ì±„ë„"]
                    if any(phrase in audio_transcription for phrase in cliche_phrases):
                        print(f"   âš  CTA ë©˜íŠ¸ ê°ì§€ â†’ ìŒì„± ë¬´ì‹œ")
                        audio_transcription = None
                else:
                    print(f"   âš  ìŒì„± ë‚´ìš© ì—†ìŒ")
                    audio_transcription = None
            except Exception as e:
                self.logger.warning(f"Audio transcription failed (ì°¸ê³ ìš©ì´ë¯€ë¡œ ë¬´ì‹œ): {e}")
                print(f"   âš  ìŒì„± ì¸ì‹ ì‹¤íŒ¨ (ë¬´ì‹œ)")
                audio_transcription = None
        else:
            audio_transcription = transcription

        # === STEP 3: VALIDATE ===
        if not vision_description and not audio_transcription:
            raise ValueError("ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: í™”ë©´ ë¶„ì„ë„ ìŒì„± ì¸ì‹ë„ ëª¨ë‘ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        # === STEP 4: GENERATE SCRIPT ===
        print(f"\n   âœ [3ë‹¨ê³„] ë‚˜ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘...")

        # Use GPT if API key available (BEST QUALITY!)
        if self.client:
            try:
                return self._generate_script_with_gpt(
                    vision_description=vision_description,
                    audio_transcription=audio_transcription,
                    duration=duration
                )
            except Exception as e:
                self.logger.error(f"GPT failed: {e}")
                print(f"   âš  GPT ì‹¤íŒ¨, í…œí”Œë¦¿ìœ¼ë¡œ ì „í™˜...")

        # Fallback to template (vision-first)
        primary_source = vision_description if vision_description else audio_transcription
        return self._generate_script_template_based(
            primary_source,
            duration,
            source="vision" if vision_description else "audio"
        )

    def _analyze_video_with_vision(self, video_path: Path) -> str:
        """
        Analyze video frames using FREE vision AI (BLIP).

        Args:
            video_path: Path to video file

        Returns:
            Description of video content
        """
        try:
            from transformers import BlipProcessor, BlipForConditionalGeneration
            from PIL import Image
            import numpy as np

            self.logger.info("Analyzing video with BLIP vision model (FREE)")

            # Load BLIP model (small, fast)
            processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
            model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

            # Extract key frames (more frames for better analysis)
            clip = VideoFileClip(str(video_path))
            duration = clip.duration

            # Sample more frames based on video length (every 2-3 seconds)
            num_frames = max(10, min(20, int(duration / 2)))  # 10-20 frames
            sample_times = [duration * (i / (num_frames - 1)) for i in range(num_frames)]

            print(f"   Analyzing {num_frames} frames...")

            descriptions = []

            for i, t in enumerate(sample_times):
                if t >= duration:
                    continue

                # Get frame
                frame = clip.get_frame(t)

                # Convert to PIL Image
                image = Image.fromarray(frame)

                # Generate caption
                inputs = processor(image, return_tensors="pt")
                outputs = model.generate(**inputs, max_new_tokens=50)
                caption = processor.decode(outputs[0], skip_special_tokens=True)

                descriptions.append(caption)
                self.logger.debug(f"Frame {i+1}/{num_frames} ({t:.1f}s): {caption}")

                # Show progress
                if (i + 1) % 5 == 0:
                    print(f"   Progress: {i+1}/{num_frames} frames analyzed...")

            clip.close()

            # Combine descriptions and translate to Korean
            if descriptions:
                # Analyze common themes
                all_text = " ".join(descriptions).lower()

                # Simple keyword-based Korean translation
                korean_elements = []

                # === FOOD & RESTAURANT ===
                if "food" in all_text or "dish" in all_text or "meal" in all_text:
                    korean_elements.append("ìŒì‹ì´ ë‹´ê¸´")
                if "plate" in all_text or "bowl" in all_text:
                    korean_elements.append("ê·¸ë¦‡ì— ë‹´ê²¨ì§„")
                if "restaurant" in all_text or "dining" in all_text:
                    korean_elements.append("ì‹ë‹¹ ë¶„ìœ„ê¸°ì˜")
                if "cooking" in all_text or "kitchen" in all_text:
                    korean_elements.append("ì£¼ë°©ì—ì„œ ì¡°ë¦¬ë˜ëŠ”")
                if "meat" in all_text:
                    korean_elements.append("ê³ ê¸° ìš”ë¦¬")
                if "noodle" in all_text or "pasta" in all_text:
                    korean_elements.append("ë©´ ìš”ë¦¬")
                if "vegetable" in all_text or "salad" in all_text:
                    korean_elements.append("ì±„ì†Œê°€ ë“¤ì–´ê°„")
                if "sauce" in all_text:
                    korean_elements.append("ì†ŒìŠ¤ê°€ ë¿Œë ¤ì§„")
                if "grill" in all_text or "fried" in all_text:
                    korean_elements.append("êµ¬ì›Œì§„")

                # === ANIMALS ===
                if "cat" in all_text or "kitten" in all_text:
                    korean_elements.append("ê³ ì–‘ì´ê°€ ë“±ì¥í•˜ê³ ")
                if "dog" in all_text or "puppy" in all_text:
                    korean_elements.append("ê°•ì•„ì§€ê°€ ë‚˜íƒ€ë‚˜ë©°")
                if "bird" in all_text:
                    korean_elements.append("ìƒˆê°€ ìˆëŠ”")

                # === ACTIONS ===
                if "sitting" in all_text:
                    korean_elements.append("ì•‰ì•„ìˆëŠ” ëª¨ìŠµ")
                if "looking" in all_text or "staring" in all_text:
                    korean_elements.append("ë¬´ì–¸ê°€ë¥¼ ì‘ì‹œí•˜ëŠ” ì¥ë©´")
                if "playing" in all_text:
                    korean_elements.append("ì‹ ë‚˜ê²Œ ë…¸ëŠ” ëª¨ìŠµ")
                if "sleeping" in all_text or "laying" in all_text:
                    korean_elements.append("í¸ì•ˆí•˜ê²Œ ì‰¬ëŠ” ì¥ë©´")
                if "walking" in all_text or "running" in all_text:
                    korean_elements.append("ì›€ì§ì´ëŠ” ëª¨ìŠµ")
                if "eating" in all_text:
                    korean_elements.append("ë¨¹ëŠ” ì¥ë©´")
                if "drinking" in all_text:
                    korean_elements.append("ë§ˆì‹œëŠ” ì¥ë©´")

                # === OBJECTS/PLACES ===
                if "chair" in all_text or "sofa" in all_text:
                    korean_elements.append("ì˜ì ìœ„ì—ì„œ")
                if "table" in all_text:
                    korean_elements.append("í…Œì´ë¸” ìœ„ì—")
                if "room" in all_text or "indoor" in all_text:
                    korean_elements.append("ì‹¤ë‚´ì—ì„œ")
                if "outdoor" in all_text or "outside" in all_text:
                    korean_elements.append("ì•¼ì™¸ì—ì„œ")
                if "grass" in all_text or "garden" in all_text:
                    korean_elements.append("ì •ì›ì—ì„œ")
                if "toy" in all_text:
                    korean_elements.append("ì¥ë‚œê°ê³¼ í•¨ê»˜")
                if "building" in all_text or "house" in all_text:
                    korean_elements.append("ê±´ë¬¼ì´ ë³´ì´ëŠ”")
                if "street" in all_text or "road" in all_text:
                    korean_elements.append("ê±°ë¦¬ì—ì„œ")

                # === PEOPLE ===
                if "person" in all_text or "man" in all_text or "woman" in all_text:
                    korean_elements.append("ì‚¬ëŒê³¼ í•¨ê»˜")
                if "child" in all_text or "kid" in all_text:
                    korean_elements.append("ì•„ì´ì™€ í•¨ê»˜")
                if "hand" in all_text or "hands" in all_text:
                    korean_elements.append("ì†ì´ ë³´ì´ëŠ”")

                # === ATMOSPHERE ===
                if "light" in all_text or "bright" in all_text:
                    korean_elements.append("ë°ì€ ì¡°ëª…ì˜")
                if "dark" in all_text or "night" in all_text:
                    korean_elements.append("ì–´ë‘ìš´ ë¶„ìœ„ê¸°")
                if "colorful" in all_text or "color" in all_text:
                    korean_elements.append("ë‹¤ì±„ë¡œìš´ ìƒ‰ê°ì˜")

                # Build result
                if korean_elements:
                    # Use more elements for richer description
                    result = ", ".join(korean_elements[:8])
                    self.logger.info(f"Korean interpretation: {result}")
                    return result
                else:
                    # Fallback - return raw descriptions for GPT to interpret
                    result = f"ì¥ë©´ ì„¤ëª…: {', '.join(descriptions[:5])}"
                    self.logger.info(f"Using raw descriptions: {result}")
                    return result

            return None

        except ImportError:
            self.logger.warning("transformers not installed, vision analysis unavailable")
            print(f"   âš  transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            print(f"   ì„¤ì¹˜: pip install --user transformers torch pillow")
            return None
        except Exception as e:
            self.logger.error(f"Vision analysis error: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None

    def _generate_script_with_ollama(self, transcription: str, duration: float) -> str:
        """
        Generate creative narration using LOCAL Ollama LLM (FREE!).

        Args:
            transcription: Video description
            duration: Video duration in seconds

        Returns:
            Creative narration script
        """
        import requests

        self.logger.info("Generating script with LOCAL Ollama LLM (FREE)")
        print(f"   ğŸ¤– ë¡œì»¬ AIë¡œ ì°½ì˜ì ì¸ ë‚˜ë ˆì´ì…˜ ìƒì„± ì¤‘...")

        # Calculate exact target length
        target_chars = int(duration * 2.8)  # ~2.8 chars per second for Korean

        # Prepare Korean prompt for creative narration
        prompt = f"""You are a witty Korean shorts narrator. The video shows: {transcription}

YOUR TASK: Create a {target_chars}-character Korean narration that is ENTERTAINING, not descriptive.

 DON'T DO THIS (boring description):
"ê³ ì–‘ì´ê°€ ì•‰ì•„ìˆëŠ” ëª¨ìŠµ, ì§‘ ì•ˆì„ ì‘ì‹œí•˜ëŠ” ì¥ë©´ì…ë‹ˆë‹¤"

 DO THIS (creative interpretation):
"ì–´? ì´ ë…€ì„ í‘œì • ì¢€ ë³´ì„¸ìš”. ë­”ê°€ ì‹¬ê°í•˜ê²Œ ê³ ë¯¼ ì¤‘ì¸ë°... ì•„ë§ˆë„ ì¸ê°„ ê´€ì°° ì¼ì§€ ì‘ì„± ì¤‘?"

RULES:
1. Pure Korean only (NO English!)
2. INTERPRET, don't describe - add personality and story
3. Use humor, curiosity, or storytelling
4. NO clichÃ©s: "ê·€ì—¬ì›Œìš”", "êµ¬ë…í•˜ì„¸ìš”", etc.
5. Exactly {target_chars} characters (Â±10)
6. Natural spoken style with personality

More examples:
- Sitting cat â†’ "ì € ìì„¸... ì™„ì „ íšŒì˜ ì¤‘ì¸ ì‚¬ì¥ë‹˜ í¬ìŠ¤ì¸ë°ìš”?"
- Looking around â†’ "ì§€ê¸ˆ ë­”ê°€ ê³„íší•˜ëŠ” ëˆˆë¹›ì´ì—ìš”. ì˜ì‹¬ìŠ¤ëŸ¬ìš´ë°..."
- Resting â†’ "ì´ê²Œ ë°”ë¡œ í”„ë¡œ ë°±ìˆ˜ì˜ ìì„¸ì£ . ë¶€ëŸ½ë„¤ìš”"

NOW CREATE: {target_chars} chars Korean narration ONLY:"""

        try:
            # Call Ollama API
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "llama3.2:3b",
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.9,
                        "top_p": 0.95,
                        "num_predict": 200
                    }
                },
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                script = result.get("response", "").strip()

                # Clean up script aggressively
                script = script.replace("ë‚˜ë ˆì´ì…˜:", "").strip()
                script = script.replace("```", "").strip()
                script = script.replace("Output:", "").strip()

                # Remove quotes if wrapped
                if script.startswith('"') and script.endswith('"'):
                    script = script[1:-1]
                if script.startswith("'") and script.endswith("'"):
                    script = script[1:-1]

                # Trim to target length if too long
                if len(script) > target_chars * 1.3:
                    script = script[:int(target_chars * 1.2)]
                    # Find last complete sentence
                    for punct in [".", "!", "?", "~"]:
                        last_idx = script.rfind(punct)
                        if last_idx > target_chars * 0.8:
                            script = script[:last_idx + 1]
                            break

                if script and len(script) > 20:
                    print(f"[OK] ì°½ì˜ì ì¸ ë‚˜ë ˆì´ì…˜ ìƒì„± ì™„ë£Œ ({len(script)}ì / ëª©í‘œ {target_chars}ì)")
                    print(f"   ìƒì„±ëœ ë‚˜ë ˆì´ì…˜:")
                    print(f"   \"{script}\"")
                    self.logger.info(f"Generated creative Ollama script: {script[:200]}")
                    return script
                else:
                    raise ValueError("Ollama generated empty or too short script")

            else:
                raise ConnectionError(f"Ollama API returned status {response.status_code}")

        except requests.exceptions.ConnectionError:
            raise ConnectionError("Ollama not running. Install: https://ollama.com, then run: ollama pull llama3.2:3b")
        except Exception as e:
            self.logger.error(f"Ollama generation failed: {e}")
            raise

    def _generate_script_template_based(self, transcription: str, duration: float, source: str = "vision") -> str:
        """
        Generate CREATIVE narration script using advanced template system.

        Args:
            transcription: Original video transcription or vision description
            duration: Video duration in seconds
            source: "audio" or "vision"

        Returns:
            Creative narration script
        """
        self.logger.info(f"Generating script with CREATIVE template method ({source})")
        print(f"    ì°½ì˜ì  í…œí”Œë¦¿ìœ¼ë¡œ ë‚˜ë ˆì´ì…˜ ìƒì„± ì¤‘... (ì†ŒìŠ¤: {source})")

        import random

        # Analyze content
        text = transcription.lower()
        target_chars = int(duration * 2.8)

        # Detect subject
        is_cat = "ê³ ì–‘ì´" in transcription
        is_dog = "ê°•ì•„ì§€" in transcription
        is_sitting = "ì•‰ì•„" in text or "sitting" in text
        is_looking = "ì‘ì‹œ" in text or "ë³´" in text or "looking" in text
        is_resting = "ì‰¬" in text or "í¸ì•ˆ" in text or "laying" in text or "sleeping" in text

        script_parts = []

        # === PART 1: Creative Hook ===
        if is_cat:
            hooks = [
                "ì ê¹ë§Œìš”, ì´ í‘œì • ì¢€ ë³´ì„¸ìš”.",
                "ì–´... ë­”ê°€ ìˆ˜ìƒí•œë°ìš”?",
                "ì´ ë…€ì„, ì§€ê¸ˆ ë­˜ í•˜ëŠ” ê±°ì£ ?",
                "ì € ëˆˆë¹›... ì‹¬ìƒì¹˜ ì•Šì€ë°ìš”.",
                "í˜¹ì‹œ ì´ê±° ë³´ì…¨ì–´ìš”?",
            ]
        elif is_dog:
            hooks = [
                "ì´ê±° ì‹¤í™”ì¸ê°€ìš”?",
                "ì ê¹, ë­”ê°€ ì´ìƒí•œë°...",
                "ì–´ë¼? ì´ ì¹œêµ¬ ì¢€ ë³´ì„¸ìš”.",
                "ì§€ê¸ˆ ì´ ìˆœê°„ì„ ë†“ì¹˜ì§€ ë§ˆì„¸ìš”!",
            ]
        else:
            hooks = [
                "ë­”ê°€ íŠ¹ì´í•œ ì¥ë©´ì´ í¬ì°©ëìŠµë‹ˆë‹¤.",
                "ìì„¸íˆ ë³´ì‹œë©´ìš”...",
                "ì´ê±° ë³´ê³  ê³„ì…¨ë‚˜ìš”?",
            ]

        script_parts.append(random.choice(hooks))

        # === PART 2: Creative Interpretation ===
        if is_cat:
            if is_sitting:
                interpretations = [
                    "ì € ìì„¸... ì™„ì „ ì¶œê·¼ ì „ ë‚˜ì˜ ëª¨ìŠµì¸ë°ìš”?",
                    "ì´ê²Œ ë°”ë¡œ í”„ë¡œ ë°±ìˆ˜ì˜ í¬ìŠ¤ì…ë‹ˆë‹¤.",
                    "ì§€ê¸ˆ íšŒì˜ ì¤‘ì¸ ì‚¬ì¥ë‹˜ ê°™ì€ í¬ìŠ¤ë„¤ìš”.",
                    "ë¶„ëª…íˆ ë­”ê°€ ì‹¬ê°í•œ ê³ ë¯¼ ì¤‘ì´ì—ìš”. ì €ë… ë©”ë‰´?",
                    "ì €ê±´ ì¸ìƒ ê³ ë¯¼í•˜ëŠ” ìì„¸ì•¼... ë‚˜ë„ ì €ë˜.",
                ]
            elif is_looking:
                interpretations = [
                    "ë­”ê°€ ê³„íší•˜ëŠ” ëˆˆë¹›... ì˜ì‹¬ìŠ¤ëŸ¬ìš´ë°ìš”.",
                    "ì € ëˆˆìœ¼ë¡œ ì¸ê°„ ê´€ì°° ì¼ì§€ ì‘ì„± ì¤‘ì¸ ë“¯.",
                    "ì§€ê¸ˆ ì„¸ê³„ ì •ë³µ ê³„íš ì„¸ìš°ëŠ” ì¤‘ ì•„ë‹Œê°€ìš”?",
                    "ì €ê±´ ì™„ì „íˆ ë­”ê°€ ê¾¸ë¯¸ëŠ” í‘œì •ì´ì—ìš”.",
                ]
            elif is_resting:
                interpretations = [
                    "ì´ê²Œ ì§„ì •í•œ ì‚¶ì˜ ì§€í˜œì£ . ë¶€ëŸ¬ì›Œìš”.",
                    "ì¼í•˜ê¸° ì‹«ì„ ë•Œ ë‚˜ì˜ ëª¨ìŠµì´ë„¤ìš”.",
                    "ë‚´ ì›Œë¼ë°¸ì€ ì–´ë””ë¡œ ê°„ ê±¸ê¹Œìš”...",
                ]
            else:
                interpretations = [
                    "ì´ ì—¬ìœ ... ë°°ìš°ê³  ì‹¶ì€ë°ìš”.",
                    "ì €ê±´ ë¶„ëª… ë­”ê°€ ê¾¸ë¯¸ê³  ìˆì–´ìš”.",
                ]
        elif is_dog:
            interpretations = [
                "ì´ëŸ° ìˆœìˆ˜í•¨ì€ ì–´ë””ì„œ ë‚˜ì˜¤ëŠ” ê±¸ê¹Œìš”?",
                "í–‰ë³µì´ ë­”ì§€ ì € ì¹œêµ¬ê°€ ì•Œë ¤ì£¼ë„¤ìš”.",
                "ì € ì—ë„ˆì§€ ì¢€ ë‚˜ëˆ ì£¼ì„¸ìš”!",
            ]
        else:
            interpretations = [
                "í‰ë²”í•´ ë³´ì´ì§€ë§Œ íŠ¹ë³„í•œ ìˆœê°„ì´ì—ìš”.",
                "ì´ëŸ° ê²Œ ì§„ì§œ ì¼ìƒì˜ í–‰ë³µ ì•„ë‹ê¹Œìš”?",
            ]

        script_parts.append(random.choice(interpretations))

        # === PART 3: Additional observations to reach target length ===
        observations = []

        if is_cat:
            observations = [
                "ì € í‘œì • ì§„ì§œ ì‹¬ê°í•œë°ìš”.",
                "ë­”ê°€ ê¹Šì€ ìƒê°ì— ì ê¸´ ë“¯.",
                "ì¸ê°„ë“¤ì€ ì´í•´ ëª» í•˜ëŠ” ê³ ë¯¼ì´ê² ì£ ?",
                "ì €ê±´ ë¶„ëª… ê³„íšì´ ìˆì–´ ë³´ì—¬ìš”.",
                "ì™„ì „ í”„ë¡œ ë°±ìˆ˜ ì¸ì •í•©ë‹ˆë‹¤.",
                "ì €ë„ ì € ì •ë„ ì—¬ìœ ëŠ” ê°–ê³  ì‹¶ë„¤ìš”.",
                "ì´ ì˜ìƒ ê³„ì† ë³´ê²Œ ë˜ë„¤ìš”.",
            ]
        elif is_dog:
            observations = [
                "ì´ ìˆœìˆ˜í•¨ ì¢€ ë³´ì„¸ìš”.",
                "ì§„ì§œ ì²œì‚¬ ì•„ë‹Œê°€ìš”?",
                "ì—ë„ˆì§€ê°€ ë„˜ì¹˜ë„¤ìš”.",
                "í–‰ë³µì´ ë­”ì§€ ì•Œë ¤ì£¼ëŠ” ë“¯.",
            ]
        else:
            observations = [
                "ì´ëŸ° ìˆœê°„ë“¤ì´ ì†Œì¤‘í•˜ì£ .",
                "í‰ë²”í•˜ì§€ë§Œ íŠ¹ë³„í•œ ì¥ë©´ì´ì—ìš”.",
                "ì¼ìƒ ì† ì‘ì€ í–‰ë³µì´ë„¤ìš”.",
            ]

        # Add observations until reaching target length
        script_parts_text = " ".join(script_parts)
        current_length = len(script_parts_text)

        while current_length < target_chars * 0.9 and observations:
            obs = random.choice(observations)
            observations.remove(obs)
            script_parts.append(obs)
            current_length += len(obs) + 1

        # === PART 4: Natural Ending (NO CLICHÃ‰ CTA) ===
        endings = [
            "ì—¬ëŸ¬ë¶„ë„ ê³µê°í•˜ì‹œë‚˜ìš”?",
            "ì´ë˜ì„œ ëª» ë§ë¦¬ì£ .",
            "ì˜¤ëŠ˜ë„ í‰í™”ë¡­ë„¤ìš”.",
            "ì—­ì‹œ ì˜ˆìƒì„ ë²—ì–´ë‚˜ì§€ ì•Šì•„ìš”.",
            "ë­”ê°€ ìœ„ë¡œê°€ ë˜ëŠ” ì¥ë©´ì´ë„¤ìš”.",
            "ì´ê²Œ ë°”ë¡œ íë§ ì•„ë‹ê¹Œìš”?",
        ]

        script_parts.append(random.choice(endings))

        # Combine
        script = " ".join(script_parts)

        # Final length check
        if len(script) > target_chars * 1.2:
            # Too long, trim to last sentence
            for punct in [".", "!", "?", "~"]:
                last_idx = script.rfind(punct, 0, int(target_chars * 1.1))
                if last_idx > target_chars * 0.7:
                    script = script[:last_idx + 1]
                    break

        print(f"[OK] ì°½ì˜ì ì¸ ë‚˜ë ˆì´ì…˜ ìƒì„± ì™„ë£Œ ({len(script)}ì / ëª©í‘œ {target_chars}ì)")
        print(f"   ìƒì„±ëœ ë‚˜ë ˆì´ì…˜:")
        print(f"   \"{script}\"")
        self.logger.info(f"Generated creative template script: {script[:200]}")

        return script

    def _generate_script_with_gpt(
        self,
        vision_description: str = None,
        audio_transcription: str = None,
        duration: float = 60
    ) -> str:
        """
        Generate creative narration script using GPT-4 (COSTS ~$0.01-0.02).

        ì£¼ì˜: vision_descriptionì„ ë©”ì¸ìœ¼ë¡œ, audio_transcriptionì€ ì°¸ê³ ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Args:
            vision_description: ì˜ìƒ í™”ë©´ ë¶„ì„ ê²°ê³¼ (ë©”ì¸)
            audio_transcription: ìŒì„± ì¸ì‹ ê²°ê³¼ (ì°¸ê³ ìš©)
            duration: Video duration in seconds

        Returns:
            Creative narration script
        """
        self.logger.info("Using GPT-4 for creative narration (vision-first)")
        print(f"   ğŸ¤– GPT-4ë¡œ ì°½ì˜ì ì¸ ë‚˜ë ˆì´ì…˜ ìƒì„± ì¤‘... (ì•½ â‚©13-26ì›)")

        target_chars = int(duration * 2.8)

        # Build context
        context_parts = []
        if vision_description:
            context_parts.append(f" ì˜ìƒ í™”ë©´ (ë©”ì¸): {vision_description}")
        if audio_transcription:
            context_parts.append(f" ìŒì„± ë‚´ìš© (ì°¸ê³ ): {audio_transcription}")

        context = "\n".join(context_parts)

        # Load prompt template
        narration_template = self._load_prompt_template("narration_gpt.txt")

        if narration_template:
            prompt = narration_template.format(
                context=context,
                duration=duration,
                target_chars=target_chars
            )
        else:
            # Fallback to default
            prompt = f"""{context}

ì˜ìƒ ê¸¸ì´: {duration:.0f}ì´ˆ (ëª©í‘œ: {target_chars}ì)

ìœ„ ì˜ìƒì„ ë°”íƒ•ìœ¼ë¡œ {duration:.0f}ì´ˆì— ë§ëŠ” ì°½ì˜ì ì´ê³  ì¬ë¯¸ìˆëŠ” í•œêµ­ì–´ ë‚˜ë ˆì´ì…˜ì„ ì‘ì„±í•˜ì„¸ìš”.

âš  ì¤‘ìš”:
- ì˜ìƒ í™”ë©´ ë¶„ì„()ì„ ë©”ì¸ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”
- ìŒì„± ë‚´ìš©()ì€ ì°¸ê³ ë§Œ í•˜ì„¸ìš” (Whisper ì˜¤ì¸ì‹ ê°€ëŠ¥ì„± ìˆìŒ)
- ìŒì„±ê³¼ í™”ë©´ì´ ë‹¤ë¥´ë©´ í™”ë©´ì„ ìš°ì„ í•˜ì„¸ìš”

 í•„ìˆ˜ ì¡°ê±´:
1. ìˆœìˆ˜ í•œêµ­ì–´ë§Œ ì‚¬ìš© (ì˜ì–´/ì™¸êµ­ì–´ ê¸ˆì§€)
2. ì‹ìƒí•œ í‘œí˜„ ê¸ˆì§€ ("ê·€ì—¬ì›Œìš”", "êµ¬ë…í•´ì£¼ì„¸ìš”" ë“±)
3. ì˜ìƒ í™”ë©´ì„ ì¬ë¯¸ìˆê²Œ í•´ì„í•˜ê³  ìŠ¤í† ë¦¬í…”ë§
4. ì •í™•íˆ {target_chars}ì (Â±10ì)
5. ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´

 êµ¬ì¡°:
- ì‹œì‘: í˜¸ê¸°ì‹¬ ìœ ë°œ (1-2ë¬¸ì¥)
- ì¤‘ê°„: ì¬ë¯¸ìˆëŠ” í•´ì„/ê´€ì°° (2-3ë¬¸ì¥)
- ë: ìì—°ìŠ¤ëŸ¬ìš´ ë§ˆë¬´ë¦¬ (CTA ê¸ˆì§€)

 ê¸ˆì§€:
- ë‹¨ìˆœ ì„¤ëª… ("ìŒì‹ì´ ìˆìŠµë‹ˆë‹¤")
- ì˜ìƒê³¼ ë¬´ê´€í•œ ì¼ë°˜ì  ë©˜íŠ¸
- "êµ¬ë…", "ì¢‹ì•„ìš”", "ì•Œë¦¼" ê°™ì€ CTA
- ìŒì„± ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë”°ë¼ ì½ê¸°

ì˜ˆì‹œ ìŠ¤íƒ€ì¼:
"ì ê¹, ì´ ë¹„ì£¼ì–¼ ì¢€ ë³´ì„¸ìš”. ë²Œì¨ë¶€í„° ì¹¨ìƒ˜ ìê·¹ë˜ëŠ”ë°ìš”? ì´ê²Œ ë°”ë¡œ ì§„ì§œ ë§›ì§‘ì˜ í¬ìŠ¤ì£ . ì € ìƒ‰ê°, ì € ì§ˆê°... í™”ë©´ìœ¼ë¡œ ë´ë„ ë§›ì´ ëŠê»´ì§€ë„¤ìš”."

ë‚˜ë ˆì´ì…˜ë§Œ ì¶œë ¥:"""

        try:
            response = self.client.chat.completions.create(
                model=os.getenv("NARRATION_MODEL", "gpt-4o-mini"),
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ìœ ë¨¸ëŸ¬ìŠ¤í•œ ì‡¼ì¸  ì˜ìƒ ë‚˜ë ˆì´í„°ì…ë‹ˆë‹¤. ì˜ìƒ í™”ë©´ì„ ë³´ê³  ì¬ë¯¸ìˆê²Œ í•´ì„í•˜ë©°, ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‹œì²­ìì™€ ì†Œí†µí•©ë‹ˆë‹¤. ìŒì„± ì¸ì‹ ê²°ê³¼ëŠ” ì°¸ê³ ë§Œ í•˜ê³  (ì˜¤ì¸ì‹ ê°€ëŠ¥), í™”ë©´ ë‚´ìš©ì„ ë©”ì¸ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.9,
                max_tokens=300
            )

            script = response.choices[0].message.content.strip()

            # Clean up
            script = script.replace('"', '').replace("'", '').strip()

            print(f"[OK] ì°½ì˜ì ì¸ ë‚˜ë ˆì´ì…˜ ìƒì„± ì™„ë£Œ ({len(script)}ì / ëª©í‘œ {target_chars}ì)")
            print(f"   ìƒì„±ëœ ë‚˜ë ˆì´ì…˜:")
            print(f"   \"{script}\"")
            self.logger.info(f"Generated GPT script (vision-first): {script[:200]}")

            return script

        except Exception as e:
            self.logger.error(f"GPT script generation failed: {e}")
            raise

    def _clean_script_for_tts(self, script: str) -> str:
        """
        Clean script text to remove problematic characters for TTS.

        Removes:
        - Backslash escape sequences (\", \\, etc.)
        - [Request interrupted by user] and similar patterns
        - Extra quotes
        - Multiple spaces

        Converts:
        - Numbers to Korean (3ë²ˆ -> ì„¸ ë²ˆ, 10ë¶„ -> ì‹­ ë¶„)

        Args:
            script: Raw script text

        Returns:
            Cleaned script text safe for TTS
        """
        import re

        cleaned = script

        # ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤ë¥¼ ë¨¼ì € ì²˜ë¦¬ (ë°±ìŠ¬ë˜ì‹œë§Œ ì œê±°í•˜ë©´ \nì´ nìœ¼ë¡œ ë‚¨ìŒ)
        cleaned = cleaned.replace('\\n', ' ')  # ì¤„ë°”ê¿ˆ -> ê³µë°±
        cleaned = cleaned.replace('\\t', ' ')  # íƒ­ -> ê³µë°±
        cleaned = cleaned.replace('\\r', '')   # ìºë¦¬ì§€ ë¦¬í„´ ì œê±°
        cleaned = cleaned.replace('\\"', '"')  # ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œ
        cleaned = cleaned.replace("\\'", "'")  # ì´ìŠ¤ì¼€ì´í”„ëœ ì‘ì€ë”°ì˜´í‘œ
        cleaned = cleaned.replace('\\\\', '')  # ì´ì¤‘ ë°±ìŠ¬ë˜ì‹œ
        # ë‚¨ì€ ë°±ìŠ¬ë˜ì‹œ ì œê±° (TTSê°€ "ë°±ìŠ¬ë˜ì‹œ"ë¡œ ì½ìŒ)
        cleaned = cleaned.replace('\\', '')

        # Remove common error/interrupt messages
        cleaned = re.sub(r'\[Request interrupted by user\]', '', cleaned)
        cleaned = re.sub(r'\[.*?interrupted.*?\]', '', cleaned, flags=re.IGNORECASE)
        cleaned = re.sub(r'\[.*?error.*?\]', '', cleaned, flags=re.IGNORECASE)

        # Remove markdown code block markers
        cleaned = cleaned.replace('```', '')

        # Fix double quotes
        cleaned = cleaned.replace('""', '"')
        cleaned = cleaned.replace("''", "'")

        # Convert numbers to Korean for better TTS pronunciation
        cleaned = self._convert_numbers_to_korean(cleaned)

        # Clean up multiple spaces
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()

        return cleaned

    def _add_emotion_tags(self, text: str, enable_emotion: bool = True) -> str:
        """
        Add SSML emotion tags to text based on punctuation and keywords.

        Args:
            text: Clean text
            enable_emotion: Whether to add emotion tags (default: True)

        Returns:
            Text with SSML emotion tags
        """
        if not enable_emotion:
            return text

        import re

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = re.split(r'([.!?\n])', text)
        result = []

        for i, sentence in enumerate(sentences):
            if not sentence.strip() or sentence in '.!?\n':
                result.append(sentence)
                continue

            sentence_lower = sentence.lower()

            # ê°ì • í‚¤ì›Œë“œ ê°ì§€
            # ê¸´ì¥/ì¶©ê²© (pitch up, rate faster)
            if any(kw in sentence_lower for kw in ['ë†€ë', 'ì¶©ê²©', 'ê¹œì§', 'ì–´', 'ì•„', 'í—‰', 'ì•—']):
                result.append(f'<prosody pitch="+15%" rate="1.15">{sentence}</prosody>')

            # ìŠ¬í””/ìš°ìš¸ (pitch down, rate slower)
            elif any(kw in sentence_lower for kw in ['ìŠ¬í”„', 'ëˆˆë¬¼', 'ìš¸', 'ì£½', 'ë– ë‚¬', 'ë¯¸ì•ˆ', 'ì•ˆíƒ€ê¹Œìš´']):
                result.append(f'<prosody pitch="-10%" rate="0.90">{sentence}</prosody>')

            # ë¶„ë…¸/ê°•ì¡° (pitch up, rate faster, louder)
            elif any(kw in sentence_lower for kw in ['í™”', 'ë¶„ë…¸', 'ë¯¸ì³¤', 'ì‹«', 'ì•ˆ ë¼', 'ì ˆëŒ€']) or sentence.endswith('!'):
                result.append(f'<prosody pitch="+10%" rate="1.10" volume="+10%">{sentence}</prosody>')

            # ì†ì‚­ì„/ë¹„ë°€ (pitch down, rate slower, quieter)
            elif any(kw in sentence_lower for kw in ['ì†ì‚­', 'ì¡°ìš©', 'ë¹„ë°€', 'ì‰¿', 'ëª°ë˜']):
                result.append(f'<prosody pitch="-5%" rate="0.85" volume="-15%">{sentence}</prosody>')

            # ì§ˆë¬¸ (pitch up at end)
            elif sentence.strip().endswith('?'):
                result.append(f'<prosody pitch="+8%">{sentence}</prosody>')

            # ê¸°ë³¸ (ë³€ê²½ ì—†ìŒ)
            else:
                result.append(sentence)

        # ëŠë¦° ì¥ë©´ ì „í™˜ì— ì•½ê°„ì˜ pause ì¶”ê°€
        result_text = ''.join(result)
        result_text = result_text.replace('.\n', '.<break time="300ms"/>\n')
        result_text = result_text.replace('!\n', '!<break time="400ms"/>\n')
        result_text = result_text.replace('?\n', '?<break time="350ms"/>\n')

        return result_text

    def _convert_numbers_to_korean(self, text: str) -> str:
        """
        Convert numbers in text to Korean pronunciation.

        Examples:
        - "3ë²ˆ" -> "ì„¸ ë²ˆ"
        - "10ë¶„" -> "ì‹­ ë¶„"
        - "2023ë…„" -> "ì´ì²œì´ì‹­ì‚¼ ë…„"
        - "1ë“±" -> "ì¼ ë“±"
        - "010-1234-5678" -> "ê³µ ì¼ ê³µ ì¼ ì´ ì‚¼ ì‚¬ ì˜¤ ìœ¡ ì¹  íŒ”"

        Args:
            text: Text containing numbers

        Returns:
            Text with numbers converted to Korean
        """
        import re

        # ì „í™”ë²ˆí˜¸ íŒ¨í„´ ì²˜ë¦¬ (010-1234-5678, 02-123-4567 ë“±)
        def convert_phone_number(match):
            """ì „í™”ë²ˆí˜¸ë¥¼ í•œ ê¸€ìì”© ì½ê¸°"""
            phone = match.group(0)
            # ìˆ«ìë§Œ ì¶”ì¶œ
            digits = re.sub(r'[^\d]', '', phone)
            # ê° ìˆ«ìë¥¼ í•œê¸€ë¡œ ë³€í™˜
            digit_names = ['ê³µ', 'ì¼', 'ì´', 'ì‚¼', 'ì‚¬', 'ì˜¤', 'ìœ¡', 'ì¹ ', 'íŒ”', 'êµ¬']
            result = ' '.join([digit_names[int(d)] for d in digits])
            return result

        # ì „í™”ë²ˆí˜¸ íŒ¨í„´ (010-xxxx-xxxx, 02-xxx-xxxx, 031-xxx-xxxx ë“±)
        # 0ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì „í™”ë²ˆí˜¸ í˜•ì‹
        phone_pattern = r'0\d{1,2}[-\s]?\d{3,4}[-\s]?\d{4}'
        text = re.sub(phone_pattern, convert_phone_number, text)

        # ë¹„ë°€ë²ˆí˜¸/ì½”ë“œ íŒ¨í„´ ì²˜ë¦¬ (4ìë¦¬ ì´ìƒ ì—°ì† ìˆ«ì)
        # ì˜ˆ: "ë¹„ë°€ë²ˆí˜¸ëŠ” 1234ì…ë‹ˆë‹¤" -> "ë¹„ë°€ë²ˆí˜¸ëŠ” ì¼ ì´ ì‚¼ ì‚¬ì…ë‹ˆë‹¤"
        def convert_code(match):
            """ì—°ì†ëœ ìˆ«ìë¥¼ í•œ ê¸€ìì”© ì½ê¸°"""
            prefix = match.group(1) if match.group(1) else ''
            code = match.group(2)
            digit_names = ['ê³µ', 'ì¼', 'ì´', 'ì‚¼', 'ì‚¬', 'ì˜¤', 'ìœ¡', 'ì¹ ', 'íŒ”', 'êµ¬']
            result = ' '.join([digit_names[int(d)] for d in code])
            return prefix + result

        # ë¹„ë°€ë²ˆí˜¸, ì•”í˜¸, ì½”ë“œ ë“±ì˜ í‚¤ì›Œë“œ ë’¤ì— ì˜¤ëŠ” ìˆ«ì
        code_pattern = r'(ë¹„ë°€ë²ˆí˜¸ëŠ”?|ì•”í˜¸ëŠ”?|ì½”ë“œëŠ”?|ë²ˆí˜¸ëŠ”?)\s*(\d{4,})'
        text = re.sub(code_pattern, convert_code, text)

        def num_to_korean(num: int, sino: bool = True) -> str:
            """
            Convert number to Korean.

            Args:
                num: Number to convert
                sino: If True, use Sino-Korean (ì¼ì´ì‚¼), if False use Native Korean (í•˜ë‚˜ë‘˜ì…‹)
            """
            if sino:
                # Sino-Korean (í•œìì–´ ìˆ«ì)
                ones = ['', 'ì¼', 'ì´', 'ì‚¼', 'ì‚¬', 'ì˜¤', 'ìœ¡', 'ì¹ ', 'íŒ”', 'êµ¬']
                tens = ['', 'ì‹­', 'ì´ì‹­', 'ì‚¼ì‹­', 'ì‚¬ì‹­', 'ì˜¤ì‹­', 'ìœ¡ì‹­', 'ì¹ ì‹­', 'íŒ”ì‹­', 'êµ¬ì‹­']

                if num == 0:
                    return 'ì˜'

                if num < 10:
                    return ones[num]
                elif num < 100:
                    ten_digit = num // 10
                    one_digit = num % 10
                    result = tens[ten_digit]
                    if one_digit > 0:
                        result += ' ' + ones[one_digit]
                    return result
                elif num < 1000:
                    hundred_digit = num // 100
                    remainder = num % 100
                    # "ì¼ë°±" -> "ë°±" (1ì€ ìƒëµ)
                    if hundred_digit == 1:
                        result = 'ë°±'
                    else:
                        result = ones[hundred_digit] + 'ë°±'
                    if remainder > 0:
                        result += ' ' + num_to_korean(remainder, sino=True)
                    return result
                elif num < 10000:
                    thousand_digit = num // 1000
                    remainder = num % 1000
                    # "ì¼ì²œ" -> "ì²œ" (1ì€ ìƒëµ)
                    if thousand_digit == 1:
                        result = 'ì²œ'
                    else:
                        result = ones[thousand_digit] + 'ì²œ'
                    if remainder > 0:
                        result += ' ' + num_to_korean(remainder, sino=True)
                    return result
                elif num < 100000000:  # 1ì–µ ë¯¸ë§Œ (ë§Œ ë‹¨ìœ„)
                    man_digit = num // 10000
                    remainder = num % 10000
                    result = num_to_korean(man_digit, sino=True) + 'ë§Œ'
                    if remainder > 0:
                        result += ' ' + num_to_korean(remainder, sino=True)
                    return result
                elif num < 1000000000000:  # 1ì¡° ë¯¸ë§Œ (ì–µ ë‹¨ìœ„)
                    eok_digit = num // 100000000
                    remainder = num % 100000000
                    result = num_to_korean(eok_digit, sino=True) + 'ì–µ'
                    if remainder > 0:
                        result += ' ' + num_to_korean(remainder, sino=True)
                    return result
                else:
                    # For very large numbers, just return as is
                    return str(num)
            else:
                # Native Korean (ê³ ìœ ì–´ ìˆ«ì) - for counting things
                native = ['', 'í•˜ë‚˜', 'ë‘˜', 'ì…‹', 'ë„·', 'ë‹¤ì„¯', 'ì—¬ì„¯', 'ì¼ê³±', 'ì—¬ëŸ', 'ì•„í™‰', 'ì—´']

                if num < 1 or num > 99:
                    # Use Sino-Korean for numbers outside native range
                    return num_to_korean(num, sino=True)

                if num <= 10:
                    return native[num]
                elif num < 20:
                    return 'ì—´' + (' ' + native[num - 10] if num > 10 else '')
                elif num < 100:
                    ten_digit = num // 10
                    one_digit = num % 10
                    # 20ëŒ€ ìˆ«ìëŠ” ë‹¨ìœ„ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
                    # "ìŠ¤ë¬¼" (ê¸°ë³¸) vs "ìŠ¤ë¬´" (ë°›ì¹¨ìœ¼ë¡œ ëë‚˜ëŠ” ë‹¨ìœ„ ì•: ê°œ, ëª…, ë§ˆë¦¬)
                    # "ì‚´"ì€ ë°›ì¹¨ì´ ì—†ìœ¼ë¯€ë¡œ "ìŠ¤ë¬¼"ì„ ì‚¬ìš©í•´ì•¼ í•¨
                    tens_native = ['', '', 'ìŠ¤ë¬¼', 'ì„œë¥¸', 'ë§ˆí”', 'ì‰°', 'ì˜ˆìˆœ', 'ì¼í”', 'ì—¬ë“ ', 'ì•„í”']
                    result = tens_native[ten_digit]
                    if one_digit > 0:
                        result += ' ' + native[one_digit]
                    return result
                else:
                    return num_to_korean(num, sino=True)

        def replace_number(match):
            """Replace matched number with Korean."""
            full_match = match.group(0)
            num_str = match.group(1)
            unit = match.group(2) if match.group(2) else ''

            try:
                num = int(num_str)

                # Decide whether to use Sino or Native Korean based on unit
                # Native Korean (ê³ ìœ ì–´) units: ë²ˆ, ê°œ, ëª…, ë§ˆë¦¬, ì‚´, ì‹œ
                # Sino Korean (í•œìì–´) units: ë¶„, ì´ˆ, ë…„, ì›”, ì¼, ë“±, ìœ„, íšŒ, ì°¨
                native_units = ['ë²ˆ', 'ë²ˆì§¸', 'ê°œ', 'ëª…', 'ë§ˆë¦¬', 'ì‚´', 'ì‹œ']
                sino_units = ['ë¶„', 'ì´ˆ', 'ë…„', 'ì›”', 'ì¼', 'ë“±', 'ìœ„', 'íšŒ', 'ì°¨', 'ì¸µ', 'ëŒ€', 'ê¶Œ', 'ì¥', 'ê³¡', 'í¸', 'í™”', 'ê¸°']

                # Determine reading style
                use_native = False
                if unit:
                    # Check if unit requires native Korean
                    if any(unit.startswith(u) for u in native_units) and num <= 99:
                        use_native = True
                    # For 10 with ë²ˆ, use native "ì—´"
                    elif num == 10 and unit.startswith('ë²ˆ'):
                        use_native = True

                korean_num = num_to_korean(num, sino=not use_native)

                # ë°›ì¹¨ íƒˆë½ ì²˜ë¦¬: ì…‹â†’ì„¸, ë„·â†’ë„¤
                if unit and use_native:
                    if korean_num == 'ì…‹':
                        korean_num = 'ì„¸'
                    elif korean_num == 'ë„·':
                        korean_num = 'ë„¤'
                    elif korean_num.startswith('ì…‹ '):
                        korean_num = 'ì„¸ ' + korean_num[2:]
                    elif korean_num.startswith('ë„· '):
                        korean_num = 'ë„¤ ' + korean_num[2:]

                    # ìŠ¤ë¬¼ â†’ ìŠ¤ë¬´ ë³€í™˜ (ë°›ì¹¨ìœ¼ë¡œ ëë‚˜ëŠ” ë‹¨ìœ„ ì•ì—ì„œë§Œ)
                    # ë°›ì¹¨ìœ¼ë¡œ ëë‚˜ëŠ” ë‹¨ìœ„: ê°œ, ëª…, ë§ˆë¦¬
                    # ë°›ì¹¨ì´ ì—†ëŠ” ë‹¨ìœ„: ì‚´, ë²ˆ, ì‹œ, ë²ˆì§¸
                    batchim_units = ['ê°œ', 'ëª…', 'ë§ˆë¦¬']
                    if any(unit.startswith(u) for u in batchim_units):
                        # "ìŠ¤ë¬¼ ì…‹" -> "ìŠ¤ë¬´ ì„¸" (ê³µë°± ìˆìŒ)
                        if korean_num.startswith('ìŠ¤ë¬¼ '):
                            korean_num = 'ìŠ¤ë¬´ ' + korean_num[3:]
                        # "ìŠ¤ë¬¼ì…‹" -> "ìŠ¤ë¬´ì„¸" (ê³µë°± ì—†ìŒ, ì‹¤ì œë¡œëŠ” ë°œìƒ ì•ˆí•¨)
                        elif korean_num.startswith('ìŠ¤ë¬¼'):
                            korean_num = 'ìŠ¤ë¬´' + korean_num[2:]
                        # "ìŠ¤ë¬¼" ë‹¨ë…
                        elif korean_num == 'ìŠ¤ë¬¼':
                            korean_num = 'ìŠ¤ë¬´'

                # Add space between number and unit for better pronunciation
                if unit:
                    return korean_num + ' ' + unit
                else:
                    return korean_num

            except ValueError:
                return full_match

        # Pattern: number followed by optional unit (ë²ˆ, ë¶„, ì´ˆ, ê°œ, ëª…, ë“±, ìœ„, ë…„, ì›”, ì¼, etc.)
        # Match numbers with common units
        pattern = r'(\d+)(ë²ˆì§¸|ë²ˆ|ë¶„|ì´ˆ|ê°œ|ëª…|ë§ˆë¦¬|ì‚´|ì‹œ|ë“±|ìœ„|ë…„|ì›”|ì¼|íšŒ|ì°¨|ì¸µ|ëŒ€|ê¶Œ|ì¥|ê³¡|í¸|í™”|ê¸°|ì›|ë‹¬ëŸ¬|í‚¬ë¡œ|ë¯¸í„°|ì„¼í‹°|ê·¸ë¨|ë¦¬í„°)?'

        result = re.sub(pattern, replace_number, text)

        return result

    def _process_control_commands(self, text: str) -> tuple:
        """
        Process control commands in narration text like [ë¬´ìŒ 3ì´ˆ], [ì¹¨ë¬µ], etc.
        Returns (cleaned_text, pauses) where pauses is list of (position_in_chars, duration).
        """
        import re

        # Patterns: [ë¬´ìŒ 3ì´ˆ], [ì¹¨ë¬µ 2ì´ˆ], [pause 3ì´ˆ], [ë¬´ìŒ], [ì¹¨ë¬µ]
        pattern = r'\[(ë¬´ìŒ|ì¹¨ë¬µ|pause)\s*(\d+(?:\.\d+)?)?ì´ˆ?\]'

        pauses = []
        cleaned_segments = []
        last_end = 0

        for match in re.finditer(pattern, text):
            # Add text before this command
            segment = text[last_end:match.start()].strip()
            if segment:
                cleaned_segments.append(segment)

            # Extract pause duration
            duration_str = match.group(2)
            if duration_str:
                duration = float(duration_str)
            else:
                # Set different defaults based on command type
                command_type = match.group(1)
                if command_type == 'ë¬´ìŒ':
                    duration = 2.0  # ë¬´ìŒ: 2 seconds
                elif command_type == 'ì¹¨ë¬µ':
                    duration = 3.0  # ì¹¨ë¬µ: 3 seconds
                elif command_type == 'pause':
                    duration = 2.0  # pause: 2 seconds
                else:
                    duration = 1.0  # Fallback

            # Record pause (position is the length so far)
            pauses.append({
                'position': len(' '.join(cleaned_segments)),
                'duration': duration
            })

            last_end = match.end()

        # Add remaining text
        remaining = text[last_end:].strip()
        if remaining:
            cleaned_segments.append(remaining)

        cleaned_text = ' '.join(cleaned_segments)

        return cleaned_text, pauses

    def generate_speech(
        self,
        script: str,
        output_path: Path,
        use_free_tts: bool = True
    ) -> Path:
        """
        Generate speech audio from script using FREE TTS.
        Handles control commands like [ë¬´ìŒ 3ì´ˆ] automatically.

        Args:
            script: Narration script (may contain control commands)
            output_path: Output audio file path
            use_free_tts: Use free TTS (edge-tts) instead of OpenAI

        Returns:
            Path to generated audio file
        """
        # First clean the script (remove backslashes, error messages, convert numbers)
        script = self._clean_script_for_tts(script)

        # Add emotion tags (optional - controlled by environment variable)
        enable_emotion = os.getenv("TTS_ENABLE_EMOTION", "true").lower() == "true"
        script = self._add_emotion_tags(script, enable_emotion=enable_emotion)

        # Then process control commands
        cleaned_script, pauses = self._process_control_commands(script)

        if pauses:
            print(f"   [Control] Detected {len(pauses)} pause commands")
            for i, pause in enumerate(pauses, 1):
                print(f"     {i}. Pause {pause['duration']}ì´ˆ at position ~{pause['position']}")

        audio_path = output_path.with_suffix('.mp3')

        if use_free_tts:
            # Use FREE Microsoft Edge TTS (HIGH QUALITY)
            return self._generate_speech_edge_tts(cleaned_script, audio_path, pauses)
        else:
            # Use OpenAI TTS (PAID)
            return self._generate_speech_openai(cleaned_script, audio_path, pauses)

    def _insert_pauses_into_audio(self, audio_path: Path, script: str, pauses: list) -> Path:
        """
        Insert silence at specified pause positions in audio file.

        Args:
            audio_path: Path to audio file to modify
            script: The cleaned script text (without control commands)
            pauses: List of pause dicts with 'position' (char index) and 'duration' (seconds)

        Returns:
            Path to modified audio file
        """
        try:
            from pydub import AudioSegment
            from pydub.silence import detect_nonsilent

            self.logger.info(f"Inserting {len(pauses)} pauses into audio")
            print(f"   [Pause] Inserting {len(pauses)} silence segments...")

            # Load audio
            audio = AudioSegment.from_file(str(audio_path))
            audio_duration_ms = len(audio)

            # Calculate time per character (approximate)
            if len(script) == 0:
                self.logger.warning("Script is empty, cannot calculate pause positions")
                return audio_path

            ms_per_char = audio_duration_ms / len(script)

            # Sort pauses by position
            sorted_pauses = sorted(pauses, key=lambda p: p['position'])

            # Build segments
            segments = []
            last_pos_ms = 0

            for i, pause in enumerate(sorted_pauses):
                # Calculate time position in audio
                time_ms = int(pause['position'] * ms_per_char)

                # Ensure position is within audio bounds
                time_ms = max(0, min(time_ms, audio_duration_ms))

                # Add audio segment before pause
                if time_ms > last_pos_ms:
                    segments.append(audio[last_pos_ms:time_ms])

                # Add silence
                silence_ms = int(pause['duration'] * 1000)
                segments.append(AudioSegment.silent(duration=silence_ms))

                print(f"     {i+1}. Inserted {pause['duration']}ì´ˆ silence at {time_ms/1000:.1f}s")

                last_pos_ms = time_ms

            # Add remaining audio
            if last_pos_ms < audio_duration_ms:
                segments.append(audio[last_pos_ms:])

            # Combine all segments
            final_audio = sum(segments) if segments else audio

            # Export back to same file
            final_audio.export(str(audio_path), format="mp3")

            new_duration = len(final_audio) / 1000
            print(f"   [OK] Audio with pauses: {new_duration:.1f}s (original: {audio_duration_ms/1000:.1f}s)")
            self.logger.info(f"Pauses inserted, new duration: {new_duration:.1f}s")

            return audio_path

        except ImportError:
            self.logger.warning("pydub not installed, skipping pause insertion")
            print(f"   [Warning] pydub not installed, pauses will be ignored")
            print(f"   Install with: pip install pydub")
            return audio_path
        except Exception as e:
            self.logger.error(f"Failed to insert pauses: {e}")
            print(f"   [Warning] Failed to insert pauses: {e}")
            return audio_path

    def _generate_speech_edge_tts(self, script: str, audio_path: Path, pauses: list = None) -> Path:
        """Generate speech using FREE Microsoft Edge TTS with pause support."""
        try:
            import asyncio
            import edge_tts

            self.logger.info("Generating speech with FREE Edge TTS")
            print(f"\n[TTS] Generating Korean narration audio (FREE)...")

            # Script is already cleaned by generate_speech()
            # Korean voices
            # ko-KR-SoonBokNeural (Female, warm, default)
            # ko-KR-SunHiNeural (Female, natural)
            # ko-KR-InJoonNeural (Male, natural)
            voice = os.getenv("TTS_VOICE", "ko-KR-SoonBokNeural")

            async def generate():
                communicate = edge_tts.Communicate(script, voice)
                await communicate.save(str(audio_path))

            # Run async function
            asyncio.run(generate())

            # If pauses exist, insert silence
            if pauses:
                self._insert_pauses_into_audio(audio_path, script, pauses)

            print(f"[OK] Free TTS narration generated")
            self.logger.info(f"Speech generated: {audio_path}")

            return audio_path

        except Exception as e:
            self.logger.error(f"Edge TTS failed: {e}")
            # Fallback to gTTS
            return self._generate_speech_gtts(script, audio_path, pauses)

    def _generate_speech_gtts(self, script: str, audio_path: Path, pauses: list = None) -> Path:
        """Generate speech using FREE Google TTS (backup) with pause support."""
        try:
            from gtts import gTTS

            self.logger.info("Generating speech with FREE Google TTS")
            print(f"\n[TTS] Generating Korean narration audio (Google TTS)...")

            # Script is already cleaned by generate_speech()
            tts = gTTS(text=script, lang='ko', slow=False)
            tts.save(str(audio_path))

            # If pauses exist, insert silence
            if pauses:
                self._insert_pauses_into_audio(audio_path, script, pauses)

            print(f"[OK] Free TTS narration generated")
            return audio_path

        except Exception as e:
            self.logger.error(f"gTTS failed: {e}")
            raise

    def _generate_speech_openai(self, script: str, audio_path: Path, pauses: list = None) -> Path:
        """Generate speech using OpenAI TTS (PAID) with pause support."""
        if not self.client:
            raise ValueError("OpenAI API key not found")

        self.logger.info("Generating speech with OpenAI TTS (PAID)")
        print(f"\n[TTS] Generating narration audio with OpenAI TTS...")

        try:
            # Script is already cleaned by generate_speech()
            voice = os.getenv("TTS_VOICE", "alloy")

            response = self.client.audio.speech.create(
                model="tts-1",
                voice=voice,
                input=script,
                speed=1.0
            )

            response.stream_to_file(str(audio_path))

            # If pauses exist, insert silence
            if pauses:
                self._insert_pauses_into_audio(audio_path, script, pauses)

            print(f"[OK] Narration audio generated")
            return audio_path

        except Exception as e:
            self.logger.error(f"OpenAI TTS failed: {e}")
            raise

    def add_narration_to_video(
        self,
        clip: VideoFileClip,
        narration_audio_path: Path,
        mix_ratio: float = 0.3
    ) -> VideoFileClip:
        """
        Add narration audio to video with duration matching.

        Args:
            clip: Video clip
            narration_audio_path: Path to narration audio
            mix_ratio: Original audio volume ratio (0-1)

        Returns:
            Video with narration
        """
        print(f"\nğŸ”Š ë‚˜ë ˆì´ì…˜ ë¯¹ì‹± ì¤‘...")

        try:
            # Load narration audio
            narration = AudioFileClip(str(narration_audio_path))

            video_duration = clip.duration
            narration_duration = narration.duration

            print(f"   ë™ì˜ìƒ ê¸¸ì´: {video_duration:.1f}ì´ˆ")
            print(f"   ë‚˜ë ˆì´ì…˜ ê¸¸ì´: {narration_duration:.1f}ì´ˆ")

            # Calculate duration difference
            duration_diff = abs(video_duration - narration_duration)
            diff_percent = (duration_diff / video_duration) * 100

            # === STRATEGY: Match narration to video duration ===
            if duration_diff > 1.0:  # More than 1 second difference
                print(f"   âš  ê¸¸ì´ ì°¨ì´: {duration_diff:.1f}ì´ˆ ({diff_percent:.1f}%)")

                if narration_duration > video_duration:
                    # Narration too long - speed up
                    speed_factor = narration_duration / video_duration
                    if speed_factor <= 1.5:  # Max 1.5x speed (still natural)
                        print(f"    ë‚˜ë ˆì´ì…˜ ì†ë„ ì¡°ì ˆ: {speed_factor:.2f}x")
                        try:
                            # Try MoviePy 2.x method
                            from moviepy.audio.fx.audio_speed import audio_speedx
                            narration = audio_speedx(narration, factor=speed_factor)
                        except ImportError:
                            # Fallback to MoviePy 1.x method
                            try:
                                narration = narration.fx(lambda clip: clip.speedx(speed_factor))
                            except:
                                # If all fails, just trim
                                print(f"   âš  ì†ë„ ì¡°ì ˆ ì‹¤íŒ¨, ìë¥´ê¸°ë¡œ ëŒ€ì²´")
                                narration = narration.subclip(0, video_duration)
                        print(f"   [OK] ë‚˜ë ˆì´ì…˜ ê¸¸ì´ ì¡°ì •: {narration.duration:.1f}ì´ˆ")
                    else:
                        # Too fast, just trim
                        print(f"   âœ‚ ë‚˜ë ˆì´ì…˜ ìë¥´ê¸° (ì†ë„ ì¡°ì ˆ ë¶ˆê°€)")
                        narration = narration.subclip(0, video_duration)

                elif narration_duration < video_duration:
                    # Narration too short - slow down
                    speed_factor = narration_duration / video_duration
                    if speed_factor >= 0.7:  # Min 0.7x speed (still natural)
                        print(f"   ğŸ¢ ë‚˜ë ˆì´ì…˜ ì†ë„ ì¡°ì ˆ: {speed_factor:.2f}x (ëŠë¦¬ê²Œ)")
                        try:
                            # Try MoviePy 2.x method
                            from moviepy.audio.fx.audio_speed import audio_speedx
                            narration = audio_speedx(narration, factor=speed_factor)
                        except ImportError:
                            # Fallback to MoviePy 1.x method
                            try:
                                narration = narration.fx(lambda clip: clip.speedx(speed_factor))
                            except:
                                # If all fails, keep original
                                print(f"   âš  ì†ë„ ì¡°ì ˆ ì‹¤íŒ¨")
                        print(f"   [OK] ë‚˜ë ˆì´ì…˜ ê¸¸ì´ ì¡°ì •: {narration.duration:.1f}ì´ˆ")
                    else:
                        # Too slow would sound unnatural, adjust video instead
                        print(f"   âš  ë‚˜ë ˆì´ì…˜ì´ ë„ˆë¬´ ì§§ìŒ ({narration_duration:.1f}ì´ˆ)")
                        print(f"    ë™ì˜ìƒ ê¸¸ì´ë¥¼ ë‚˜ë ˆì´ì…˜ì— ë§ì¶¤")
                        # Will be handled by caller (editor.py)
            else:
                print(f"   [OK] ê¸¸ì´ ì°¨ì´ ë¬´ì‹œ ê°€ëŠ¥ ({duration_diff:.1f}ì´ˆ)")

            # Final adjustment - ensure narration doesn't exceed video
            if narration.duration > clip.duration:
                narration = narration.subclip(0, clip.duration)

            if clip.audio:
                # Mix with original audio (reduce original volume)
                from moviepy.audio.fx.all import volumex
                original_audio = clip.audio.fx(volumex, mix_ratio)
                mixed_audio = CompositeAudioClip([original_audio, narration])
                clip = clip.set_audio(mixed_audio)
            else:
                # Just set narration as audio
                clip = clip.set_audio(narration)

            print(f"[OK] ë‚˜ë ˆì´ì…˜ ë¯¹ì‹± ì™„ë£Œ")
            return clip

        except Exception as e:
            self.logger.error(f"Failed to add narration: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return clip

    def create_narrated_video(
        self,
        video_path: Path,
        output_path: Path,
        video_description: str = None
    ) -> Path:
        """
        Create video with AI narration.

        Args:
            video_path: Input video path
            output_path: Output video path
            video_description: Optional video description

        Returns:
            Path to narrated video
        """
        # Generate script
        script = self.generate_narration_script(video_path, video_description)

        # Generate speech
        audio_path = self.generate_speech(script, output_path)

        # Add to video
        clip = VideoFileClip(str(video_path))
        clip = self.add_narration_to_video(clip, audio_path)

        # Export
        print(f"\n[Export] Exporting narrated video...")
        clip.write_videofile(
            str(output_path),
            codec="libx264",
            audio_codec="aac",
            logger='bar'
        )

        clip.close()

        # Cleanup temp audio
        audio_path.unlink(missing_ok=True)

        return output_path
